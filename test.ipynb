{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Import necessary libraries and modules\n",
    "# =============================================================================\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import os\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from mamba_ssm import Mamba2\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "\n",
    "# =============================================================================\n",
    "# Configuration parameters\n",
    "# =============================================================================\n",
    "class Config:\n",
    "    \"\"\"Experimental configuration class\"\"\"\n",
    "    # Pre-training model configuration\n",
    "    USE_PRETRAINED = False  \n",
    "    \n",
    "    # Data configuration\n",
    "    MAX_ROWS = 1000\n",
    "    MISSING_RATIO = 0.0\n",
    "    \n",
    "    # Model configuration\n",
    "    IN_CHANNELS = 3\n",
    "    OUT_CHANNELS = 256\n",
    "    KERNEL_SIZE = 5\n",
    "    STRIDE = 5\n",
    "    D_STATE = 128\n",
    "    \n",
    "    # Training configuration\n",
    "    BATCH_SIZE = 32\n",
    "    LEARNING_RATE = 1e-3\n",
    "    EPOCHS = 100\n",
    "    EARLY_STOPPING_PATIENCE = 20\n",
    "    \n",
    "    # Cross-validation configuration\n",
    "    N_SPLITS = 10\n",
    "    RANDOM_SEED = 42\n",
    "\n",
    "# Initialize configuration\n",
    "config = Config()\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# CMD Genotype-Phenotype Association Analysis System\n",
    "\n",
    "## System Overview\n",
    "This notebook implements a genotype-phenotype association analysis system based on Mamba2 architecture, supporting pre-training and 10-fold cross-validation.\n",
    "\n",
    "## Main Features\n",
    "- **Pre-training Model**: Uses autoencoder for genotype data pre-training\n",
    "- **Phenotype Prediction**: Predicts phenotype values based on pre-trained features\n",
    "- **Cross-validation**: Supports 10-fold cross-validation for model performance evaluation\n",
    "- **Early Stopping**: Early stopping strategy to prevent overfitting\n",
    "\n",
    "## Configuration Description\n",
    "All configuration parameters are centralized in the `Config` class, including:\n",
    "- `USE_PRETRAINED`: Whether to use pre-trained model\n",
    "- `MISSING_RATIO`: Missing data ratio\n",
    "- `EPOCHS`: Number of training epochs\n",
    "- `EARLY_STOPPING_PATIENCE`: Early stopping patience value\n",
    "\n",
    "## Usage Workflow\n",
    "1. **Data Preprocessing**: Load and preprocess genotype data\n",
    "2. **Pre-training Phase**: Train autoencoder model (optional)\n",
    "3. **Phenotype Prediction**: Train phenotype prediction model\n",
    "4. **Cross-validation**: 10-fold cross-validation for performance evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install causal-conv1d\n",
    "# !pip install mamba-ssm\n",
    "# !pip install \"triton == 2.1.0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Genotype data shape: (1000, 20000)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Data loading and preprocessing\n",
    "# =============================================================================\n",
    "def load_genotype_data(file_path, max_rows=None):\n",
    "    \"\"\"\n",
    "    Load genotype data\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Data file path\n",
    "        max_rows (int): Maximum number of rows limit\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Genotype data frame\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    index = []\n",
    "    \n",
    "    with open(file_path, mode=\"r\") as file:\n",
    "        header = None\n",
    "        for i, line in enumerate(file):\n",
    "            if max_rows and i >= max_rows + 1:\n",
    "                break\n",
    "            row = line.strip().split(\",\")\n",
    "            if i == 0:\n",
    "                header = row[7:20007]  # Skip first 7 columns, take 20000 columns\n",
    "            else:\n",
    "                index.append(row[0])\n",
    "                data.append(row[7:20007])\n",
    "    \n",
    "    df = pd.DataFrame(data, columns=header, index=index)\n",
    "    return df\n",
    "\n",
    "# Load data\n",
    "input_name = 'test_geno.csv'\n",
    "df_ori = load_genotype_data(input_name, max_rows=config.MAX_ROWS)\n",
    "print(f\"Genotype data shape: {df_ori.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing ratio: 0.0\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Data preprocessing functions\n",
    "# =============================================================================\n",
    "def apply_missing_mask(data, missing_ratio):\n",
    "    \"\"\"\n",
    "    Apply missing mask to genotype data\n",
    "    \n",
    "    Args:\n",
    "        data (pd.DataFrame): Original genotype data\n",
    "        missing_ratio (float): Missing ratio\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Data after applying mask\n",
    "    \"\"\"\n",
    "    data_array = data.to_numpy().copy()\n",
    "    if missing_ratio > 0:\n",
    "        mask = np.random.uniform(0, 1, size=data_array.shape)\n",
    "        data_array[mask < missing_ratio] = -1\n",
    "    return data_array\n",
    "\n",
    "def encode_genotype_to_categorical(genotype_data):\n",
    "    \"\"\"\n",
    "    Encode genotype data to categorical format\n",
    "    \n",
    "    Args:\n",
    "        genotype_data (np.ndarray): Genotype data array\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Encoded categorical data\n",
    "    \"\"\"\n",
    "    codebook = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1], [0, 0, 0]])\n",
    "    return codebook[genotype_data.astype(int)]\n",
    "\n",
    "# Apply data preprocessing\n",
    "mask_data = apply_missing_mask(df_ori, config.MISSING_RATIO)\n",
    "print(f\"Missing ratio: {config.MISSING_RATIO}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Masked data shape: (1000, 20000)\n"
     ]
    }
   ],
   "source": [
    "# Create DataFrame for masked data\n",
    "mask_data_copy = pd.DataFrame(mask_data)\n",
    "mask_data_copy.index = df_ori.index\n",
    "print(f\"Masked data shape: {mask_data_copy.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded data shape: (1000, 20000, 3)\n"
     ]
    }
   ],
   "source": [
    "# Encode genotype data to categorical format\n",
    "df_onehot = encode_genotype_to_categorical(mask_data)\n",
    "df_onehot_no_miss = encode_genotype_to_categorical(df_ori.to_numpy())\n",
    "print(f\"Encoded data shape: {df_onehot.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded data shape: (1000, 20000, 3)\n"
     ]
    }
   ],
   "source": [
    "# Verify data shape\n",
    "print(f\"Encoded data shape: {df_onehot.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# Dataset class definitions\n",
    "# =============================================================================\n",
    "class GenotypeDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Genotype dataset class - for pre-training\n",
    "    \"\"\"\n",
    "    def __init__(self, masked_data, original_data):\n",
    "        \"\"\"\n",
    "        Initialize dataset\n",
    "        \n",
    "        Args:\n",
    "            masked_data (np.ndarray): Masked data\n",
    "            original_data (np.ndarray): Original data\n",
    "        \"\"\"\n",
    "        self.masked_data = masked_data\n",
    "        self.original_data = original_data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.masked_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.masked_data[idx]\n",
    "        y = self.original_data[idx]\n",
    "\n",
    "        # Adjust data dimensions\n",
    "        if len(x.shape) == 2:\n",
    "            x = x.transpose(1, 0)\n",
    "            y = y.transpose(1, 0)\n",
    "        elif len(x.shape) == 3:\n",
    "            x = x.transpose(0, 2, 1)\n",
    "            y = y.transpose(0, 2, 1)\n",
    "            \n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "\n",
    "class PhenotypeDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Phenotype dataset class - for phenotype prediction\n",
    "    \"\"\"\n",
    "    def __init__(self, genotype_data, phenotype_data):\n",
    "        \"\"\"\n",
    "        Initialize phenotype dataset\n",
    "        \n",
    "        Args:\n",
    "            genotype_data (np.ndarray): Genotype data\n",
    "            phenotype_data (np.ndarray): Phenotype data\n",
    "        \"\"\"\n",
    "        self.genotype_data = np.asarray(genotype_data, dtype=np.float32)\n",
    "        self.phenotype_data = np.asarray(phenotype_data, dtype=np.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.genotype_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.genotype_data[idx]\n",
    "        y = self.phenotype_data[idx]\n",
    "\n",
    "        # Adjust data dimensions\n",
    "        if len(x.shape) == 2:\n",
    "            x = x.transpose(1, 0)\n",
    "        elif len(x.shape) == 3:\n",
    "            x = x.transpose(0, 2, 1)\n",
    "\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# Model architecture definitions\n",
    "# =============================================================================\n",
    "class Mamba2AutoEncoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Mamba2-based autoencoder model - for pre-training\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, d_state=128):\n",
    "        super(Mamba2AutoEncoder, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, stride=stride)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        # Mamba2 layer\n",
    "        self.mamba = Mamba2(\n",
    "            d_model=out_channels,\n",
    "            d_state=d_state,\n",
    "            d_conv=4,\n",
    "            expand=2,\n",
    "        ).to(\"cuda\")\n",
    "        \n",
    "        # Decoder layers\n",
    "        self.conv1 = nn.Conv1d(out_channels, stride, kernel_size, stride=1, padding='same')\n",
    "        self.conv2 = nn.ConvTranspose1d(stride, in_channels, kernel_size, stride=stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        x = x.permute(0, 2, 1)  # (B, C, L) -> (B, L, C)\n",
    "        x = self.mamba(x)\n",
    "        x = x.permute(0, 2, 1)  # (B, L, C) -> (B, C, L)\n",
    "        \n",
    "        # Decoder\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "    def get_encoder_features(self, x):\n",
    "        \"\"\"Get encoder features\"\"\"\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.mamba(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.conv1(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Mamba2PhenotypePredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    Mamba2-based phenotype prediction model\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, d_state=128, input_length=None):\n",
    "        super(Mamba2PhenotypePredictor, self).__init__()\n",
    "        self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, stride=stride)\n",
    "        self.bn = nn.BatchNorm1d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Mamba2 layer\n",
    "        self.mamba = Mamba2(\n",
    "            d_model=out_channels,\n",
    "            d_state=d_state,\n",
    "            d_conv=4,\n",
    "            expand=2,\n",
    "        ).to(\"cuda\")\n",
    "\n",
    "        self.conv1 = nn.Conv1d(out_channels, stride, kernel_size, stride=1, padding=\"same\")\n",
    "        \n",
    "        # Calculate linear layer input dimension\n",
    "        if input_length is None:\n",
    "            input_length = self._calculate_input_length(20000, kernel_size, stride)\n",
    "        \n",
    "        self.predictor = nn.Sequential(\n",
    "            nn.Linear(input_length, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 1)\n",
    "        )\n",
    "\n",
    "    def _calculate_input_length(self, input_length, kernel_size, stride):\n",
    "        \"\"\"Calculate linear layer input dimension\"\"\"\n",
    "        conv1_output = (input_length - kernel_size) // stride + 1\n",
    "        conv2_output = conv1_output  # padding='same'\n",
    "        return stride * conv2_output\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        x = self.relu(x)\n",
    "\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.mamba(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "        x = x.flatten(-2, -1)\n",
    "        x = self.predictor(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Training and evaluation functions\n",
    "# =============================================================================\n",
    "def train_autoencoder(model, train_loader, device, criterion, optimizer):\n",
    "    \"\"\"\n",
    "    Train autoencoder model\n",
    "    \n",
    "    Args:\n",
    "        model: Autoencoder model\n",
    "        train_loader: Training data loader\n",
    "        device: Device\n",
    "        criterion: Loss function\n",
    "        optimizer: Optimizer\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (average loss, training accuracy)\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        # Create mask\n",
    "        bools = ((labels.sum(axis=1) != 0).unsqueeze(1).expand(labels.shape)).to(device)\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        true_labels = torch.argmax(labels, dim=1)\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        \n",
    "        # Adjust output dimensions\n",
    "        if outputs.shape[2] < bools.shape[2]:\n",
    "            outputs = F.pad(outputs, (0, bools.shape[2] - outputs.shape[2]))\n",
    "        elif outputs.shape[2] > bools.shape[2]:\n",
    "            outputs = outputs[:, :, :bools.shape[2]]\n",
    "\n",
    "        outputs_ = outputs * bools\n",
    "        loss = criterion(outputs_, true_labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        pred = torch.argmax(outputs, dim=1)\n",
    "        correct += (pred == true_labels).sum().item()\n",
    "        total += true_labels.numel()\n",
    "\n",
    "    avg_loss = running_loss / len(train_loader)\n",
    "    train_accuracy = correct / total\n",
    "    \n",
    "    return avg_loss, train_accuracy\n",
    "\n",
    "\n",
    "def evaluate_autoencoder(model, valid_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluate autoencoder model\n",
    "    \n",
    "    Args:\n",
    "        model: Autoencoder model\n",
    "        valid_loader: Validation data loader\n",
    "        device: Device\n",
    "    \n",
    "    Returns:\n",
    "        float: Validation accuracy\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in valid_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            pred = torch.argmax(outputs, dim=1)\n",
    "            true_labels = torch.argmax(labels, dim=1)\n",
    "            \n",
    "            # Adjust prediction dimensions\n",
    "            if pred.shape[1] < true_labels.shape[1]:\n",
    "                pred = F.pad(pred, (0, true_labels.shape[1] - pred.shape[1]))\n",
    "            elif pred.shape[1] > true_labels.shape[1]:\n",
    "                pred = pred[:, :true_labels.shape[1]]\n",
    "            \n",
    "            correct += (pred == true_labels).sum().item()\n",
    "            total += true_labels.numel()\n",
    "\n",
    "    valid_accuracy = correct / total\n",
    "    return valid_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Data splitting and model initialization\n",
    "# =============================================================================\n",
    "def prepare_pretraining_data(genotype_data, original_data, test_size=0.1, random_seed=42):\n",
    "    \"\"\"\n",
    "    Prepare pre-training data\n",
    "    \n",
    "    Args:\n",
    "        genotype_data: Genotype data\n",
    "        original_data: Original data\n",
    "        test_size: Test set ratio\n",
    "        random_seed: Random seed\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (training data loader, validation data loader)\n",
    "    \"\"\"\n",
    "    train_X, valid_X = train_test_split(genotype_data, test_size=test_size, random_state=random_seed)\n",
    "    train_X_original, valid_X_original = train_test_split(original_data, test_size=test_size, random_state=random_seed)\n",
    "    \n",
    "    train_dataset = GenotypeDataset(train_X, train_X_original)\n",
    "    valid_dataset = GenotypeDataset(valid_X, valid_X_original)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=config.BATCH_SIZE, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=len(valid_dataset), shuffle=False)\n",
    "    \n",
    "    return train_loader, valid_loader\n",
    "\n",
    "# Prepare pre-training data\n",
    "train_loader, valid_loader = prepare_pretraining_data(\n",
    "    df_onehot, df_onehot_no_miss, \n",
    "    test_size=0.1, random_seed=config.RANDOM_SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting autoencoder pre-training...\n",
      "Epoch 1/100 - Training accuracy: 0.69692, Validation accuracy: 0.83724, Loss: 0.66664172\n",
      "Epoch 2/100 - Training accuracy: 0.88513, Validation accuracy: 0.90978, Loss: 0.28811342\n",
      "Epoch 3/100 - Training accuracy: 0.92174, Validation accuracy: 0.93513, Loss: 0.17894833\n",
      "Epoch 4/100 - Training accuracy: 0.94296, Validation accuracy: 0.94984, Loss: 0.13288135\n",
      "Epoch 5/100 - Training accuracy: 0.95362, Validation accuracy: 0.95497, Loss: 0.10647228\n",
      "Epoch 6/100 - Training accuracy: 0.95746, Validation accuracy: 0.95934, Loss: 0.09223406\n",
      "Epoch 7/100 - Training accuracy: 0.96045, Validation accuracy: 0.96219, Loss: 0.08366253\n",
      "Epoch 8/100 - Training accuracy: 0.96361, Validation accuracy: 0.96379, Loss: 0.07765317\n",
      "Epoch 9/100 - Training accuracy: 0.96498, Validation accuracy: 0.96499, Loss: 0.07355069\n",
      "Epoch 10/100 - Training accuracy: 0.96586, Validation accuracy: 0.96500, Loss: 0.07145612\n",
      "Epoch 11/100 - Training accuracy: 0.96672, Validation accuracy: 0.96726, Loss: 0.06887861\n",
      "Epoch 12/100 - Training accuracy: 0.96782, Validation accuracy: 0.96779, Loss: 0.06647073\n",
      "Epoch 13/100 - Training accuracy: 0.96322, Validation accuracy: 0.94680, Loss: 0.08044539\n",
      "Epoch 14/100 - Training accuracy: 0.95680, Validation accuracy: 0.96556, Loss: 0.08795950\n",
      "Epoch 15/100 - Training accuracy: 0.96704, Validation accuracy: 0.96761, Loss: 0.06679124\n",
      "Epoch 16/100 - Training accuracy: 0.96848, Validation accuracy: 0.96881, Loss: 0.06301671\n",
      "Epoch 17/100 - Training accuracy: 0.96950, Validation accuracy: 0.96941, Loss: 0.06133615\n",
      "Epoch 18/100 - Training accuracy: 0.97001, Validation accuracy: 0.96993, Loss: 0.05955561\n",
      "Epoch 19/100 - Training accuracy: 0.97063, Validation accuracy: 0.97060, Loss: 0.05810777\n",
      "Epoch 20/100 - Training accuracy: 0.97125, Validation accuracy: 0.97112, Loss: 0.05682446\n",
      "Epoch 21/100 - Training accuracy: 0.97148, Validation accuracy: 0.97113, Loss: 0.05560373\n",
      "Epoch 22/100 - Training accuracy: 0.97163, Validation accuracy: 0.97136, Loss: 0.05449381\n",
      "Epoch 23/100 - Training accuracy: 0.97164, Validation accuracy: 0.97124, Loss: 0.05358742\n",
      "Epoch 24/100 - Training accuracy: 0.97182, Validation accuracy: 0.97170, Loss: 0.05264212\n",
      "Epoch 25/100 - Training accuracy: 0.97191, Validation accuracy: 0.97160, Loss: 0.05168949\n",
      "Epoch 26/100 - Training accuracy: 0.97219, Validation accuracy: 0.97208, Loss: 0.05097961\n",
      "Epoch 27/100 - Training accuracy: 0.97254, Validation accuracy: 0.97231, Loss: 0.05046933\n",
      "Epoch 28/100 - Training accuracy: 0.91325, Validation accuracy: 0.92875, Loss: 0.21102607\n",
      "Epoch 29/100 - Training accuracy: 0.93843, Validation accuracy: 0.94904, Loss: 0.11940869\n",
      "Epoch 30/100 - Training accuracy: 0.95765, Validation accuracy: 0.96452, Loss: 0.08246262\n",
      "Epoch 31/100 - Training accuracy: 0.96693, Validation accuracy: 0.96812, Loss: 0.06722101\n",
      "Epoch 32/100 - Training accuracy: 0.96864, Validation accuracy: 0.96859, Loss: 0.06098948\n",
      "Epoch 33/100 - Training accuracy: 0.96956, Validation accuracy: 0.96967, Loss: 0.05742984\n",
      "Epoch 34/100 - Training accuracy: 0.97121, Validation accuracy: 0.97195, Loss: 0.05491159\n",
      "Epoch 35/100 - Training accuracy: 0.97282, Validation accuracy: 0.97318, Loss: 0.05248593\n",
      "Epoch 36/100 - Training accuracy: 0.97366, Validation accuracy: 0.97365, Loss: 0.05082913\n",
      "Epoch 37/100 - Training accuracy: 0.97450, Validation accuracy: 0.97473, Loss: 0.04989719\n",
      "Epoch 38/100 - Training accuracy: 0.97495, Validation accuracy: 0.97546, Loss: 0.04895475\n",
      "Epoch 39/100 - Training accuracy: 0.97558, Validation accuracy: 0.97577, Loss: 0.04788746\n",
      "Epoch 40/100 - Training accuracy: 0.97595, Validation accuracy: 0.97615, Loss: 0.04720734\n",
      "Epoch 41/100 - Training accuracy: 0.97616, Validation accuracy: 0.97612, Loss: 0.04655420\n",
      "Epoch 42/100 - Training accuracy: 0.97639, Validation accuracy: 0.97644, Loss: 0.04599520\n",
      "Epoch 43/100 - Training accuracy: 0.97663, Validation accuracy: 0.97655, Loss: 0.04541495\n",
      "Epoch 44/100 - Training accuracy: 0.97642, Validation accuracy: 0.97484, Loss: 0.04568881\n",
      "Epoch 45/100 - Training accuracy: 0.97482, Validation accuracy: 0.97605, Loss: 0.05012649\n",
      "Epoch 46/100 - Training accuracy: 0.97665, Validation accuracy: 0.97733, Loss: 0.04550548\n",
      "Epoch 47/100 - Training accuracy: 0.97764, Validation accuracy: 0.97794, Loss: 0.04443593\n",
      "Epoch 48/100 - Training accuracy: 0.97838, Validation accuracy: 0.97869, Loss: 0.04393777\n",
      "Epoch 49/100 - Training accuracy: 0.97926, Validation accuracy: 0.97966, Loss: 0.04343410\n",
      "Epoch 50/100 - Training accuracy: 0.98022, Validation accuracy: 0.98074, Loss: 0.04311334\n",
      "Epoch 51/100 - Training accuracy: 0.98100, Validation accuracy: 0.98106, Loss: 0.04298766\n",
      "Epoch 52/100 - Training accuracy: 0.98123, Validation accuracy: 0.98117, Loss: 0.04256919\n",
      "Epoch 53/100 - Training accuracy: 0.98138, Validation accuracy: 0.98136, Loss: 0.04204230\n",
      "Epoch 54/100 - Training accuracy: 0.98179, Validation accuracy: 0.98198, Loss: 0.04142862\n",
      "Epoch 55/100 - Training accuracy: 0.98207, Validation accuracy: 0.98198, Loss: 0.04115827\n",
      "Epoch 56/100 - Training accuracy: 0.98228, Validation accuracy: 0.98290, Loss: 0.04092872\n",
      "Epoch 57/100 - Training accuracy: 0.98294, Validation accuracy: 0.98314, Loss: 0.04034895\n",
      "Epoch 58/100 - Training accuracy: 0.98336, Validation accuracy: 0.98327, Loss: 0.04021682\n",
      "Epoch 59/100 - Training accuracy: 0.98343, Validation accuracy: 0.98349, Loss: 0.03993332\n",
      "Epoch 60/100 - Training accuracy: 0.97613, Validation accuracy: 0.93202, Loss: 0.08452592\n",
      "Epoch 61/100 - Training accuracy: 0.94657, Validation accuracy: 0.96082, Loss: 0.13161882\n",
      "Epoch 62/100 - Training accuracy: 0.96718, Validation accuracy: 0.97195, Loss: 0.06750483\n",
      "Epoch 63/100 - Training accuracy: 0.97480, Validation accuracy: 0.97693, Loss: 0.05429429\n",
      "Epoch 64/100 - Training accuracy: 0.97859, Validation accuracy: 0.98043, Loss: 0.04973722\n",
      "Epoch 65/100 - Training accuracy: 0.98126, Validation accuracy: 0.98153, Loss: 0.04695953\n",
      "Epoch 66/100 - Training accuracy: 0.98210, Validation accuracy: 0.98257, Loss: 0.04553728\n",
      "Epoch 67/100 - Training accuracy: 0.98298, Validation accuracy: 0.98317, Loss: 0.04436070\n",
      "Epoch 68/100 - Training accuracy: 0.98356, Validation accuracy: 0.98381, Loss: 0.04343502\n",
      "Epoch 69/100 - Training accuracy: 0.98406, Validation accuracy: 0.98417, Loss: 0.04288242\n",
      "Epoch 70/100 - Training accuracy: 0.98439, Validation accuracy: 0.98439, Loss: 0.04206011\n",
      "Epoch 71/100 - Training accuracy: 0.98457, Validation accuracy: 0.98462, Loss: 0.04155207\n",
      "Epoch 72/100 - Training accuracy: 0.98487, Validation accuracy: 0.98494, Loss: 0.04131048\n",
      "Epoch 73/100 - Training accuracy: 0.98506, Validation accuracy: 0.98504, Loss: 0.04085672\n",
      "Epoch 74/100 - Training accuracy: 0.98514, Validation accuracy: 0.98511, Loss: 0.04061992\n",
      "Epoch 75/100 - Training accuracy: 0.98518, Validation accuracy: 0.98508, Loss: 0.04031457\n",
      "Epoch 76/100 - Training accuracy: 0.98520, Validation accuracy: 0.98511, Loss: 0.03998770\n",
      "Epoch 77/100 - Training accuracy: 0.98519, Validation accuracy: 0.98513, Loss: 0.03971307\n",
      "Epoch 78/100 - Training accuracy: 0.98520, Validation accuracy: 0.98513, Loss: 0.03957448\n",
      "Epoch 79/100 - Training accuracy: 0.98520, Validation accuracy: 0.98513, Loss: 0.03933991\n",
      "Epoch 80/100 - Training accuracy: 0.98521, Validation accuracy: 0.98515, Loss: 0.03918787\n",
      "Epoch 81/100 - Training accuracy: 0.98523, Validation accuracy: 0.98517, Loss: 0.03908906\n",
      "Epoch 82/100 - Training accuracy: 0.98524, Validation accuracy: 0.98518, Loss: 0.03871866\n",
      "Epoch 83/100 - Training accuracy: 0.98524, Validation accuracy: 0.98519, Loss: 0.03861532\n",
      "Epoch 84/100 - Training accuracy: 0.98526, Validation accuracy: 0.98519, Loss: 0.03828480\n",
      "Epoch 85/100 - Training accuracy: 0.98527, Validation accuracy: 0.98522, Loss: 0.03832952\n",
      "Epoch 86/100 - Training accuracy: 0.98530, Validation accuracy: 0.98539, Loss: 0.03794303\n",
      "Epoch 87/100 - Training accuracy: 0.98473, Validation accuracy: 0.98486, Loss: 0.03960449\n",
      "Epoch 88/100 - Training accuracy: 0.98527, Validation accuracy: 0.98539, Loss: 0.03827849\n",
      "Epoch 89/100 - Training accuracy: 0.98537, Validation accuracy: 0.98540, Loss: 0.03752989\n",
      "Epoch 90/100 - Training accuracy: 0.98533, Validation accuracy: 0.98522, Loss: 0.03728058\n",
      "Epoch 91/100 - Training accuracy: 0.98529, Validation accuracy: 0.98520, Loss: 0.03681512\n",
      "Epoch 92/100 - Training accuracy: 0.98569, Validation accuracy: 0.98637, Loss: 0.03590530\n",
      "Epoch 93/100 - Training accuracy: 0.98693, Validation accuracy: 0.98742, Loss: 0.03433809\n",
      "Epoch 94/100 - Training accuracy: 0.98434, Validation accuracy: 0.98481, Loss: 0.04047528\n",
      "Epoch 95/100 - Training accuracy: 0.98517, Validation accuracy: 0.98687, Loss: 0.03874390\n",
      "Epoch 96/100 - Training accuracy: 0.98731, Validation accuracy: 0.98736, Loss: 0.03394595\n",
      "Epoch 97/100 - Training accuracy: 0.98760, Validation accuracy: 0.98750, Loss: 0.03254591\n",
      "Epoch 98/100 - Training accuracy: 0.98765, Validation accuracy: 0.98752, Loss: 0.03199035\n",
      "Epoch 99/100 - Training accuracy: 0.98774, Validation accuracy: 0.98765, Loss: 0.03156871\n",
      "Epoch 100/100 - Training accuracy: 0.98788, Validation accuracy: 0.98788, Loss: 0.03114989\n",
      "Pre-training completed! Best model saved at: ./model/model_state.pth\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Pre-training phase\n",
    "# =============================================================================\n",
    "def pretrain_autoencoder(train_loader, valid_loader, device, epochs=100):\n",
    "    \"\"\"\n",
    "    Pre-train autoencoder model\n",
    "    \n",
    "    Args:\n",
    "        train_loader: Training data loader\n",
    "        valid_loader: Validation data loader\n",
    "        device: Device\n",
    "        epochs: Number of training epochs\n",
    "    \n",
    "    Returns:\n",
    "        str: Best model save path\n",
    "    \"\"\"\n",
    "    # Initialize model\n",
    "    model = Mamba2AutoEncoder(\n",
    "        in_channels=config.IN_CHANNELS,\n",
    "        out_channels=config.OUT_CHANNELS,\n",
    "        kernel_size=config.KERNEL_SIZE,\n",
    "        stride=config.STRIDE,\n",
    "        d_state=config.D_STATE\n",
    "    ).to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config.LEARNING_RATE)\n",
    "    \n",
    "    # Create model save directory\n",
    "    model_dir = \"./model\"\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    # Training records\n",
    "    train_accuracies = []\n",
    "    valid_accuracies = []\n",
    "    losses = []\n",
    "    best_valid_acc = 0\n",
    "    model_file_path = None\n",
    "    \n",
    "    print(\"Starting autoencoder pre-training...\")\n",
    "    for epoch in range(epochs):\n",
    "        avg_loss, acc_train = train_autoencoder(model, train_loader, device, criterion, optimizer)\n",
    "        acc_valid = evaluate_autoencoder(model, valid_loader, device)\n",
    "\n",
    "        # Convert tensor to numpy\n",
    "        if isinstance(acc_train, torch.Tensor):\n",
    "            acc_train = acc_train.cpu().numpy()\n",
    "        if isinstance(acc_valid, torch.Tensor):\n",
    "            acc_valid = acc_valid.cpu().numpy()\n",
    "\n",
    "        # Save best model\n",
    "        if acc_valid > best_valid_acc:\n",
    "            best_valid_acc = acc_valid\n",
    "            model_file_path = os.path.join(model_dir, \"model_state.pth\")\n",
    "            torch.save(model.state_dict(), model_file_path)\n",
    "\n",
    "        train_accuracies.append(acc_train)\n",
    "        valid_accuracies.append(acc_valid)\n",
    "        losses.append(avg_loss)\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{epochs} - Training accuracy: {acc_train:.5f}, Validation accuracy: {acc_valid:.5f}, Loss: {avg_loss:.8f}')\n",
    "\n",
    "    print(f\"Pre-training completed! Best model saved at: {model_file_path}\")\n",
    "    return model_file_path\n",
    "\n",
    "# Execute pre-training\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "pretrained_model_path = pretrain_autoencoder(train_loader, valid_loader, device, config.EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phenotype data shape: (1000, 7)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Phenotype data loading and preprocessing\n",
    "# =============================================================================\n",
    "def load_phenotype_data(file_path):\n",
    "    \"\"\"\n",
    "    Load phenotype data\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Phenotype data file path\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: Phenotype data frame\n",
    "    \"\"\"\n",
    "    return pd.read_csv(file_path, sep=',', index_col=0)\n",
    "\n",
    "# Load phenotype data\n",
    "phenotype_data = load_phenotype_data(\"1000pheno.txt\")\n",
    "print(f\"Phenotype data shape: {phenotype_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phenotype column name: AL\n",
      "After normalization - Mean: 0.0000, Std: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Phenotype data preprocessing\n",
    "# =============================================================================\n",
    "def preprocess_phenotype_data(phenotype_df, genotype_df, phenotype_column=1):\n",
    "    \"\"\"\n",
    "    Preprocess phenotype data and align with genotype data\n",
    "    \n",
    "    Args:\n",
    "        phenotype_df: Phenotype data frame\n",
    "        genotype_df: Genotype data frame\n",
    "        phenotype_column: Phenotype column index\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (genotype data, phenotype data, scaler)\n",
    "    \"\"\"\n",
    "    # Find common samples\n",
    "    common_samples = set(phenotype_df.index) & set(genotype_df.index)\n",
    "    common_samples = list(common_samples)\n",
    "    \n",
    "    # Extract corresponding data\n",
    "    phenotype_values = phenotype_df.loc[common_samples].iloc[:, phenotype_column].values\n",
    "    genotype_values = genotype_df.loc[common_samples].values\n",
    "    \n",
    "    # Encode genotype data to categorical format\n",
    "    genotype_encoded = encode_genotype_to_categorical(genotype_values)\n",
    "    \n",
    "    # Normalize phenotype data\n",
    "    scaler = StandardScaler()\n",
    "    phenotype_normalized = scaler.fit_transform(phenotype_values.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    print(f\"Phenotype column name: {phenotype_df.columns[phenotype_column]}\")\n",
    "    print(f\"After normalization - Mean: {np.mean(phenotype_normalized):.4f}, Std: {np.std(phenotype_normalized):.4f}\")\n",
    "    \n",
    "    return genotype_encoded, phenotype_normalized, scaler\n",
    "\n",
    "# Preprocess phenotype data\n",
    "genotype_encoded, phenotype_normalized, phenotype_scaler = preprocess_phenotype_data(\n",
    "    phenotype_data, mask_data_copy, phenotype_column=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# Phenotype prediction training and evaluation functions\n",
    "# =============================================================================\n",
    "def train_phenotype_predictor(epoch, model, device, optimizer, criterion, train_loader, test_loader, scaler):\n",
    "    \"\"\"\n",
    "    Train phenotype prediction model\n",
    "    \n",
    "    Args:\n",
    "        epoch: Current epoch\n",
    "        model: Prediction model\n",
    "        device: Device\n",
    "        optimizer: Optimizer\n",
    "        criterion: Loss function\n",
    "        train_loader: Training data loader\n",
    "        test_loader: Test data loader\n",
    "        scaler: Scaler\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (correlation coefficient, prediction output, loss values list)\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    # Set all parameters trainable\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    all_loss = 0\n",
    "    fold_loss_values = []\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.float(), target.float()\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output.view(-1), target.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        all_loss += loss.item()\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'Training epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
    "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "    \n",
    "    avg_loss = all_loss / len(train_loader)\n",
    "    fold_loss_values.append(avg_loss)\n",
    "    print(f'=========> Epoch: {epoch} Average loss: {avg_loss:.4f}')\n",
    "\n",
    "    # Evaluate model\n",
    "    corr, predictions = evaluate_phenotype_predictor(model, device, test_loader, scaler)\n",
    "    return corr, predictions, fold_loss_values\n",
    "\n",
    "\n",
    "def evaluate_phenotype_predictor(model, device, test_loader, scaler):\n",
    "    \"\"\"\n",
    "    Evaluate phenotype prediction model\n",
    "    \n",
    "    Args:\n",
    "        model: Prediction model\n",
    "        device: Device\n",
    "        test_loader: Test data loader\n",
    "        scaler: Scaler\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (correlation coefficient, prediction output)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.float(), target.float()\n",
    "            data, target = data.to(device), target\n",
    "            \n",
    "            output = model(data)\n",
    "            \n",
    "            # Inverse normalize prediction results and true values\n",
    "            output_original = scaler.inverse_transform(\n",
    "                output.view(-1).detach().cpu().numpy().reshape(-1, 1)\n",
    "            ).flatten()\n",
    "            target_original = scaler.inverse_transform(\n",
    "                target.reshape(-1).reshape(-1, 1)\n",
    "            ).flatten()\n",
    "            \n",
    "            # Calculate correlation coefficient\n",
    "            corr = np.corrcoef(output_original, target_original)[0, 1]\n",
    "            print(f\"Correlation coefficient: {corr:.4f}\")\n",
    "            return corr, output_original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_params = torch.load(model_file_path)\n",
    "# model_state = model2.state_dict()\n",
    "\n",
    "# # 过滤掉形状不匹配的参数\n",
    "# filtered_params = {k: v for k, v in all_params.items() if k in model_state and v.size() == model_state[k].size()}\n",
    "\n",
    "# # 更新模型参数\n",
    "# model_state.update(filtered_params)\n",
    "# model2.load_state_dict(model_state, strict=False)\n",
    "\n",
    "# print(\"部分参数已加载，以下参数未匹配到:\")\n",
    "# for name, param in model2.state_dict().items():\n",
    "#     if name not in filtered_params:\n",
    "#         print(f\"{name}: expected {param.size()}, got {all_params[name].size() if name in all_params else 'missing'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========== Cross-validation Fold 1/10 ==========\n",
      "🔄 Fold 1: Using random initialization\n",
      "Training epoch: 1 [0/900 (0%)]\tLoss: 0.967151\n",
      "Training epoch: 1 [160/900 (18%)]\tLoss: 1.669616\n",
      "Training epoch: 1 [320/900 (35%)]\tLoss: 0.657429\n",
      "Training epoch: 1 [480/900 (53%)]\tLoss: 0.562617\n",
      "Training epoch: 1 [640/900 (70%)]\tLoss: 0.659228\n",
      "Training epoch: 1 [800/900 (88%)]\tLoss: 0.765003\n",
      "=========> Epoch: 1 Average loss: 0.8141\n",
      "Correlation coefficient: 0.7091\n",
      "✅ Epoch 1: New best correlation = 0.7091\n",
      "Training epoch: 2 [0/900 (0%)]\tLoss: 0.427131\n",
      "Training epoch: 2 [160/900 (18%)]\tLoss: 0.635957\n",
      "Training epoch: 2 [320/900 (35%)]\tLoss: 0.456179\n",
      "Training epoch: 2 [480/900 (53%)]\tLoss: 0.255066\n",
      "Training epoch: 2 [640/900 (70%)]\tLoss: 0.303335\n",
      "Training epoch: 2 [800/900 (88%)]\tLoss: 0.307972\n",
      "=========> Epoch: 2 Average loss: 0.3918\n",
      "Correlation coefficient: 0.7105\n",
      "✅ Epoch 2: New best correlation = 0.7105\n",
      "Training epoch: 3 [0/900 (0%)]\tLoss: 0.103315\n",
      "Training epoch: 3 [160/900 (18%)]\tLoss: 0.132544\n",
      "Training epoch: 3 [320/900 (35%)]\tLoss: 0.089169\n",
      "Training epoch: 3 [480/900 (53%)]\tLoss: 0.029605\n",
      "Training epoch: 3 [640/900 (70%)]\tLoss: 0.085588\n",
      "Training epoch: 3 [800/900 (88%)]\tLoss: 0.177230\n",
      "=========> Epoch: 3 Average loss: 0.1471\n",
      "Correlation coefficient: 0.7045\n",
      "Training epoch: 4 [0/900 (0%)]\tLoss: 0.144951\n",
      "Training epoch: 4 [160/900 (18%)]\tLoss: 0.092868\n",
      "Training epoch: 4 [320/900 (35%)]\tLoss: 0.031964\n",
      "Training epoch: 4 [480/900 (53%)]\tLoss: 0.067033\n",
      "Training epoch: 4 [640/900 (70%)]\tLoss: 0.039080\n",
      "Training epoch: 4 [800/900 (88%)]\tLoss: 0.024137\n",
      "=========> Epoch: 4 Average loss: 0.0560\n",
      "Correlation coefficient: 0.6948\n",
      "Training epoch: 5 [0/900 (0%)]\tLoss: 0.037764\n",
      "Training epoch: 5 [160/900 (18%)]\tLoss: 0.030545\n",
      "Training epoch: 5 [320/900 (35%)]\tLoss: 0.018061\n",
      "Training epoch: 5 [480/900 (53%)]\tLoss: 0.020965\n",
      "Training epoch: 5 [640/900 (70%)]\tLoss: 0.032963\n",
      "Training epoch: 5 [800/900 (88%)]\tLoss: 0.071009\n",
      "=========> Epoch: 5 Average loss: 0.0331\n",
      "Correlation coefficient: 0.7099\n",
      "Training epoch: 6 [0/900 (0%)]\tLoss: 0.025987\n",
      "Training epoch: 6 [160/900 (18%)]\tLoss: 0.019484\n",
      "Training epoch: 6 [320/900 (35%)]\tLoss: 0.051129\n",
      "Training epoch: 6 [480/900 (53%)]\tLoss: 0.007144\n",
      "Training epoch: 6 [640/900 (70%)]\tLoss: 0.008461\n",
      "Training epoch: 6 [800/900 (88%)]\tLoss: 0.004844\n",
      "=========> Epoch: 6 Average loss: 0.0160\n",
      "Correlation coefficient: 0.7021\n",
      "Training epoch: 7 [0/900 (0%)]\tLoss: 0.010199\n",
      "Training epoch: 7 [160/900 (18%)]\tLoss: 0.011006\n",
      "Training epoch: 7 [320/900 (35%)]\tLoss: 0.008246\n",
      "Training epoch: 7 [480/900 (53%)]\tLoss: 0.005823\n",
      "Training epoch: 7 [640/900 (70%)]\tLoss: 0.007111\n",
      "Training epoch: 7 [800/900 (88%)]\tLoss: 0.010137\n",
      "=========> Epoch: 7 Average loss: 0.0094\n",
      "Correlation coefficient: 0.7047\n",
      "Training epoch: 8 [0/900 (0%)]\tLoss: 0.002016\n",
      "Training epoch: 8 [160/900 (18%)]\tLoss: 0.011644\n",
      "Training epoch: 8 [320/900 (35%)]\tLoss: 0.004669\n",
      "Training epoch: 8 [480/900 (53%)]\tLoss: 0.009751\n",
      "Training epoch: 8 [640/900 (70%)]\tLoss: 0.003231\n",
      "Training epoch: 8 [800/900 (88%)]\tLoss: 0.004870\n",
      "=========> Epoch: 8 Average loss: 0.0059\n",
      "Correlation coefficient: 0.7045\n",
      "Training epoch: 9 [0/900 (0%)]\tLoss: 0.001755\n",
      "Training epoch: 9 [160/900 (18%)]\tLoss: 0.015811\n",
      "Training epoch: 9 [320/900 (35%)]\tLoss: 0.004166\n",
      "Training epoch: 9 [480/900 (53%)]\tLoss: 0.003983\n",
      "Training epoch: 9 [640/900 (70%)]\tLoss: 0.001604\n",
      "Training epoch: 9 [800/900 (88%)]\tLoss: 0.004396\n",
      "=========> Epoch: 9 Average loss: 0.0048\n",
      "Correlation coefficient: 0.7057\n",
      "Training epoch: 10 [0/900 (0%)]\tLoss: 0.002926\n",
      "Training epoch: 10 [160/900 (18%)]\tLoss: 0.000860\n",
      "Training epoch: 10 [320/900 (35%)]\tLoss: 0.004646\n",
      "Training epoch: 10 [480/900 (53%)]\tLoss: 0.004109\n",
      "Training epoch: 10 [640/900 (70%)]\tLoss: 0.001743\n",
      "Training epoch: 10 [800/900 (88%)]\tLoss: 0.001397\n",
      "=========> Epoch: 10 Average loss: 0.0035\n",
      "Correlation coefficient: 0.7064\n",
      "Training epoch: 11 [0/900 (0%)]\tLoss: 0.000965\n",
      "Training epoch: 11 [160/900 (18%)]\tLoss: 0.011237\n",
      "Training epoch: 11 [320/900 (35%)]\tLoss: 0.002275\n",
      "Training epoch: 11 [480/900 (53%)]\tLoss: 0.001443\n",
      "Training epoch: 11 [640/900 (70%)]\tLoss: 0.001120\n",
      "Training epoch: 11 [800/900 (88%)]\tLoss: 0.001974\n",
      "=========> Epoch: 11 Average loss: 0.0035\n",
      "Correlation coefficient: 0.7081\n",
      "Training epoch: 12 [0/900 (0%)]\tLoss: 0.001380\n",
      "Training epoch: 12 [160/900 (18%)]\tLoss: 0.005205\n",
      "Training epoch: 12 [320/900 (35%)]\tLoss: 0.002050\n",
      "Training epoch: 12 [480/900 (53%)]\tLoss: 0.001368\n",
      "Training epoch: 12 [640/900 (70%)]\tLoss: 0.002747\n",
      "Training epoch: 12 [800/900 (88%)]\tLoss: 0.006581\n",
      "=========> Epoch: 12 Average loss: 0.0041\n",
      "Correlation coefficient: 0.7053\n",
      "Training epoch: 13 [0/900 (0%)]\tLoss: 0.005930\n",
      "Training epoch: 13 [160/900 (18%)]\tLoss: 0.003875\n",
      "Training epoch: 13 [320/900 (35%)]\tLoss: 0.006089\n",
      "Training epoch: 13 [480/900 (53%)]\tLoss: 0.003466\n",
      "Training epoch: 13 [640/900 (70%)]\tLoss: 0.002503\n",
      "Training epoch: 13 [800/900 (88%)]\tLoss: 0.001839\n",
      "=========> Epoch: 13 Average loss: 0.0048\n",
      "Correlation coefficient: 0.7093\n",
      "Training epoch: 14 [0/900 (0%)]\tLoss: 0.003420\n",
      "Training epoch: 14 [160/900 (18%)]\tLoss: 0.010051\n",
      "Training epoch: 14 [320/900 (35%)]\tLoss: 0.004844\n",
      "Training epoch: 14 [480/900 (53%)]\tLoss: 0.005997\n",
      "Training epoch: 14 [640/900 (70%)]\tLoss: 0.006417\n",
      "Training epoch: 14 [800/900 (88%)]\tLoss: 0.003520\n",
      "=========> Epoch: 14 Average loss: 0.0062\n",
      "Correlation coefficient: 0.7018\n",
      "Training epoch: 15 [0/900 (0%)]\tLoss: 0.002357\n",
      "Training epoch: 15 [160/900 (18%)]\tLoss: 0.009020\n",
      "Training epoch: 15 [320/900 (35%)]\tLoss: 0.002754\n",
      "Training epoch: 15 [480/900 (53%)]\tLoss: 0.018458\n",
      "Training epoch: 15 [640/900 (70%)]\tLoss: 0.003090\n",
      "Training epoch: 15 [800/900 (88%)]\tLoss: 0.003845\n",
      "=========> Epoch: 15 Average loss: 0.0071\n",
      "Correlation coefficient: 0.7100\n",
      "Training epoch: 16 [0/900 (0%)]\tLoss: 0.006774\n",
      "Training epoch: 16 [160/900 (18%)]\tLoss: 0.009214\n",
      "Training epoch: 16 [320/900 (35%)]\tLoss: 0.004872\n",
      "Training epoch: 16 [480/900 (53%)]\tLoss: 0.015796\n",
      "Training epoch: 16 [640/900 (70%)]\tLoss: 0.018007\n",
      "Training epoch: 16 [800/900 (88%)]\tLoss: 0.019792\n",
      "=========> Epoch: 16 Average loss: 0.0102\n",
      "Correlation coefficient: 0.7097\n",
      "Training epoch: 17 [0/900 (0%)]\tLoss: 0.017027\n",
      "Training epoch: 17 [160/900 (18%)]\tLoss: 0.005468\n",
      "Training epoch: 17 [320/900 (35%)]\tLoss: 0.006444\n",
      "Training epoch: 17 [480/900 (53%)]\tLoss: 0.023486\n",
      "Training epoch: 17 [640/900 (70%)]\tLoss: 0.033251\n",
      "Training epoch: 17 [800/900 (88%)]\tLoss: 0.002555\n",
      "=========> Epoch: 17 Average loss: 0.0167\n",
      "Correlation coefficient: 0.7013\n",
      "Training epoch: 18 [0/900 (0%)]\tLoss: 0.017719\n",
      "Training epoch: 18 [160/900 (18%)]\tLoss: 0.022894\n",
      "Training epoch: 18 [320/900 (35%)]\tLoss: 0.012397\n",
      "Training epoch: 18 [480/900 (53%)]\tLoss: 0.011375\n",
      "Training epoch: 18 [640/900 (70%)]\tLoss: 0.008532\n",
      "Training epoch: 18 [800/900 (88%)]\tLoss: 0.012565\n",
      "=========> Epoch: 18 Average loss: 0.0203\n",
      "Correlation coefficient: 0.7156\n",
      "✅ Epoch 18: New best correlation = 0.7156\n",
      "Training epoch: 19 [0/900 (0%)]\tLoss: 0.026828\n",
      "Training epoch: 19 [160/900 (18%)]\tLoss: 0.006049\n",
      "Training epoch: 19 [320/900 (35%)]\tLoss: 0.007635\n",
      "Training epoch: 19 [480/900 (53%)]\tLoss: 0.023910\n",
      "Training epoch: 19 [640/900 (70%)]\tLoss: 0.026417\n",
      "Training epoch: 19 [800/900 (88%)]\tLoss: 0.022646\n",
      "=========> Epoch: 19 Average loss: 0.0193\n",
      "Correlation coefficient: 0.7045\n",
      "Training epoch: 20 [0/900 (0%)]\tLoss: 0.011781\n",
      "Training epoch: 20 [160/900 (18%)]\tLoss: 0.010820\n",
      "Training epoch: 20 [320/900 (35%)]\tLoss: 0.007024\n",
      "Training epoch: 20 [480/900 (53%)]\tLoss: 0.019108\n",
      "Training epoch: 20 [640/900 (70%)]\tLoss: 0.025380\n",
      "Training epoch: 20 [800/900 (88%)]\tLoss: 0.016148\n",
      "=========> Epoch: 20 Average loss: 0.0210\n",
      "Correlation coefficient: 0.6980\n",
      "Training epoch: 21 [0/900 (0%)]\tLoss: 0.033467\n",
      "Training epoch: 21 [160/900 (18%)]\tLoss: 0.024508\n",
      "Training epoch: 21 [320/900 (35%)]\tLoss: 0.028674\n",
      "Training epoch: 21 [480/900 (53%)]\tLoss: 0.011526\n",
      "Training epoch: 21 [640/900 (70%)]\tLoss: 0.013213\n",
      "Training epoch: 21 [800/900 (88%)]\tLoss: 0.014576\n",
      "=========> Epoch: 21 Average loss: 0.0187\n",
      "Correlation coefficient: 0.7128\n",
      "Training epoch: 22 [0/900 (0%)]\tLoss: 0.016199\n",
      "Training epoch: 22 [160/900 (18%)]\tLoss: 0.022304\n",
      "Training epoch: 22 [320/900 (35%)]\tLoss: 0.012901\n",
      "Training epoch: 22 [480/900 (53%)]\tLoss: 0.020051\n",
      "Training epoch: 22 [640/900 (70%)]\tLoss: 0.017002\n",
      "Training epoch: 22 [800/900 (88%)]\tLoss: 0.011272\n",
      "=========> Epoch: 22 Average loss: 0.0191\n",
      "Correlation coefficient: 0.6911\n",
      "Training epoch: 23 [0/900 (0%)]\tLoss: 0.012508\n",
      "Training epoch: 23 [160/900 (18%)]\tLoss: 0.012678\n",
      "Training epoch: 23 [320/900 (35%)]\tLoss: 0.014597\n",
      "Training epoch: 23 [480/900 (53%)]\tLoss: 0.007184\n",
      "Training epoch: 23 [640/900 (70%)]\tLoss: 0.006124\n",
      "Training epoch: 23 [800/900 (88%)]\tLoss: 0.018734\n",
      "=========> Epoch: 23 Average loss: 0.0132\n",
      "Correlation coefficient: 0.7037\n",
      "Training epoch: 24 [0/900 (0%)]\tLoss: 0.002083\n",
      "Training epoch: 24 [160/900 (18%)]\tLoss: 0.005922\n",
      "Training epoch: 24 [320/900 (35%)]\tLoss: 0.011984\n",
      "Training epoch: 24 [480/900 (53%)]\tLoss: 0.008514\n",
      "Training epoch: 24 [640/900 (70%)]\tLoss: 0.004651\n",
      "Training epoch: 24 [800/900 (88%)]\tLoss: 0.005397\n",
      "=========> Epoch: 24 Average loss: 0.0091\n",
      "Correlation coefficient: 0.7097\n",
      "Training epoch: 25 [0/900 (0%)]\tLoss: 0.010579\n",
      "Training epoch: 25 [160/900 (18%)]\tLoss: 0.005483\n",
      "Training epoch: 25 [320/900 (35%)]\tLoss: 0.010928\n",
      "Training epoch: 25 [480/900 (53%)]\tLoss: 0.007875\n",
      "Training epoch: 25 [640/900 (70%)]\tLoss: 0.005166\n",
      "Training epoch: 25 [800/900 (88%)]\tLoss: 0.005049\n",
      "=========> Epoch: 25 Average loss: 0.0071\n",
      "Correlation coefficient: 0.7093\n",
      "Training epoch: 26 [0/900 (0%)]\tLoss: 0.003425\n",
      "Training epoch: 26 [160/900 (18%)]\tLoss: 0.004814\n",
      "Training epoch: 26 [320/900 (35%)]\tLoss: 0.011559\n",
      "Training epoch: 26 [480/900 (53%)]\tLoss: 0.011253\n",
      "Training epoch: 26 [640/900 (70%)]\tLoss: 0.008530\n",
      "Training epoch: 26 [800/900 (88%)]\tLoss: 0.015964\n",
      "=========> Epoch: 26 Average loss: 0.0062\n",
      "Correlation coefficient: 0.7092\n",
      "Training epoch: 27 [0/900 (0%)]\tLoss: 0.001704\n",
      "Training epoch: 27 [160/900 (18%)]\tLoss: 0.003082\n",
      "Training epoch: 27 [320/900 (35%)]\tLoss: 0.006861\n",
      "Training epoch: 27 [480/900 (53%)]\tLoss: 0.004061\n",
      "Training epoch: 27 [640/900 (70%)]\tLoss: 0.003486\n",
      "Training epoch: 27 [800/900 (88%)]\tLoss: 0.007599\n",
      "=========> Epoch: 27 Average loss: 0.0054\n",
      "Correlation coefficient: 0.7056\n",
      "Training epoch: 28 [0/900 (0%)]\tLoss: 0.003928\n",
      "Training epoch: 28 [160/900 (18%)]\tLoss: 0.008552\n",
      "Training epoch: 28 [320/900 (35%)]\tLoss: 0.001539\n",
      "Training epoch: 28 [480/900 (53%)]\tLoss: 0.001996\n",
      "Training epoch: 28 [640/900 (70%)]\tLoss: 0.007006\n",
      "Training epoch: 28 [800/900 (88%)]\tLoss: 0.003166\n",
      "=========> Epoch: 28 Average loss: 0.0041\n",
      "Correlation coefficient: 0.7089\n",
      "Training epoch: 29 [0/900 (0%)]\tLoss: 0.002124\n",
      "Training epoch: 29 [160/900 (18%)]\tLoss: 0.001488\n",
      "Training epoch: 29 [320/900 (35%)]\tLoss: 0.003802\n",
      "Training epoch: 29 [480/900 (53%)]\tLoss: 0.003504\n",
      "Training epoch: 29 [640/900 (70%)]\tLoss: 0.008886\n",
      "Training epoch: 29 [800/900 (88%)]\tLoss: 0.002789\n",
      "=========> Epoch: 29 Average loss: 0.0036\n",
      "Correlation coefficient: 0.7112\n",
      "Training epoch: 30 [0/900 (0%)]\tLoss: 0.003424\n",
      "Training epoch: 30 [160/900 (18%)]\tLoss: 0.005109\n",
      "Training epoch: 30 [320/900 (35%)]\tLoss: 0.006216\n",
      "Training epoch: 30 [480/900 (53%)]\tLoss: 0.002412\n",
      "Training epoch: 30 [640/900 (70%)]\tLoss: 0.001954\n",
      "Training epoch: 30 [800/900 (88%)]\tLoss: 0.002608\n",
      "=========> Epoch: 30 Average loss: 0.0039\n",
      "Correlation coefficient: 0.7047\n",
      "Training epoch: 31 [0/900 (0%)]\tLoss: 0.003232\n",
      "Training epoch: 31 [160/900 (18%)]\tLoss: 0.005009\n",
      "Training epoch: 31 [320/900 (35%)]\tLoss: 0.002621\n",
      "Training epoch: 31 [480/900 (53%)]\tLoss: 0.004583\n",
      "Training epoch: 31 [640/900 (70%)]\tLoss: 0.005937\n",
      "Training epoch: 31 [800/900 (88%)]\tLoss: 0.003882\n",
      "=========> Epoch: 31 Average loss: 0.0051\n",
      "Correlation coefficient: 0.7112\n",
      "Training epoch: 32 [0/900 (0%)]\tLoss: 0.004715\n",
      "Training epoch: 32 [160/900 (18%)]\tLoss: 0.004690\n",
      "Training epoch: 32 [320/900 (35%)]\tLoss: 0.012808\n",
      "Training epoch: 32 [480/900 (53%)]\tLoss: 0.009964\n",
      "Training epoch: 32 [640/900 (70%)]\tLoss: 0.038120\n",
      "Training epoch: 32 [800/900 (88%)]\tLoss: 0.008749\n",
      "=========> Epoch: 32 Average loss: 0.0117\n",
      "Correlation coefficient: 0.7018\n",
      "Training epoch: 33 [0/900 (0%)]\tLoss: 0.014672\n",
      "Training epoch: 33 [160/900 (18%)]\tLoss: 0.013753\n",
      "Training epoch: 33 [320/900 (35%)]\tLoss: 0.008509\n",
      "Training epoch: 33 [480/900 (53%)]\tLoss: 0.009418\n",
      "Training epoch: 33 [640/900 (70%)]\tLoss: 0.010077\n",
      "Training epoch: 33 [800/900 (88%)]\tLoss: 0.043346\n",
      "=========> Epoch: 33 Average loss: 0.0172\n",
      "Correlation coefficient: 0.7127\n",
      "Training epoch: 34 [0/900 (0%)]\tLoss: 0.021673\n",
      "Training epoch: 34 [160/900 (18%)]\tLoss: 0.020981\n",
      "Training epoch: 34 [320/900 (35%)]\tLoss: 0.021960\n",
      "Training epoch: 34 [480/900 (53%)]\tLoss: 0.040207\n",
      "Training epoch: 34 [640/900 (70%)]\tLoss: 0.009591\n",
      "Training epoch: 34 [800/900 (88%)]\tLoss: 0.042439\n",
      "=========> Epoch: 34 Average loss: 0.0176\n",
      "Correlation coefficient: 0.7101\n",
      "Training epoch: 35 [0/900 (0%)]\tLoss: 0.006034\n",
      "Training epoch: 35 [160/900 (18%)]\tLoss: 0.069509\n",
      "Training epoch: 35 [320/900 (35%)]\tLoss: 0.005925\n",
      "Training epoch: 35 [480/900 (53%)]\tLoss: 0.013129\n",
      "Training epoch: 35 [640/900 (70%)]\tLoss: 0.010409\n",
      "Training epoch: 35 [800/900 (88%)]\tLoss: 0.012571\n",
      "=========> Epoch: 35 Average loss: 0.0223\n",
      "Correlation coefficient: 0.7063\n",
      "Training epoch: 36 [0/900 (0%)]\tLoss: 0.014854\n",
      "Training epoch: 36 [160/900 (18%)]\tLoss: 0.065442\n",
      "Training epoch: 36 [320/900 (35%)]\tLoss: 0.009528\n",
      "Training epoch: 36 [480/900 (53%)]\tLoss: 0.013450\n",
      "Training epoch: 36 [640/900 (70%)]\tLoss: 0.009675\n",
      "Training epoch: 36 [800/900 (88%)]\tLoss: 0.019517\n",
      "=========> Epoch: 36 Average loss: 0.0232\n",
      "Correlation coefficient: 0.7051\n",
      "Training epoch: 37 [0/900 (0%)]\tLoss: 0.010875\n",
      "Training epoch: 37 [160/900 (18%)]\tLoss: 0.008710\n",
      "Training epoch: 37 [320/900 (35%)]\tLoss: 0.010735\n",
      "Training epoch: 37 [480/900 (53%)]\tLoss: 0.014690\n",
      "Training epoch: 37 [640/900 (70%)]\tLoss: 0.020847\n",
      "Training epoch: 37 [800/900 (88%)]\tLoss: 0.012988\n",
      "=========> Epoch: 37 Average loss: 0.0165\n",
      "Correlation coefficient: 0.7107\n",
      "Training epoch: 38 [0/900 (0%)]\tLoss: 0.020564\n",
      "Training epoch: 38 [160/900 (18%)]\tLoss: 0.010723\n",
      "Training epoch: 38 [320/900 (35%)]\tLoss: 0.011373\n",
      "Training epoch: 38 [480/900 (53%)]\tLoss: 0.011621\n",
      "Training epoch: 38 [640/900 (70%)]\tLoss: 0.007358\n",
      "Training epoch: 38 [800/900 (88%)]\tLoss: 0.006093\n",
      "=========> Epoch: 38 Average loss: 0.0128\n",
      "Correlation coefficient: 0.7111\n",
      "Training epoch: 39 [0/900 (0%)]\tLoss: 0.005146\n",
      "Training epoch: 39 [160/900 (18%)]\tLoss: 0.007916\n",
      "Training epoch: 39 [320/900 (35%)]\tLoss: 0.008092\n",
      "Training epoch: 39 [480/900 (53%)]\tLoss: 0.004061\n",
      "Training epoch: 39 [640/900 (70%)]\tLoss: 0.003618\n",
      "Training epoch: 39 [800/900 (88%)]\tLoss: 0.004862\n",
      "=========> Epoch: 39 Average loss: 0.0075\n",
      "Correlation coefficient: 0.7172\n",
      "✅ Epoch 39: New best correlation = 0.7172\n",
      "Training epoch: 40 [0/900 (0%)]\tLoss: 0.012062\n",
      "Training epoch: 40 [160/900 (18%)]\tLoss: 0.010342\n",
      "Training epoch: 40 [320/900 (35%)]\tLoss: 0.002794\n",
      "Training epoch: 40 [480/900 (53%)]\tLoss: 0.004358\n",
      "Training epoch: 40 [640/900 (70%)]\tLoss: 0.006325\n",
      "Training epoch: 40 [800/900 (88%)]\tLoss: 0.003206\n",
      "=========> Epoch: 40 Average loss: 0.0063\n",
      "Correlation coefficient: 0.7155\n",
      "Training epoch: 41 [0/900 (0%)]\tLoss: 0.006277\n",
      "Training epoch: 41 [160/900 (18%)]\tLoss: 0.004735\n",
      "Training epoch: 41 [320/900 (35%)]\tLoss: 0.002216\n",
      "Training epoch: 41 [480/900 (53%)]\tLoss: 0.003830\n",
      "Training epoch: 41 [640/900 (70%)]\tLoss: 0.005848\n",
      "Training epoch: 41 [800/900 (88%)]\tLoss: 0.005013\n",
      "=========> Epoch: 41 Average loss: 0.0061\n",
      "Correlation coefficient: 0.7186\n",
      "✅ Epoch 41: New best correlation = 0.7186\n",
      "Training epoch: 42 [0/900 (0%)]\tLoss: 0.001465\n",
      "Training epoch: 42 [160/900 (18%)]\tLoss: 0.003156\n",
      "Training epoch: 42 [320/900 (35%)]\tLoss: 0.001683\n",
      "Training epoch: 42 [480/900 (53%)]\tLoss: 0.004762\n",
      "Training epoch: 42 [640/900 (70%)]\tLoss: 0.001744\n",
      "Training epoch: 42 [800/900 (88%)]\tLoss: 0.002451\n",
      "=========> Epoch: 42 Average loss: 0.0048\n",
      "Correlation coefficient: 0.7124\n",
      "Training epoch: 43 [0/900 (0%)]\tLoss: 0.007029\n",
      "Training epoch: 43 [160/900 (18%)]\tLoss: 0.014194\n",
      "Training epoch: 43 [320/900 (35%)]\tLoss: 0.001447\n",
      "Training epoch: 43 [480/900 (53%)]\tLoss: 0.004351\n",
      "Training epoch: 43 [640/900 (70%)]\tLoss: 0.001857\n",
      "Training epoch: 43 [800/900 (88%)]\tLoss: 0.002640\n",
      "=========> Epoch: 43 Average loss: 0.0054\n",
      "Correlation coefficient: 0.7198\n",
      "✅ Epoch 43: New best correlation = 0.7198\n",
      "Training epoch: 44 [0/900 (0%)]\tLoss: 0.001956\n",
      "Training epoch: 44 [160/900 (18%)]\tLoss: 0.006069\n",
      "Training epoch: 44 [320/900 (35%)]\tLoss: 0.008577\n",
      "Training epoch: 44 [480/900 (53%)]\tLoss: 0.008112\n",
      "Training epoch: 44 [640/900 (70%)]\tLoss: 0.003035\n",
      "Training epoch: 44 [800/900 (88%)]\tLoss: 0.004086\n",
      "=========> Epoch: 44 Average loss: 0.0053\n",
      "Correlation coefficient: 0.7152\n",
      "Training epoch: 45 [0/900 (0%)]\tLoss: 0.005799\n",
      "Training epoch: 45 [160/900 (18%)]\tLoss: 0.002890\n",
      "Training epoch: 45 [320/900 (35%)]\tLoss: 0.003941\n",
      "Training epoch: 45 [480/900 (53%)]\tLoss: 0.003174\n",
      "Training epoch: 45 [640/900 (70%)]\tLoss: 0.001666\n",
      "Training epoch: 45 [800/900 (88%)]\tLoss: 0.006917\n",
      "=========> Epoch: 45 Average loss: 0.0045\n",
      "Correlation coefficient: 0.7077\n",
      "Training epoch: 46 [0/900 (0%)]\tLoss: 0.006376\n",
      "Training epoch: 46 [160/900 (18%)]\tLoss: 0.003098\n",
      "Training epoch: 46 [320/900 (35%)]\tLoss: 0.003930\n",
      "Training epoch: 46 [480/900 (53%)]\tLoss: 0.004655\n",
      "Training epoch: 46 [640/900 (70%)]\tLoss: 0.004167\n",
      "Training epoch: 46 [800/900 (88%)]\tLoss: 0.003948\n",
      "=========> Epoch: 46 Average loss: 0.0060\n",
      "Correlation coefficient: 0.7119\n",
      "Training epoch: 47 [0/900 (0%)]\tLoss: 0.010954\n",
      "Training epoch: 47 [160/900 (18%)]\tLoss: 0.003276\n",
      "Training epoch: 47 [320/900 (35%)]\tLoss: 0.013130\n",
      "Training epoch: 47 [480/900 (53%)]\tLoss: 0.004897\n",
      "Training epoch: 47 [640/900 (70%)]\tLoss: 0.007272\n",
      "Training epoch: 47 [800/900 (88%)]\tLoss: 0.006828\n",
      "=========> Epoch: 47 Average loss: 0.0077\n",
      "Correlation coefficient: 0.7061\n",
      "Training epoch: 48 [0/900 (0%)]\tLoss: 0.005190\n",
      "Training epoch: 48 [160/900 (18%)]\tLoss: 0.010703\n",
      "Training epoch: 48 [320/900 (35%)]\tLoss: 0.004720\n",
      "Training epoch: 48 [480/900 (53%)]\tLoss: 0.006725\n",
      "Training epoch: 48 [640/900 (70%)]\tLoss: 0.005393\n",
      "Training epoch: 48 [800/900 (88%)]\tLoss: 0.014709\n",
      "=========> Epoch: 48 Average loss: 0.0081\n",
      "Correlation coefficient: 0.7216\n",
      "✅ Epoch 48: New best correlation = 0.7216\n",
      "Training epoch: 49 [0/900 (0%)]\tLoss: 0.006335\n",
      "Training epoch: 49 [160/900 (18%)]\tLoss: 0.005090\n",
      "Training epoch: 49 [320/900 (35%)]\tLoss: 0.004483\n",
      "Training epoch: 49 [480/900 (53%)]\tLoss: 0.011773\n",
      "Training epoch: 49 [640/900 (70%)]\tLoss: 0.020833\n",
      "Training epoch: 49 [800/900 (88%)]\tLoss: 0.014543\n",
      "=========> Epoch: 49 Average loss: 0.0096\n",
      "Correlation coefficient: 0.7104\n",
      "Training epoch: 50 [0/900 (0%)]\tLoss: 0.038383\n",
      "Training epoch: 50 [160/900 (18%)]\tLoss: 0.007635\n",
      "Training epoch: 50 [320/900 (35%)]\tLoss: 0.008264\n",
      "Training epoch: 50 [480/900 (53%)]\tLoss: 0.010294\n",
      "Training epoch: 50 [640/900 (70%)]\tLoss: 0.017992\n",
      "Training epoch: 50 [800/900 (88%)]\tLoss: 0.007016\n",
      "=========> Epoch: 50 Average loss: 0.0107\n",
      "Correlation coefficient: 0.7153\n",
      "Training epoch: 51 [0/900 (0%)]\tLoss: 0.011485\n",
      "Training epoch: 51 [160/900 (18%)]\tLoss: 0.005177\n",
      "Training epoch: 51 [320/900 (35%)]\tLoss: 0.016452\n",
      "Training epoch: 51 [480/900 (53%)]\tLoss: 0.003536\n",
      "Training epoch: 51 [640/900 (70%)]\tLoss: 0.005280\n",
      "Training epoch: 51 [800/900 (88%)]\tLoss: 0.010326\n",
      "=========> Epoch: 51 Average loss: 0.0109\n",
      "Correlation coefficient: 0.7187\n",
      "Training epoch: 52 [0/900 (0%)]\tLoss: 0.007051\n",
      "Training epoch: 52 [160/900 (18%)]\tLoss: 0.006737\n",
      "Training epoch: 52 [320/900 (35%)]\tLoss: 0.012983\n",
      "Training epoch: 52 [480/900 (53%)]\tLoss: 0.013380\n",
      "Training epoch: 52 [640/900 (70%)]\tLoss: 0.014244\n",
      "Training epoch: 52 [800/900 (88%)]\tLoss: 0.014381\n",
      "=========> Epoch: 52 Average loss: 0.0122\n",
      "Correlation coefficient: 0.7056\n",
      "Training epoch: 53 [0/900 (0%)]\tLoss: 0.010027\n",
      "Training epoch: 53 [160/900 (18%)]\tLoss: 0.022169\n",
      "Training epoch: 53 [320/900 (35%)]\tLoss: 0.005743\n",
      "Training epoch: 53 [480/900 (53%)]\tLoss: 0.013792\n",
      "Training epoch: 53 [640/900 (70%)]\tLoss: 0.014073\n",
      "Training epoch: 53 [800/900 (88%)]\tLoss: 0.005700\n",
      "=========> Epoch: 53 Average loss: 0.0163\n",
      "Correlation coefficient: 0.7204\n",
      "Training epoch: 54 [0/900 (0%)]\tLoss: 0.018467\n",
      "Training epoch: 54 [160/900 (18%)]\tLoss: 0.012068\n",
      "Training epoch: 54 [320/900 (35%)]\tLoss: 0.009338\n",
      "Training epoch: 54 [480/900 (53%)]\tLoss: 0.012065\n",
      "Training epoch: 54 [640/900 (70%)]\tLoss: 0.016308\n",
      "Training epoch: 54 [800/900 (88%)]\tLoss: 0.016766\n",
      "=========> Epoch: 54 Average loss: 0.0130\n",
      "Correlation coefficient: 0.7127\n",
      "Training epoch: 55 [0/900 (0%)]\tLoss: 0.006477\n",
      "Training epoch: 55 [160/900 (18%)]\tLoss: 0.005006\n",
      "Training epoch: 55 [320/900 (35%)]\tLoss: 0.010894\n",
      "Training epoch: 55 [480/900 (53%)]\tLoss: 0.005089\n",
      "Training epoch: 55 [640/900 (70%)]\tLoss: 0.005014\n",
      "Training epoch: 55 [800/900 (88%)]\tLoss: 0.006181\n",
      "=========> Epoch: 55 Average loss: 0.0072\n",
      "Correlation coefficient: 0.7030\n",
      "Training epoch: 56 [0/900 (0%)]\tLoss: 0.003433\n",
      "Training epoch: 56 [160/900 (18%)]\tLoss: 0.005494\n",
      "Training epoch: 56 [320/900 (35%)]\tLoss: 0.004186\n",
      "Training epoch: 56 [480/900 (53%)]\tLoss: 0.002409\n",
      "Training epoch: 56 [640/900 (70%)]\tLoss: 0.005236\n",
      "Training epoch: 56 [800/900 (88%)]\tLoss: 0.007941\n",
      "=========> Epoch: 56 Average loss: 0.0064\n",
      "Correlation coefficient: 0.7087\n",
      "Training epoch: 57 [0/900 (0%)]\tLoss: 0.004759\n",
      "Training epoch: 57 [160/900 (18%)]\tLoss: 0.007350\n",
      "Training epoch: 57 [320/900 (35%)]\tLoss: 0.005819\n",
      "Training epoch: 57 [480/900 (53%)]\tLoss: 0.004429\n",
      "Training epoch: 57 [640/900 (70%)]\tLoss: 0.003190\n",
      "Training epoch: 57 [800/900 (88%)]\tLoss: 0.004460\n",
      "=========> Epoch: 57 Average loss: 0.0056\n",
      "Correlation coefficient: 0.7138\n",
      "Training epoch: 58 [0/900 (0%)]\tLoss: 0.004794\n",
      "Training epoch: 58 [160/900 (18%)]\tLoss: 0.007272\n",
      "Training epoch: 58 [320/900 (35%)]\tLoss: 0.004328\n",
      "Training epoch: 58 [480/900 (53%)]\tLoss: 0.010809\n",
      "Training epoch: 58 [640/900 (70%)]\tLoss: 0.002455\n",
      "Training epoch: 58 [800/900 (88%)]\tLoss: 0.001890\n",
      "=========> Epoch: 58 Average loss: 0.0049\n",
      "Correlation coefficient: 0.7071\n",
      "Training epoch: 59 [0/900 (0%)]\tLoss: 0.001974\n",
      "Training epoch: 59 [160/900 (18%)]\tLoss: 0.006734\n",
      "Training epoch: 59 [320/900 (35%)]\tLoss: 0.008386\n",
      "Training epoch: 59 [480/900 (53%)]\tLoss: 0.001485\n",
      "Training epoch: 59 [640/900 (70%)]\tLoss: 0.003643\n",
      "Training epoch: 59 [800/900 (88%)]\tLoss: 0.011016\n",
      "=========> Epoch: 59 Average loss: 0.0047\n",
      "Correlation coefficient: 0.7112\n",
      "Training epoch: 60 [0/900 (0%)]\tLoss: 0.003643\n",
      "Training epoch: 60 [160/900 (18%)]\tLoss: 0.005972\n",
      "Training epoch: 60 [320/900 (35%)]\tLoss: 0.004428\n",
      "Training epoch: 60 [480/900 (53%)]\tLoss: 0.008053\n",
      "Training epoch: 60 [640/900 (70%)]\tLoss: 0.006262\n",
      "Training epoch: 60 [800/900 (88%)]\tLoss: 0.000807\n",
      "=========> Epoch: 60 Average loss: 0.0035\n",
      "Correlation coefficient: 0.7181\n",
      "Training epoch: 61 [0/900 (0%)]\tLoss: 0.005601\n",
      "Training epoch: 61 [160/900 (18%)]\tLoss: 0.003573\n",
      "Training epoch: 61 [320/900 (35%)]\tLoss: 0.001894\n",
      "Training epoch: 61 [480/900 (53%)]\tLoss: 0.001890\n",
      "Training epoch: 61 [640/900 (70%)]\tLoss: 0.002188\n",
      "Training epoch: 61 [800/900 (88%)]\tLoss: 0.001085\n",
      "=========> Epoch: 61 Average loss: 0.0033\n",
      "Correlation coefficient: 0.7110\n",
      "Training epoch: 62 [0/900 (0%)]\tLoss: 0.002263\n",
      "Training epoch: 62 [160/900 (18%)]\tLoss: 0.002181\n",
      "Training epoch: 62 [320/900 (35%)]\tLoss: 0.002813\n",
      "Training epoch: 62 [480/900 (53%)]\tLoss: 0.001037\n",
      "Training epoch: 62 [640/900 (70%)]\tLoss: 0.000879\n",
      "Training epoch: 62 [800/900 (88%)]\tLoss: 0.001618\n",
      "=========> Epoch: 62 Average loss: 0.0027\n",
      "Correlation coefficient: 0.7154\n",
      "Training epoch: 63 [0/900 (0%)]\tLoss: 0.001984\n",
      "Training epoch: 63 [160/900 (18%)]\tLoss: 0.003158\n",
      "Training epoch: 63 [320/900 (35%)]\tLoss: 0.004403\n",
      "Training epoch: 63 [480/900 (53%)]\tLoss: 0.000955\n",
      "Training epoch: 63 [640/900 (70%)]\tLoss: 0.004238\n",
      "Training epoch: 63 [800/900 (88%)]\tLoss: 0.002375\n",
      "=========> Epoch: 63 Average loss: 0.0049\n",
      "Correlation coefficient: 0.7146\n",
      "Training epoch: 64 [0/900 (0%)]\tLoss: 0.003340\n",
      "Training epoch: 64 [160/900 (18%)]\tLoss: 0.003622\n",
      "Training epoch: 64 [320/900 (35%)]\tLoss: 0.009018\n",
      "Training epoch: 64 [480/900 (53%)]\tLoss: 0.015827\n",
      "Training epoch: 64 [640/900 (70%)]\tLoss: 0.004940\n",
      "Training epoch: 64 [800/900 (88%)]\tLoss: 0.001347\n",
      "=========> Epoch: 64 Average loss: 0.0078\n",
      "Correlation coefficient: 0.7199\n",
      "Training epoch: 65 [0/900 (0%)]\tLoss: 0.006929\n",
      "Training epoch: 65 [160/900 (18%)]\tLoss: 0.003976\n",
      "Training epoch: 65 [320/900 (35%)]\tLoss: 0.006474\n",
      "Training epoch: 65 [480/900 (53%)]\tLoss: 0.016185\n",
      "Training epoch: 65 [640/900 (70%)]\tLoss: 0.008160\n",
      "Training epoch: 65 [800/900 (88%)]\tLoss: 0.003990\n",
      "=========> Epoch: 65 Average loss: 0.0059\n",
      "Correlation coefficient: 0.7200\n",
      "Training epoch: 66 [0/900 (0%)]\tLoss: 0.001348\n",
      "Training epoch: 66 [160/900 (18%)]\tLoss: 0.006768\n",
      "Training epoch: 66 [320/900 (35%)]\tLoss: 0.005556\n",
      "Training epoch: 66 [480/900 (53%)]\tLoss: 0.002492\n",
      "Training epoch: 66 [640/900 (70%)]\tLoss: 0.003225\n",
      "Training epoch: 66 [800/900 (88%)]\tLoss: 0.002196\n",
      "=========> Epoch: 66 Average loss: 0.0044\n",
      "Correlation coefficient: 0.7166\n",
      "Training epoch: 67 [0/900 (0%)]\tLoss: 0.003688\n",
      "Training epoch: 67 [160/900 (18%)]\tLoss: 0.002840\n",
      "Training epoch: 67 [320/900 (35%)]\tLoss: 0.001777\n",
      "Training epoch: 67 [480/900 (53%)]\tLoss: 0.003031\n",
      "Training epoch: 67 [640/900 (70%)]\tLoss: 0.001739\n",
      "Training epoch: 67 [800/900 (88%)]\tLoss: 0.002059\n",
      "=========> Epoch: 67 Average loss: 0.0038\n",
      "Correlation coefficient: 0.7166\n",
      "Training epoch: 68 [0/900 (0%)]\tLoss: 0.007481\n",
      "Training epoch: 68 [160/900 (18%)]\tLoss: 0.004599\n",
      "Training epoch: 68 [320/900 (35%)]\tLoss: 0.009050\n",
      "Training epoch: 68 [480/900 (53%)]\tLoss: 0.005930\n",
      "Training epoch: 68 [640/900 (70%)]\tLoss: 0.002270\n",
      "Training epoch: 68 [800/900 (88%)]\tLoss: 0.002367\n",
      "=========> Epoch: 68 Average loss: 0.0044\n",
      "Correlation coefficient: 0.7199\n",
      "Training epoch: 69 [0/900 (0%)]\tLoss: 0.006647\n",
      "Training epoch: 69 [160/900 (18%)]\tLoss: 0.005530\n",
      "Training epoch: 69 [320/900 (35%)]\tLoss: 0.003809\n",
      "Training epoch: 69 [480/900 (53%)]\tLoss: 0.004156\n",
      "Training epoch: 69 [640/900 (70%)]\tLoss: 0.004715\n",
      "Training epoch: 69 [800/900 (88%)]\tLoss: 0.002730\n",
      "=========> Epoch: 69 Average loss: 0.0050\n",
      "Correlation coefficient: 0.7079\n",
      "Training epoch: 70 [0/900 (0%)]\tLoss: 0.002783\n",
      "Training epoch: 70 [160/900 (18%)]\tLoss: 0.003594\n",
      "Training epoch: 70 [320/900 (35%)]\tLoss: 0.002193\n",
      "Training epoch: 70 [480/900 (53%)]\tLoss: 0.005997\n",
      "Training epoch: 70 [640/900 (70%)]\tLoss: 0.010492\n",
      "Training epoch: 70 [800/900 (88%)]\tLoss: 0.017742\n",
      "=========> Epoch: 70 Average loss: 0.0037\n",
      "Correlation coefficient: 0.7198\n",
      "Training epoch: 71 [0/900 (0%)]\tLoss: 0.001808\n",
      "Training epoch: 71 [160/900 (18%)]\tLoss: 0.004685\n",
      "Training epoch: 71 [320/900 (35%)]\tLoss: 0.005773\n",
      "Training epoch: 71 [480/900 (53%)]\tLoss: 0.002918\n",
      "Training epoch: 71 [640/900 (70%)]\tLoss: 0.001602\n",
      "Training epoch: 71 [800/900 (88%)]\tLoss: 0.003555\n",
      "=========> Epoch: 71 Average loss: 0.0042\n",
      "Correlation coefficient: 0.7084\n",
      "Training epoch: 72 [0/900 (0%)]\tLoss: 0.004212\n",
      "Training epoch: 72 [160/900 (18%)]\tLoss: 0.003619\n",
      "Training epoch: 72 [320/900 (35%)]\tLoss: 0.008285\n",
      "Training epoch: 72 [480/900 (53%)]\tLoss: 0.006144\n",
      "Training epoch: 72 [640/900 (70%)]\tLoss: 0.001524\n",
      "Training epoch: 72 [800/900 (88%)]\tLoss: 0.005852\n",
      "=========> Epoch: 72 Average loss: 0.0064\n",
      "Correlation coefficient: 0.7144\n",
      "Training epoch: 73 [0/900 (0%)]\tLoss: 0.009329\n",
      "Training epoch: 73 [160/900 (18%)]\tLoss: 0.017140\n",
      "Training epoch: 73 [320/900 (35%)]\tLoss: 0.004083\n",
      "Training epoch: 73 [480/900 (53%)]\tLoss: 0.004214\n",
      "Training epoch: 73 [640/900 (70%)]\tLoss: 0.008417\n",
      "Training epoch: 73 [800/900 (88%)]\tLoss: 0.009772\n",
      "=========> Epoch: 73 Average loss: 0.0090\n",
      "Correlation coefficient: 0.7195\n",
      "Training epoch: 74 [0/900 (0%)]\tLoss: 0.008279\n",
      "Training epoch: 74 [160/900 (18%)]\tLoss: 0.015088\n",
      "Training epoch: 74 [320/900 (35%)]\tLoss: 0.008758\n",
      "Training epoch: 74 [480/900 (53%)]\tLoss: 0.010725\n",
      "Training epoch: 74 [640/900 (70%)]\tLoss: 0.018386\n",
      "Training epoch: 74 [800/900 (88%)]\tLoss: 0.035712\n",
      "=========> Epoch: 74 Average loss: 0.0173\n",
      "Correlation coefficient: 0.7138\n",
      "Training epoch: 75 [0/900 (0%)]\tLoss: 0.023412\n",
      "Training epoch: 75 [160/900 (18%)]\tLoss: 0.039565\n",
      "Training epoch: 75 [320/900 (35%)]\tLoss: 0.021050\n",
      "Training epoch: 75 [480/900 (53%)]\tLoss: 0.016058\n",
      "Training epoch: 75 [640/900 (70%)]\tLoss: 0.041334\n",
      "Training epoch: 75 [800/900 (88%)]\tLoss: 0.028808\n",
      "=========> Epoch: 75 Average loss: 0.0223\n",
      "Correlation coefficient: 0.7021\n",
      "Training epoch: 76 [0/900 (0%)]\tLoss: 0.021125\n",
      "Training epoch: 76 [160/900 (18%)]\tLoss: 0.017540\n",
      "Training epoch: 76 [320/900 (35%)]\tLoss: 0.028852\n",
      "Training epoch: 76 [480/900 (53%)]\tLoss: 0.110022\n",
      "Training epoch: 76 [640/900 (70%)]\tLoss: 0.023927\n",
      "Training epoch: 76 [800/900 (88%)]\tLoss: 0.023051\n",
      "=========> Epoch: 76 Average loss: 0.0227\n",
      "Correlation coefficient: 0.7183\n",
      "Training epoch: 77 [0/900 (0%)]\tLoss: 0.043739\n",
      "Training epoch: 77 [160/900 (18%)]\tLoss: 0.020951\n",
      "Training epoch: 77 [320/900 (35%)]\tLoss: 0.015728\n",
      "Training epoch: 77 [480/900 (53%)]\tLoss: 0.006174\n",
      "Training epoch: 77 [640/900 (70%)]\tLoss: 0.006737\n",
      "Training epoch: 77 [800/900 (88%)]\tLoss: 0.007001\n",
      "=========> Epoch: 77 Average loss: 0.0241\n",
      "Correlation coefficient: 0.6930\n",
      "Training epoch: 78 [0/900 (0%)]\tLoss: 0.024746\n",
      "Training epoch: 78 [160/900 (18%)]\tLoss: 0.010220\n",
      "Training epoch: 78 [320/900 (35%)]\tLoss: 0.013472\n",
      "Training epoch: 78 [480/900 (53%)]\tLoss: 0.037535\n",
      "Training epoch: 78 [640/900 (70%)]\tLoss: 0.023013\n",
      "Training epoch: 78 [800/900 (88%)]\tLoss: 0.020384\n",
      "=========> Epoch: 78 Average loss: 0.0204\n",
      "Correlation coefficient: 0.6904\n",
      "Training epoch: 79 [0/900 (0%)]\tLoss: 0.026028\n",
      "Training epoch: 79 [160/900 (18%)]\tLoss: 0.023787\n",
      "Training epoch: 79 [320/900 (35%)]\tLoss: 0.020305\n",
      "Training epoch: 79 [480/900 (53%)]\tLoss: 0.010977\n",
      "Training epoch: 79 [640/900 (70%)]\tLoss: 0.009301\n",
      "Training epoch: 79 [800/900 (88%)]\tLoss: 0.011905\n",
      "=========> Epoch: 79 Average loss: 0.0142\n",
      "Correlation coefficient: 0.7121\n",
      "Training epoch: 80 [0/900 (0%)]\tLoss: 0.009635\n",
      "Training epoch: 80 [160/900 (18%)]\tLoss: 0.014449\n",
      "Training epoch: 80 [320/900 (35%)]\tLoss: 0.010485\n",
      "Training epoch: 80 [480/900 (53%)]\tLoss: 0.013093\n",
      "Training epoch: 80 [640/900 (70%)]\tLoss: 0.011377\n",
      "Training epoch: 80 [800/900 (88%)]\tLoss: 0.016224\n",
      "=========> Epoch: 80 Average loss: 0.0188\n",
      "Correlation coefficient: 0.7137\n",
      "Training epoch: 81 [0/900 (0%)]\tLoss: 0.015969\n",
      "Training epoch: 81 [160/900 (18%)]\tLoss: 0.010674\n",
      "Training epoch: 81 [320/900 (35%)]\tLoss: 0.041056\n",
      "Training epoch: 81 [480/900 (53%)]\tLoss: 0.008593\n",
      "Training epoch: 81 [640/900 (70%)]\tLoss: 0.029356\n",
      "Training epoch: 81 [800/900 (88%)]\tLoss: 0.012729\n",
      "=========> Epoch: 81 Average loss: 0.0212\n",
      "Correlation coefficient: 0.7150\n",
      "Training epoch: 82 [0/900 (0%)]\tLoss: 0.010292\n",
      "Training epoch: 82 [160/900 (18%)]\tLoss: 0.015240\n",
      "Training epoch: 82 [320/900 (35%)]\tLoss: 0.010322\n",
      "Training epoch: 82 [480/900 (53%)]\tLoss: 0.017930\n",
      "Training epoch: 82 [640/900 (70%)]\tLoss: 0.014780\n",
      "Training epoch: 82 [800/900 (88%)]\tLoss: 0.015403\n",
      "=========> Epoch: 82 Average loss: 0.0156\n",
      "Correlation coefficient: 0.7113\n",
      "Training epoch: 83 [0/900 (0%)]\tLoss: 0.004150\n",
      "Training epoch: 83 [160/900 (18%)]\tLoss: 0.003208\n",
      "Training epoch: 83 [320/900 (35%)]\tLoss: 0.007769\n",
      "Training epoch: 83 [480/900 (53%)]\tLoss: 0.009442\n",
      "Training epoch: 83 [640/900 (70%)]\tLoss: 0.018263\n",
      "Training epoch: 83 [800/900 (88%)]\tLoss: 0.019681\n",
      "=========> Epoch: 83 Average loss: 0.0105\n",
      "Correlation coefficient: 0.7108\n",
      "Training epoch: 84 [0/900 (0%)]\tLoss: 0.003499\n",
      "Training epoch: 84 [160/900 (18%)]\tLoss: 0.004711\n",
      "Training epoch: 84 [320/900 (35%)]\tLoss: 0.005021\n",
      "Training epoch: 84 [480/900 (53%)]\tLoss: 0.003462\n",
      "Training epoch: 84 [640/900 (70%)]\tLoss: 0.004230\n",
      "Training epoch: 84 [800/900 (88%)]\tLoss: 0.003732\n",
      "=========> Epoch: 84 Average loss: 0.0061\n",
      "Correlation coefficient: 0.7076\n",
      "Training epoch: 85 [0/900 (0%)]\tLoss: 0.002988\n",
      "Training epoch: 85 [160/900 (18%)]\tLoss: 0.003402\n",
      "Training epoch: 85 [320/900 (35%)]\tLoss: 0.002960\n",
      "Training epoch: 85 [480/900 (53%)]\tLoss: 0.002460\n",
      "Training epoch: 85 [640/900 (70%)]\tLoss: 0.002139\n",
      "Training epoch: 85 [800/900 (88%)]\tLoss: 0.007323\n",
      "=========> Epoch: 85 Average loss: 0.0031\n",
      "Correlation coefficient: 0.7124\n",
      "Training epoch: 86 [0/900 (0%)]\tLoss: 0.004059\n",
      "Training epoch: 86 [160/900 (18%)]\tLoss: 0.000809\n",
      "Training epoch: 86 [320/900 (35%)]\tLoss: 0.000906\n",
      "Training epoch: 86 [480/900 (53%)]\tLoss: 0.001459\n",
      "Training epoch: 86 [640/900 (70%)]\tLoss: 0.001564\n",
      "Training epoch: 86 [800/900 (88%)]\tLoss: 0.001119\n",
      "=========> Epoch: 86 Average loss: 0.0016\n",
      "Correlation coefficient: 0.7071\n",
      "Training epoch: 87 [0/900 (0%)]\tLoss: 0.002575\n",
      "Training epoch: 87 [160/900 (18%)]\tLoss: 0.002128\n",
      "Training epoch: 87 [320/900 (35%)]\tLoss: 0.001061\n",
      "Training epoch: 87 [480/900 (53%)]\tLoss: 0.000903\n",
      "Training epoch: 87 [640/900 (70%)]\tLoss: 0.001788\n",
      "Training epoch: 87 [800/900 (88%)]\tLoss: 0.001085\n",
      "=========> Epoch: 87 Average loss: 0.0015\n",
      "Correlation coefficient: 0.7123\n",
      "Training epoch: 88 [0/900 (0%)]\tLoss: 0.000474\n",
      "Training epoch: 88 [160/900 (18%)]\tLoss: 0.001081\n",
      "Training epoch: 88 [320/900 (35%)]\tLoss: 0.001632\n",
      "Training epoch: 88 [480/900 (53%)]\tLoss: 0.001063\n",
      "Training epoch: 88 [640/900 (70%)]\tLoss: 0.000743\n",
      "Training epoch: 88 [800/900 (88%)]\tLoss: 0.001386\n",
      "=========> Epoch: 88 Average loss: 0.0015\n",
      "Correlation coefficient: 0.7080\n",
      "Training epoch: 89 [0/900 (0%)]\tLoss: 0.000581\n",
      "Training epoch: 89 [160/900 (18%)]\tLoss: 0.001678\n",
      "Training epoch: 89 [320/900 (35%)]\tLoss: 0.000511\n",
      "Training epoch: 89 [480/900 (53%)]\tLoss: 0.000929\n",
      "Training epoch: 89 [640/900 (70%)]\tLoss: 0.001508\n",
      "Training epoch: 89 [800/900 (88%)]\tLoss: 0.001332\n",
      "=========> Epoch: 89 Average loss: 0.0010\n",
      "Correlation coefficient: 0.7111\n",
      "Training epoch: 90 [0/900 (0%)]\tLoss: 0.000422\n",
      "Training epoch: 90 [160/900 (18%)]\tLoss: 0.000780\n",
      "Training epoch: 90 [320/900 (35%)]\tLoss: 0.000319\n",
      "Training epoch: 90 [480/900 (53%)]\tLoss: 0.000536\n",
      "Training epoch: 90 [640/900 (70%)]\tLoss: 0.000503\n",
      "Training epoch: 90 [800/900 (88%)]\tLoss: 0.000679\n",
      "=========> Epoch: 90 Average loss: 0.0007\n",
      "Correlation coefficient: 0.7088\n",
      "Training epoch: 91 [0/900 (0%)]\tLoss: 0.001306\n",
      "Training epoch: 91 [160/900 (18%)]\tLoss: 0.000991\n",
      "Training epoch: 91 [320/900 (35%)]\tLoss: 0.000454\n",
      "Training epoch: 91 [480/900 (53%)]\tLoss: 0.000240\n",
      "Training epoch: 91 [640/900 (70%)]\tLoss: 0.000142\n",
      "Training epoch: 91 [800/900 (88%)]\tLoss: 0.000649\n",
      "=========> Epoch: 91 Average loss: 0.0006\n",
      "Correlation coefficient: 0.7098\n",
      "Training epoch: 92 [0/900 (0%)]\tLoss: 0.000880\n",
      "Training epoch: 92 [160/900 (18%)]\tLoss: 0.001046\n",
      "Training epoch: 92 [320/900 (35%)]\tLoss: 0.000466\n",
      "Training epoch: 92 [480/900 (53%)]\tLoss: 0.001290\n",
      "Training epoch: 92 [640/900 (70%)]\tLoss: 0.001369\n",
      "Training epoch: 92 [800/900 (88%)]\tLoss: 0.000320\n",
      "=========> Epoch: 92 Average loss: 0.0006\n",
      "Correlation coefficient: 0.7101\n",
      "Training epoch: 93 [0/900 (0%)]\tLoss: 0.000357\n",
      "Training epoch: 93 [160/900 (18%)]\tLoss: 0.000128\n",
      "Training epoch: 93 [320/900 (35%)]\tLoss: 0.000310\n",
      "Training epoch: 93 [480/900 (53%)]\tLoss: 0.000214\n",
      "Training epoch: 93 [640/900 (70%)]\tLoss: 0.000316\n",
      "Training epoch: 93 [800/900 (88%)]\tLoss: 0.000532\n",
      "=========> Epoch: 93 Average loss: 0.0004\n",
      "Correlation coefficient: 0.7091\n",
      "Training epoch: 94 [0/900 (0%)]\tLoss: 0.000430\n",
      "Training epoch: 94 [160/900 (18%)]\tLoss: 0.000068\n",
      "Training epoch: 94 [320/900 (35%)]\tLoss: 0.000217\n",
      "Training epoch: 94 [480/900 (53%)]\tLoss: 0.000354\n",
      "Training epoch: 94 [640/900 (70%)]\tLoss: 0.000489\n",
      "Training epoch: 94 [800/900 (88%)]\tLoss: 0.000216\n",
      "=========> Epoch: 94 Average loss: 0.0004\n",
      "Correlation coefficient: 0.7121\n",
      "Training epoch: 95 [0/900 (0%)]\tLoss: 0.000844\n",
      "Training epoch: 95 [160/900 (18%)]\tLoss: 0.000792\n",
      "Training epoch: 95 [320/900 (35%)]\tLoss: 0.000443\n",
      "Training epoch: 95 [480/900 (53%)]\tLoss: 0.000577\n",
      "Training epoch: 95 [640/900 (70%)]\tLoss: 0.000286\n",
      "Training epoch: 95 [800/900 (88%)]\tLoss: 0.000786\n",
      "=========> Epoch: 95 Average loss: 0.0005\n",
      "Correlation coefficient: 0.7091\n",
      "Training epoch: 96 [0/900 (0%)]\tLoss: 0.001111\n",
      "Training epoch: 96 [160/900 (18%)]\tLoss: 0.000454\n",
      "Training epoch: 96 [320/900 (35%)]\tLoss: 0.000597\n",
      "Training epoch: 96 [480/900 (53%)]\tLoss: 0.001375\n",
      "Training epoch: 96 [640/900 (70%)]\tLoss: 0.000592\n",
      "Training epoch: 96 [800/900 (88%)]\tLoss: 0.002672\n",
      "=========> Epoch: 96 Average loss: 0.0007\n",
      "Correlation coefficient: 0.7071\n",
      "Training epoch: 97 [0/900 (0%)]\tLoss: 0.000752\n",
      "Training epoch: 97 [160/900 (18%)]\tLoss: 0.000782\n",
      "Training epoch: 97 [320/900 (35%)]\tLoss: 0.001011\n",
      "Training epoch: 97 [480/900 (53%)]\tLoss: 0.000532\n",
      "Training epoch: 97 [640/900 (70%)]\tLoss: 0.000499\n",
      "Training epoch: 97 [800/900 (88%)]\tLoss: 0.000730\n",
      "=========> Epoch: 97 Average loss: 0.0009\n",
      "Correlation coefficient: 0.7107\n",
      "Training epoch: 98 [0/900 (0%)]\tLoss: 0.000607\n",
      "Training epoch: 98 [160/900 (18%)]\tLoss: 0.000720\n",
      "Training epoch: 98 [320/900 (35%)]\tLoss: 0.000859\n",
      "Training epoch: 98 [480/900 (53%)]\tLoss: 0.002750\n",
      "Training epoch: 98 [640/900 (70%)]\tLoss: 0.001323\n",
      "Training epoch: 98 [800/900 (88%)]\tLoss: 0.003090\n",
      "=========> Epoch: 98 Average loss: 0.0014\n",
      "Correlation coefficient: 0.7121\n",
      "⏹️  Epoch 98 early stopping (no improvement for 50 epochs)\n",
      "🏁 Fold 1 best correlation: 0.7216\n",
      "\n",
      "========== Cross-validation Fold 2/10 ==========\n",
      "🔄 Fold 2: Using random initialization\n",
      "Training epoch: 1 [0/900 (0%)]\tLoss: 0.699245\n",
      "Training epoch: 1 [160/900 (18%)]\tLoss: 0.729108\n",
      "Training epoch: 1 [320/900 (35%)]\tLoss: 0.397495\n",
      "Training epoch: 1 [480/900 (53%)]\tLoss: 0.603523\n",
      "Training epoch: 1 [640/900 (70%)]\tLoss: 0.634822\n",
      "Training epoch: 1 [800/900 (88%)]\tLoss: 0.605637\n",
      "=========> Epoch: 1 Average loss: 0.8044\n",
      "Correlation coefficient: 0.7190\n",
      "✅ Epoch 1: New best correlation = 0.7190\n",
      "Training epoch: 2 [0/900 (0%)]\tLoss: 0.195820\n",
      "Training epoch: 2 [160/900 (18%)]\tLoss: 0.292863\n",
      "Training epoch: 2 [320/900 (35%)]\tLoss: 0.250462\n",
      "Training epoch: 2 [480/900 (53%)]\tLoss: 0.450372\n",
      "Training epoch: 2 [640/900 (70%)]\tLoss: 0.456739\n",
      "Training epoch: 2 [800/900 (88%)]\tLoss: 0.375106\n",
      "=========> Epoch: 2 Average loss: 0.3949\n",
      "Correlation coefficient: 0.7533\n",
      "✅ Epoch 2: New best correlation = 0.7533\n",
      "Training epoch: 3 [0/900 (0%)]\tLoss: 0.194313\n",
      "Training epoch: 3 [160/900 (18%)]\tLoss: 0.219349\n",
      "Training epoch: 3 [320/900 (35%)]\tLoss: 0.103376\n",
      "Training epoch: 3 [480/900 (53%)]\tLoss: 0.206201\n",
      "Training epoch: 3 [640/900 (70%)]\tLoss: 0.191965\n",
      "Training epoch: 3 [800/900 (88%)]\tLoss: 0.058240\n",
      "=========> Epoch: 3 Average loss: 0.1532\n",
      "Correlation coefficient: 0.7501\n",
      "Training epoch: 4 [0/900 (0%)]\tLoss: 0.051394\n",
      "Training epoch: 4 [160/900 (18%)]\tLoss: 0.074910\n",
      "Training epoch: 4 [320/900 (35%)]\tLoss: 0.063962\n",
      "Training epoch: 4 [480/900 (53%)]\tLoss: 0.109247\n",
      "Training epoch: 4 [640/900 (70%)]\tLoss: 0.043045\n",
      "Training epoch: 4 [800/900 (88%)]\tLoss: 0.046648\n",
      "=========> Epoch: 4 Average loss: 0.0623\n",
      "Correlation coefficient: 0.7711\n",
      "✅ Epoch 4: New best correlation = 0.7711\n",
      "Training epoch: 5 [0/900 (0%)]\tLoss: 0.017316\n",
      "Training epoch: 5 [160/900 (18%)]\tLoss: 0.035520\n",
      "Training epoch: 5 [320/900 (35%)]\tLoss: 0.030926\n",
      "Training epoch: 5 [480/900 (53%)]\tLoss: 0.040550\n",
      "Training epoch: 5 [640/900 (70%)]\tLoss: 0.027622\n",
      "Training epoch: 5 [800/900 (88%)]\tLoss: 0.022762\n",
      "=========> Epoch: 5 Average loss: 0.0375\n",
      "Correlation coefficient: 0.7767\n",
      "✅ Epoch 5: New best correlation = 0.7767\n",
      "Training epoch: 6 [0/900 (0%)]\tLoss: 0.024935\n",
      "Training epoch: 6 [160/900 (18%)]\tLoss: 0.040244\n",
      "Training epoch: 6 [320/900 (35%)]\tLoss: 0.019547\n",
      "Training epoch: 6 [480/900 (53%)]\tLoss: 0.013178\n",
      "Training epoch: 6 [640/900 (70%)]\tLoss: 0.015346\n",
      "Training epoch: 6 [800/900 (88%)]\tLoss: 0.011850\n",
      "=========> Epoch: 6 Average loss: 0.0270\n",
      "Correlation coefficient: 0.7663\n",
      "Training epoch: 7 [0/900 (0%)]\tLoss: 0.012829\n",
      "Training epoch: 7 [160/900 (18%)]\tLoss: 0.035640\n",
      "Training epoch: 7 [320/900 (35%)]\tLoss: 0.054766\n",
      "Training epoch: 7 [480/900 (53%)]\tLoss: 0.055322\n",
      "Training epoch: 7 [640/900 (70%)]\tLoss: 0.008911\n",
      "Training epoch: 7 [800/900 (88%)]\tLoss: 0.014195\n",
      "=========> Epoch: 7 Average loss: 0.0178\n",
      "Correlation coefficient: 0.7701\n",
      "Training epoch: 8 [0/900 (0%)]\tLoss: 0.009709\n",
      "Training epoch: 8 [160/900 (18%)]\tLoss: 0.015468\n",
      "Training epoch: 8 [320/900 (35%)]\tLoss: 0.014250\n",
      "Training epoch: 8 [480/900 (53%)]\tLoss: 0.018992\n",
      "Training epoch: 8 [640/900 (70%)]\tLoss: 0.006857\n",
      "Training epoch: 8 [800/900 (88%)]\tLoss: 0.010992\n",
      "=========> Epoch: 8 Average loss: 0.0120\n",
      "Correlation coefficient: 0.7686\n",
      "Training epoch: 9 [0/900 (0%)]\tLoss: 0.002356\n",
      "Training epoch: 9 [160/900 (18%)]\tLoss: 0.003518\n",
      "Training epoch: 9 [320/900 (35%)]\tLoss: 0.002883\n",
      "Training epoch: 9 [480/900 (53%)]\tLoss: 0.003317\n",
      "Training epoch: 9 [640/900 (70%)]\tLoss: 0.005059\n",
      "Training epoch: 9 [800/900 (88%)]\tLoss: 0.006013\n",
      "=========> Epoch: 9 Average loss: 0.0076\n",
      "Correlation coefficient: 0.7744\n",
      "Training epoch: 10 [0/900 (0%)]\tLoss: 0.004669\n",
      "Training epoch: 10 [160/900 (18%)]\tLoss: 0.002508\n",
      "Training epoch: 10 [320/900 (35%)]\tLoss: 0.006582\n",
      "Training epoch: 10 [480/900 (53%)]\tLoss: 0.001165\n",
      "Training epoch: 10 [640/900 (70%)]\tLoss: 0.005786\n",
      "Training epoch: 10 [800/900 (88%)]\tLoss: 0.001958\n",
      "=========> Epoch: 10 Average loss: 0.0048\n",
      "Correlation coefficient: 0.7731\n",
      "Training epoch: 11 [0/900 (0%)]\tLoss: 0.001472\n",
      "Training epoch: 11 [160/900 (18%)]\tLoss: 0.003720\n",
      "Training epoch: 11 [320/900 (35%)]\tLoss: 0.006309\n",
      "Training epoch: 11 [480/900 (53%)]\tLoss: 0.003706\n",
      "Training epoch: 11 [640/900 (70%)]\tLoss: 0.004439\n",
      "Training epoch: 11 [800/900 (88%)]\tLoss: 0.003007\n",
      "=========> Epoch: 11 Average loss: 0.0058\n",
      "Correlation coefficient: 0.7730\n",
      "Training epoch: 12 [0/900 (0%)]\tLoss: 0.004891\n",
      "Training epoch: 12 [160/900 (18%)]\tLoss: 0.003855\n",
      "Training epoch: 12 [320/900 (35%)]\tLoss: 0.008303\n",
      "Training epoch: 12 [480/900 (53%)]\tLoss: 0.003327\n",
      "Training epoch: 12 [640/900 (70%)]\tLoss: 0.004685\n",
      "Training epoch: 12 [800/900 (88%)]\tLoss: 0.004147\n",
      "=========> Epoch: 12 Average loss: 0.0069\n",
      "Correlation coefficient: 0.7739\n",
      "Training epoch: 13 [0/900 (0%)]\tLoss: 0.004006\n",
      "Training epoch: 13 [160/900 (18%)]\tLoss: 0.007316\n",
      "Training epoch: 13 [320/900 (35%)]\tLoss: 0.011846\n",
      "Training epoch: 13 [480/900 (53%)]\tLoss: 0.005959\n",
      "Training epoch: 13 [640/900 (70%)]\tLoss: 0.005711\n",
      "Training epoch: 13 [800/900 (88%)]\tLoss: 0.002929\n",
      "=========> Epoch: 13 Average loss: 0.0069\n",
      "Correlation coefficient: 0.7706\n",
      "Training epoch: 14 [0/900 (0%)]\tLoss: 0.006712\n",
      "Training epoch: 14 [160/900 (18%)]\tLoss: 0.006836\n",
      "Training epoch: 14 [320/900 (35%)]\tLoss: 0.003908\n",
      "Training epoch: 14 [480/900 (53%)]\tLoss: 0.004494\n",
      "Training epoch: 14 [640/900 (70%)]\tLoss: 0.022432\n",
      "Training epoch: 14 [800/900 (88%)]\tLoss: 0.004110\n",
      "=========> Epoch: 14 Average loss: 0.0082\n",
      "Correlation coefficient: 0.7783\n",
      "✅ Epoch 14: New best correlation = 0.7783\n",
      "Training epoch: 15 [0/900 (0%)]\tLoss: 0.007161\n",
      "Training epoch: 15 [160/900 (18%)]\tLoss: 0.014654\n",
      "Training epoch: 15 [320/900 (35%)]\tLoss: 0.013639\n",
      "Training epoch: 15 [480/900 (53%)]\tLoss: 0.012901\n",
      "Training epoch: 15 [640/900 (70%)]\tLoss: 0.007673\n",
      "Training epoch: 15 [800/900 (88%)]\tLoss: 0.008575\n",
      "=========> Epoch: 15 Average loss: 0.0131\n",
      "Correlation coefficient: 0.7683\n",
      "Training epoch: 16 [0/900 (0%)]\tLoss: 0.015964\n",
      "Training epoch: 16 [160/900 (18%)]\tLoss: 0.006551\n",
      "Training epoch: 16 [320/900 (35%)]\tLoss: 0.008737\n",
      "Training epoch: 16 [480/900 (53%)]\tLoss: 0.016591\n",
      "Training epoch: 16 [640/900 (70%)]\tLoss: 0.037708\n",
      "Training epoch: 16 [800/900 (88%)]\tLoss: 0.005393\n",
      "=========> Epoch: 16 Average loss: 0.0176\n",
      "Correlation coefficient: 0.7811\n",
      "✅ Epoch 16: New best correlation = 0.7811\n",
      "Training epoch: 17 [0/900 (0%)]\tLoss: 0.008473\n",
      "Training epoch: 17 [160/900 (18%)]\tLoss: 0.026130\n",
      "Training epoch: 17 [320/900 (35%)]\tLoss: 0.007747\n",
      "Training epoch: 17 [480/900 (53%)]\tLoss: 0.025651\n",
      "Training epoch: 17 [640/900 (70%)]\tLoss: 0.018362\n",
      "Training epoch: 17 [800/900 (88%)]\tLoss: 0.006263\n",
      "=========> Epoch: 17 Average loss: 0.0182\n",
      "Correlation coefficient: 0.7754\n",
      "Training epoch: 18 [0/900 (0%)]\tLoss: 0.010994\n",
      "Training epoch: 18 [160/900 (18%)]\tLoss: 0.013348\n",
      "Training epoch: 18 [320/900 (35%)]\tLoss: 0.010029\n",
      "Training epoch: 18 [480/900 (53%)]\tLoss: 0.020073\n",
      "Training epoch: 18 [640/900 (70%)]\tLoss: 0.010525\n",
      "Training epoch: 18 [800/900 (88%)]\tLoss: 0.012945\n",
      "=========> Epoch: 18 Average loss: 0.0158\n",
      "Correlation coefficient: 0.7790\n",
      "Training epoch: 19 [0/900 (0%)]\tLoss: 0.017335\n",
      "Training epoch: 19 [160/900 (18%)]\tLoss: 0.021631\n",
      "Training epoch: 19 [320/900 (35%)]\tLoss: 0.007994\n",
      "Training epoch: 19 [480/900 (53%)]\tLoss: 0.010135\n",
      "Training epoch: 19 [640/900 (70%)]\tLoss: 0.017451\n",
      "Training epoch: 19 [800/900 (88%)]\tLoss: 0.008845\n",
      "=========> Epoch: 19 Average loss: 0.0153\n",
      "Correlation coefficient: 0.7746\n",
      "Training epoch: 20 [0/900 (0%)]\tLoss: 0.021974\n",
      "Training epoch: 20 [160/900 (18%)]\tLoss: 0.010636\n",
      "Training epoch: 20 [320/900 (35%)]\tLoss: 0.008025\n",
      "Training epoch: 20 [480/900 (53%)]\tLoss: 0.012947\n",
      "Training epoch: 20 [640/900 (70%)]\tLoss: 0.024340\n",
      "Training epoch: 20 [800/900 (88%)]\tLoss: 0.017411\n",
      "=========> Epoch: 20 Average loss: 0.0127\n",
      "Correlation coefficient: 0.7784\n",
      "Training epoch: 21 [0/900 (0%)]\tLoss: 0.009492\n",
      "Training epoch: 21 [160/900 (18%)]\tLoss: 0.009193\n",
      "Training epoch: 21 [320/900 (35%)]\tLoss: 0.004897\n",
      "Training epoch: 21 [480/900 (53%)]\tLoss: 0.013234\n",
      "Training epoch: 21 [640/900 (70%)]\tLoss: 0.007760\n",
      "Training epoch: 21 [800/900 (88%)]\tLoss: 0.004664\n",
      "=========> Epoch: 21 Average loss: 0.0089\n",
      "Correlation coefficient: 0.7739\n",
      "Training epoch: 22 [0/900 (0%)]\tLoss: 0.010240\n",
      "Training epoch: 22 [160/900 (18%)]\tLoss: 0.009854\n",
      "Training epoch: 22 [320/900 (35%)]\tLoss: 0.007859\n",
      "Training epoch: 22 [480/900 (53%)]\tLoss: 0.006811\n",
      "Training epoch: 22 [640/900 (70%)]\tLoss: 0.007223\n",
      "Training epoch: 22 [800/900 (88%)]\tLoss: 0.006429\n",
      "=========> Epoch: 22 Average loss: 0.0081\n",
      "Correlation coefficient: 0.7806\n",
      "Training epoch: 23 [0/900 (0%)]\tLoss: 0.002675\n",
      "Training epoch: 23 [160/900 (18%)]\tLoss: 0.007106\n",
      "Training epoch: 23 [320/900 (35%)]\tLoss: 0.009550\n",
      "Training epoch: 23 [480/900 (53%)]\tLoss: 0.009922\n",
      "Training epoch: 23 [640/900 (70%)]\tLoss: 0.007670\n",
      "Training epoch: 23 [800/900 (88%)]\tLoss: 0.010508\n",
      "=========> Epoch: 23 Average loss: 0.0080\n",
      "Correlation coefficient: 0.7740\n",
      "Training epoch: 24 [0/900 (0%)]\tLoss: 0.012560\n",
      "Training epoch: 24 [160/900 (18%)]\tLoss: 0.016637\n",
      "Training epoch: 24 [320/900 (35%)]\tLoss: 0.006067\n",
      "Training epoch: 24 [480/900 (53%)]\tLoss: 0.010064\n",
      "Training epoch: 24 [640/900 (70%)]\tLoss: 0.006066\n",
      "Training epoch: 24 [800/900 (88%)]\tLoss: 0.015076\n",
      "=========> Epoch: 24 Average loss: 0.0115\n",
      "Correlation coefficient: 0.7748\n",
      "Training epoch: 25 [0/900 (0%)]\tLoss: 0.005082\n",
      "Training epoch: 25 [160/900 (18%)]\tLoss: 0.017936\n",
      "Training epoch: 25 [320/900 (35%)]\tLoss: 0.020761\n",
      "Training epoch: 25 [480/900 (53%)]\tLoss: 0.014275\n",
      "Training epoch: 25 [640/900 (70%)]\tLoss: 0.012249\n",
      "Training epoch: 25 [800/900 (88%)]\tLoss: 0.013484\n",
      "=========> Epoch: 25 Average loss: 0.0174\n",
      "Correlation coefficient: 0.7805\n",
      "Training epoch: 26 [0/900 (0%)]\tLoss: 0.010922\n",
      "Training epoch: 26 [160/900 (18%)]\tLoss: 0.019073\n",
      "Training epoch: 26 [320/900 (35%)]\tLoss: 0.009532\n",
      "Training epoch: 26 [480/900 (53%)]\tLoss: 0.015384\n",
      "Training epoch: 26 [640/900 (70%)]\tLoss: 0.006682\n",
      "Training epoch: 26 [800/900 (88%)]\tLoss: 0.009222\n",
      "=========> Epoch: 26 Average loss: 0.0144\n",
      "Correlation coefficient: 0.7844\n",
      "✅ Epoch 26: New best correlation = 0.7844\n",
      "Training epoch: 27 [0/900 (0%)]\tLoss: 0.004427\n",
      "Training epoch: 27 [160/900 (18%)]\tLoss: 0.012387\n",
      "Training epoch: 27 [320/900 (35%)]\tLoss: 0.024403\n",
      "Training epoch: 27 [480/900 (53%)]\tLoss: 0.011075\n",
      "Training epoch: 27 [640/900 (70%)]\tLoss: 0.015981\n",
      "Training epoch: 27 [800/900 (88%)]\tLoss: 0.016398\n",
      "=========> Epoch: 27 Average loss: 0.0130\n",
      "Correlation coefficient: 0.7801\n",
      "Training epoch: 28 [0/900 (0%)]\tLoss: 0.009969\n",
      "Training epoch: 28 [160/900 (18%)]\tLoss: 0.007098\n",
      "Training epoch: 28 [320/900 (35%)]\tLoss: 0.005301\n",
      "Training epoch: 28 [480/900 (53%)]\tLoss: 0.025463\n",
      "Training epoch: 28 [640/900 (70%)]\tLoss: 0.003618\n",
      "Training epoch: 28 [800/900 (88%)]\tLoss: 0.006796\n",
      "=========> Epoch: 28 Average loss: 0.0138\n",
      "Correlation coefficient: 0.7802\n",
      "Training epoch: 29 [0/900 (0%)]\tLoss: 0.021407\n",
      "Training epoch: 29 [160/900 (18%)]\tLoss: 0.012458\n",
      "Training epoch: 29 [320/900 (35%)]\tLoss: 0.017617\n",
      "Training epoch: 29 [480/900 (53%)]\tLoss: 0.013119\n",
      "Training epoch: 29 [640/900 (70%)]\tLoss: 0.007083\n",
      "Training epoch: 29 [800/900 (88%)]\tLoss: 0.024348\n",
      "=========> Epoch: 29 Average loss: 0.0150\n",
      "Correlation coefficient: 0.7733\n",
      "Training epoch: 30 [0/900 (0%)]\tLoss: 0.021578\n",
      "Training epoch: 30 [160/900 (18%)]\tLoss: 0.018907\n",
      "Training epoch: 30 [320/900 (35%)]\tLoss: 0.019143\n",
      "Training epoch: 30 [480/900 (53%)]\tLoss: 0.019248\n",
      "Training epoch: 30 [640/900 (70%)]\tLoss: 0.070672\n",
      "Training epoch: 30 [800/900 (88%)]\tLoss: 0.012645\n",
      "=========> Epoch: 30 Average loss: 0.0160\n",
      "Correlation coefficient: 0.7833\n",
      "Training epoch: 31 [0/900 (0%)]\tLoss: 0.005512\n",
      "Training epoch: 31 [160/900 (18%)]\tLoss: 0.010762\n",
      "Training epoch: 31 [320/900 (35%)]\tLoss: 0.020416\n",
      "Training epoch: 31 [480/900 (53%)]\tLoss: 0.010457\n",
      "Training epoch: 31 [640/900 (70%)]\tLoss: 0.015116\n",
      "Training epoch: 31 [800/900 (88%)]\tLoss: 0.016188\n",
      "=========> Epoch: 31 Average loss: 0.0134\n",
      "Correlation coefficient: 0.7729\n",
      "Training epoch: 32 [0/900 (0%)]\tLoss: 0.014178\n",
      "Training epoch: 32 [160/900 (18%)]\tLoss: 0.015440\n",
      "Training epoch: 32 [320/900 (35%)]\tLoss: 0.015276\n",
      "Training epoch: 32 [480/900 (53%)]\tLoss: 0.006975\n",
      "Training epoch: 32 [640/900 (70%)]\tLoss: 0.011620\n",
      "Training epoch: 32 [800/900 (88%)]\tLoss: 0.014267\n",
      "=========> Epoch: 32 Average loss: 0.0174\n",
      "Correlation coefficient: 0.7781\n",
      "Training epoch: 33 [0/900 (0%)]\tLoss: 0.006048\n",
      "Training epoch: 33 [160/900 (18%)]\tLoss: 0.012181\n",
      "Training epoch: 33 [320/900 (35%)]\tLoss: 0.026876\n",
      "Training epoch: 33 [480/900 (53%)]\tLoss: 0.022329\n",
      "Training epoch: 33 [640/900 (70%)]\tLoss: 0.005407\n",
      "Training epoch: 33 [800/900 (88%)]\tLoss: 0.010413\n",
      "=========> Epoch: 33 Average loss: 0.0161\n",
      "Correlation coefficient: 0.7764\n",
      "Training epoch: 34 [0/900 (0%)]\tLoss: 0.008915\n",
      "Training epoch: 34 [160/900 (18%)]\tLoss: 0.007771\n",
      "Training epoch: 34 [320/900 (35%)]\tLoss: 0.013594\n",
      "Training epoch: 34 [480/900 (53%)]\tLoss: 0.013214\n",
      "Training epoch: 34 [640/900 (70%)]\tLoss: 0.013077\n",
      "Training epoch: 34 [800/900 (88%)]\tLoss: 0.005831\n",
      "=========> Epoch: 34 Average loss: 0.0123\n",
      "Correlation coefficient: 0.7808\n",
      "Training epoch: 35 [0/900 (0%)]\tLoss: 0.013804\n",
      "Training epoch: 35 [160/900 (18%)]\tLoss: 0.020147\n",
      "Training epoch: 35 [320/900 (35%)]\tLoss: 0.032607\n",
      "Training epoch: 35 [480/900 (53%)]\tLoss: 0.006479\n",
      "Training epoch: 35 [640/900 (70%)]\tLoss: 0.008326\n",
      "Training epoch: 35 [800/900 (88%)]\tLoss: 0.014635\n",
      "=========> Epoch: 35 Average loss: 0.0118\n",
      "Correlation coefficient: 0.7822\n",
      "Training epoch: 36 [0/900 (0%)]\tLoss: 0.005716\n",
      "Training epoch: 36 [160/900 (18%)]\tLoss: 0.008424\n",
      "Training epoch: 36 [320/900 (35%)]\tLoss: 0.003252\n",
      "Training epoch: 36 [480/900 (53%)]\tLoss: 0.009993\n",
      "Training epoch: 36 [640/900 (70%)]\tLoss: 0.004430\n",
      "Training epoch: 36 [800/900 (88%)]\tLoss: 0.003201\n",
      "=========> Epoch: 36 Average loss: 0.0094\n",
      "Correlation coefficient: 0.7807\n",
      "Training epoch: 37 [0/900 (0%)]\tLoss: 0.004812\n",
      "Training epoch: 37 [160/900 (18%)]\tLoss: 0.015163\n",
      "Training epoch: 37 [320/900 (35%)]\tLoss: 0.018976\n",
      "Training epoch: 37 [480/900 (53%)]\tLoss: 0.005548\n",
      "Training epoch: 37 [640/900 (70%)]\tLoss: 0.004462\n",
      "Training epoch: 37 [800/900 (88%)]\tLoss: 0.003551\n",
      "=========> Epoch: 37 Average loss: 0.0075\n",
      "Correlation coefficient: 0.7812\n",
      "Training epoch: 38 [0/900 (0%)]\tLoss: 0.001972\n",
      "Training epoch: 38 [160/900 (18%)]\tLoss: 0.009003\n",
      "Training epoch: 38 [320/900 (35%)]\tLoss: 0.003630\n",
      "Training epoch: 38 [480/900 (53%)]\tLoss: 0.002943\n",
      "Training epoch: 38 [640/900 (70%)]\tLoss: 0.003946\n",
      "Training epoch: 38 [800/900 (88%)]\tLoss: 0.009767\n",
      "=========> Epoch: 38 Average loss: 0.0066\n",
      "Correlation coefficient: 0.7755\n",
      "Training epoch: 39 [0/900 (0%)]\tLoss: 0.007363\n",
      "Training epoch: 39 [160/900 (18%)]\tLoss: 0.010467\n",
      "Training epoch: 39 [320/900 (35%)]\tLoss: 0.010154\n",
      "Training epoch: 39 [480/900 (53%)]\tLoss: 0.008617\n",
      "Training epoch: 39 [640/900 (70%)]\tLoss: 0.006081\n",
      "Training epoch: 39 [800/900 (88%)]\tLoss: 0.004548\n",
      "=========> Epoch: 39 Average loss: 0.0079\n",
      "Correlation coefficient: 0.7802\n",
      "Training epoch: 40 [0/900 (0%)]\tLoss: 0.008944\n",
      "Training epoch: 40 [160/900 (18%)]\tLoss: 0.013176\n",
      "Training epoch: 40 [320/900 (35%)]\tLoss: 0.015163\n",
      "Training epoch: 40 [480/900 (53%)]\tLoss: 0.008283\n",
      "Training epoch: 40 [640/900 (70%)]\tLoss: 0.010645\n",
      "Training epoch: 40 [800/900 (88%)]\tLoss: 0.004483\n",
      "=========> Epoch: 40 Average loss: 0.0088\n",
      "Correlation coefficient: 0.7765\n",
      "Training epoch: 41 [0/900 (0%)]\tLoss: 0.004144\n",
      "Training epoch: 41 [160/900 (18%)]\tLoss: 0.005046\n",
      "Training epoch: 41 [320/900 (35%)]\tLoss: 0.051564\n",
      "Training epoch: 41 [480/900 (53%)]\tLoss: 0.008046\n",
      "Training epoch: 41 [640/900 (70%)]\tLoss: 0.004631\n",
      "Training epoch: 41 [800/900 (88%)]\tLoss: 0.017201\n",
      "=========> Epoch: 41 Average loss: 0.0090\n",
      "Correlation coefficient: 0.7757\n",
      "Training epoch: 42 [0/900 (0%)]\tLoss: 0.009636\n",
      "Training epoch: 42 [160/900 (18%)]\tLoss: 0.005661\n",
      "Training epoch: 42 [320/900 (35%)]\tLoss: 0.003602\n",
      "Training epoch: 42 [480/900 (53%)]\tLoss: 0.006548\n",
      "Training epoch: 42 [640/900 (70%)]\tLoss: 0.005549\n",
      "Training epoch: 42 [800/900 (88%)]\tLoss: 0.005824\n",
      "=========> Epoch: 42 Average loss: 0.0102\n",
      "Correlation coefficient: 0.7720\n",
      "Training epoch: 43 [0/900 (0%)]\tLoss: 0.015494\n",
      "Training epoch: 43 [160/900 (18%)]\tLoss: 0.010032\n",
      "Training epoch: 43 [320/900 (35%)]\tLoss: 0.006466\n",
      "Training epoch: 43 [480/900 (53%)]\tLoss: 0.005367\n",
      "Training epoch: 43 [640/900 (70%)]\tLoss: 0.034017\n",
      "Training epoch: 43 [800/900 (88%)]\tLoss: 0.008787\n",
      "=========> Epoch: 43 Average loss: 0.0118\n",
      "Correlation coefficient: 0.7687\n",
      "Training epoch: 44 [0/900 (0%)]\tLoss: 0.009167\n",
      "Training epoch: 44 [160/900 (18%)]\tLoss: 0.004037\n",
      "Training epoch: 44 [320/900 (35%)]\tLoss: 0.010887\n",
      "Training epoch: 44 [480/900 (53%)]\tLoss: 0.021329\n",
      "Training epoch: 44 [640/900 (70%)]\tLoss: 0.014134\n",
      "Training epoch: 44 [800/900 (88%)]\tLoss: 0.004437\n",
      "=========> Epoch: 44 Average loss: 0.0117\n",
      "Correlation coefficient: 0.7745\n",
      "Training epoch: 45 [0/900 (0%)]\tLoss: 0.013303\n",
      "Training epoch: 45 [160/900 (18%)]\tLoss: 0.016081\n",
      "Training epoch: 45 [320/900 (35%)]\tLoss: 0.006175\n",
      "Training epoch: 45 [480/900 (53%)]\tLoss: 0.008114\n",
      "Training epoch: 45 [640/900 (70%)]\tLoss: 0.012447\n",
      "Training epoch: 45 [800/900 (88%)]\tLoss: 0.011468\n",
      "=========> Epoch: 45 Average loss: 0.0144\n",
      "Correlation coefficient: 0.7686\n",
      "Training epoch: 46 [0/900 (0%)]\tLoss: 0.006705\n",
      "Training epoch: 46 [160/900 (18%)]\tLoss: 0.005250\n",
      "Training epoch: 46 [320/900 (35%)]\tLoss: 0.014621\n",
      "Training epoch: 46 [480/900 (53%)]\tLoss: 0.008597\n",
      "Training epoch: 46 [640/900 (70%)]\tLoss: 0.005939\n",
      "Training epoch: 46 [800/900 (88%)]\tLoss: 0.003681\n",
      "=========> Epoch: 46 Average loss: 0.0137\n",
      "Correlation coefficient: 0.7728\n",
      "Training epoch: 47 [0/900 (0%)]\tLoss: 0.006055\n",
      "Training epoch: 47 [160/900 (18%)]\tLoss: 0.012240\n",
      "Training epoch: 47 [320/900 (35%)]\tLoss: 0.022468\n",
      "Training epoch: 47 [480/900 (53%)]\tLoss: 0.027078\n",
      "Training epoch: 47 [640/900 (70%)]\tLoss: 0.007874\n",
      "Training epoch: 47 [800/900 (88%)]\tLoss: 0.012246\n",
      "=========> Epoch: 47 Average loss: 0.0138\n",
      "Correlation coefficient: 0.7745\n",
      "Training epoch: 48 [0/900 (0%)]\tLoss: 0.010612\n",
      "Training epoch: 48 [160/900 (18%)]\tLoss: 0.022265\n",
      "Training epoch: 48 [320/900 (35%)]\tLoss: 0.008782\n",
      "Training epoch: 48 [480/900 (53%)]\tLoss: 0.019669\n",
      "Training epoch: 48 [640/900 (70%)]\tLoss: 0.004183\n",
      "Training epoch: 48 [800/900 (88%)]\tLoss: 0.019501\n",
      "=========> Epoch: 48 Average loss: 0.0141\n",
      "Correlation coefficient: 0.7677\n",
      "Training epoch: 49 [0/900 (0%)]\tLoss: 0.011612\n",
      "Training epoch: 49 [160/900 (18%)]\tLoss: 0.006481\n",
      "Training epoch: 49 [320/900 (35%)]\tLoss: 0.008834\n",
      "Training epoch: 49 [480/900 (53%)]\tLoss: 0.007099\n",
      "Training epoch: 49 [640/900 (70%)]\tLoss: 0.010741\n",
      "Training epoch: 49 [800/900 (88%)]\tLoss: 0.011307\n",
      "=========> Epoch: 49 Average loss: 0.0162\n",
      "Correlation coefficient: 0.7789\n",
      "Training epoch: 50 [0/900 (0%)]\tLoss: 0.055509\n",
      "Training epoch: 50 [160/900 (18%)]\tLoss: 0.018226\n",
      "Training epoch: 50 [320/900 (35%)]\tLoss: 0.013007\n",
      "Training epoch: 50 [480/900 (53%)]\tLoss: 0.014655\n",
      "Training epoch: 50 [640/900 (70%)]\tLoss: 0.014335\n",
      "Training epoch: 50 [800/900 (88%)]\tLoss: 0.022582\n",
      "=========> Epoch: 50 Average loss: 0.0234\n",
      "Correlation coefficient: 0.7725\n",
      "Training epoch: 51 [0/900 (0%)]\tLoss: 0.016535\n",
      "Training epoch: 51 [160/900 (18%)]\tLoss: 0.036697\n",
      "Training epoch: 51 [320/900 (35%)]\tLoss: 0.030882\n",
      "Training epoch: 51 [480/900 (53%)]\tLoss: 0.009202\n",
      "Training epoch: 51 [640/900 (70%)]\tLoss: 0.028555\n",
      "Training epoch: 51 [800/900 (88%)]\tLoss: 0.024886\n",
      "=========> Epoch: 51 Average loss: 0.0244\n",
      "Correlation coefficient: 0.7550\n",
      "Training epoch: 52 [0/900 (0%)]\tLoss: 0.020274\n",
      "Training epoch: 52 [160/900 (18%)]\tLoss: 0.018994\n",
      "Training epoch: 52 [320/900 (35%)]\tLoss: 0.030595\n",
      "Training epoch: 52 [480/900 (53%)]\tLoss: 0.022630\n",
      "Training epoch: 52 [640/900 (70%)]\tLoss: 0.011217\n",
      "Training epoch: 52 [800/900 (88%)]\tLoss: 0.006916\n",
      "=========> Epoch: 52 Average loss: 0.0230\n",
      "Correlation coefficient: 0.7604\n",
      "Training epoch: 53 [0/900 (0%)]\tLoss: 0.024291\n",
      "Training epoch: 53 [160/900 (18%)]\tLoss: 0.015411\n",
      "Training epoch: 53 [320/900 (35%)]\tLoss: 0.010219\n",
      "Training epoch: 53 [480/900 (53%)]\tLoss: 0.024541\n",
      "Training epoch: 53 [640/900 (70%)]\tLoss: 0.013539\n",
      "Training epoch: 53 [800/900 (88%)]\tLoss: 0.015517\n",
      "=========> Epoch: 53 Average loss: 0.0227\n",
      "Correlation coefficient: 0.7682\n",
      "Training epoch: 54 [0/900 (0%)]\tLoss: 0.018363\n",
      "Training epoch: 54 [160/900 (18%)]\tLoss: 0.008579\n",
      "Training epoch: 54 [320/900 (35%)]\tLoss: 0.003671\n",
      "Training epoch: 54 [480/900 (53%)]\tLoss: 0.012989\n",
      "Training epoch: 54 [640/900 (70%)]\tLoss: 0.023097\n",
      "Training epoch: 54 [800/900 (88%)]\tLoss: 0.025692\n",
      "=========> Epoch: 54 Average loss: 0.0178\n",
      "Correlation coefficient: 0.7735\n",
      "Training epoch: 55 [0/900 (0%)]\tLoss: 0.019601\n",
      "Training epoch: 55 [160/900 (18%)]\tLoss: 0.012320\n",
      "Training epoch: 55 [320/900 (35%)]\tLoss: 0.006119\n",
      "Training epoch: 55 [480/900 (53%)]\tLoss: 0.009347\n",
      "Training epoch: 55 [640/900 (70%)]\tLoss: 0.010355\n",
      "Training epoch: 55 [800/900 (88%)]\tLoss: 0.004227\n",
      "=========> Epoch: 55 Average loss: 0.0201\n",
      "Correlation coefficient: 0.7704\n",
      "Training epoch: 56 [0/900 (0%)]\tLoss: 0.004555\n",
      "Training epoch: 56 [160/900 (18%)]\tLoss: 0.016142\n",
      "Training epoch: 56 [320/900 (35%)]\tLoss: 0.009428\n",
      "Training epoch: 56 [480/900 (53%)]\tLoss: 0.014053\n",
      "Training epoch: 56 [640/900 (70%)]\tLoss: 0.050328\n",
      "Training epoch: 56 [800/900 (88%)]\tLoss: 0.030051\n",
      "=========> Epoch: 56 Average loss: 0.0187\n",
      "Correlation coefficient: 0.7709\n",
      "Training epoch: 57 [0/900 (0%)]\tLoss: 0.008079\n",
      "Training epoch: 57 [160/900 (18%)]\tLoss: 0.018463\n",
      "Training epoch: 57 [320/900 (35%)]\tLoss: 0.029851\n",
      "Training epoch: 57 [480/900 (53%)]\tLoss: 0.006651\n",
      "Training epoch: 57 [640/900 (70%)]\tLoss: 0.010771\n",
      "Training epoch: 57 [800/900 (88%)]\tLoss: 0.003289\n",
      "=========> Epoch: 57 Average loss: 0.0108\n",
      "Correlation coefficient: 0.7706\n",
      "Training epoch: 58 [0/900 (0%)]\tLoss: 0.005927\n",
      "Training epoch: 58 [160/900 (18%)]\tLoss: 0.007920\n",
      "Training epoch: 58 [320/900 (35%)]\tLoss: 0.009243\n",
      "Training epoch: 58 [480/900 (53%)]\tLoss: 0.001884\n",
      "Training epoch: 58 [640/900 (70%)]\tLoss: 0.002872\n",
      "Training epoch: 58 [800/900 (88%)]\tLoss: 0.005137\n",
      "=========> Epoch: 58 Average loss: 0.0078\n",
      "Correlation coefficient: 0.7787\n",
      "Training epoch: 59 [0/900 (0%)]\tLoss: 0.006076\n",
      "Training epoch: 59 [160/900 (18%)]\tLoss: 0.005093\n",
      "Training epoch: 59 [320/900 (35%)]\tLoss: 0.009785\n",
      "Training epoch: 59 [480/900 (53%)]\tLoss: 0.008233\n",
      "Training epoch: 59 [640/900 (70%)]\tLoss: 0.012659\n",
      "Training epoch: 59 [800/900 (88%)]\tLoss: 0.001161\n",
      "=========> Epoch: 59 Average loss: 0.0047\n",
      "Correlation coefficient: 0.7705\n",
      "Training epoch: 60 [0/900 (0%)]\tLoss: 0.001727\n",
      "Training epoch: 60 [160/900 (18%)]\tLoss: 0.002112\n",
      "Training epoch: 60 [320/900 (35%)]\tLoss: 0.003266\n",
      "Training epoch: 60 [480/900 (53%)]\tLoss: 0.002365\n",
      "Training epoch: 60 [640/900 (70%)]\tLoss: 0.001332\n",
      "Training epoch: 60 [800/900 (88%)]\tLoss: 0.000972\n",
      "=========> Epoch: 60 Average loss: 0.0034\n",
      "Correlation coefficient: 0.7802\n",
      "Training epoch: 61 [0/900 (0%)]\tLoss: 0.002231\n",
      "Training epoch: 61 [160/900 (18%)]\tLoss: 0.001376\n",
      "Training epoch: 61 [320/900 (35%)]\tLoss: 0.001362\n",
      "Training epoch: 61 [480/900 (53%)]\tLoss: 0.013473\n",
      "Training epoch: 61 [640/900 (70%)]\tLoss: 0.001589\n",
      "Training epoch: 61 [800/900 (88%)]\tLoss: 0.001883\n",
      "=========> Epoch: 61 Average loss: 0.0026\n",
      "Correlation coefficient: 0.7756\n",
      "Training epoch: 62 [0/900 (0%)]\tLoss: 0.000710\n",
      "Training epoch: 62 [160/900 (18%)]\tLoss: 0.001525\n",
      "Training epoch: 62 [320/900 (35%)]\tLoss: 0.000705\n",
      "Training epoch: 62 [480/900 (53%)]\tLoss: 0.002782\n",
      "Training epoch: 62 [640/900 (70%)]\tLoss: 0.007014\n",
      "Training epoch: 62 [800/900 (88%)]\tLoss: 0.005798\n",
      "=========> Epoch: 62 Average loss: 0.0026\n",
      "Correlation coefficient: 0.7780\n",
      "Training epoch: 63 [0/900 (0%)]\tLoss: 0.001322\n",
      "Training epoch: 63 [160/900 (18%)]\tLoss: 0.001339\n",
      "Training epoch: 63 [320/900 (35%)]\tLoss: 0.001568\n",
      "Training epoch: 63 [480/900 (53%)]\tLoss: 0.004875\n",
      "Training epoch: 63 [640/900 (70%)]\tLoss: 0.001974\n",
      "Training epoch: 63 [800/900 (88%)]\tLoss: 0.003632\n",
      "=========> Epoch: 63 Average loss: 0.0027\n",
      "Correlation coefficient: 0.7766\n",
      "Training epoch: 64 [0/900 (0%)]\tLoss: 0.001715\n",
      "Training epoch: 64 [160/900 (18%)]\tLoss: 0.001014\n",
      "Training epoch: 64 [320/900 (35%)]\tLoss: 0.000752\n",
      "Training epoch: 64 [480/900 (53%)]\tLoss: 0.001558\n",
      "Training epoch: 64 [640/900 (70%)]\tLoss: 0.000832\n",
      "Training epoch: 64 [800/900 (88%)]\tLoss: 0.002613\n",
      "=========> Epoch: 64 Average loss: 0.0027\n",
      "Correlation coefficient: 0.7786\n",
      "Training epoch: 65 [0/900 (0%)]\tLoss: 0.001579\n",
      "Training epoch: 65 [160/900 (18%)]\tLoss: 0.003331\n",
      "Training epoch: 65 [320/900 (35%)]\tLoss: 0.002568\n",
      "Training epoch: 65 [480/900 (53%)]\tLoss: 0.001265\n",
      "Training epoch: 65 [640/900 (70%)]\tLoss: 0.001393\n",
      "Training epoch: 65 [800/900 (88%)]\tLoss: 0.001341\n",
      "=========> Epoch: 65 Average loss: 0.0027\n",
      "Correlation coefficient: 0.7786\n",
      "Training epoch: 66 [0/900 (0%)]\tLoss: 0.002671\n",
      "Training epoch: 66 [160/900 (18%)]\tLoss: 0.001888\n",
      "Training epoch: 66 [320/900 (35%)]\tLoss: 0.001615\n",
      "Training epoch: 66 [480/900 (53%)]\tLoss: 0.000999\n",
      "Training epoch: 66 [640/900 (70%)]\tLoss: 0.005308\n",
      "Training epoch: 66 [800/900 (88%)]\tLoss: 0.007822\n",
      "=========> Epoch: 66 Average loss: 0.0028\n",
      "Correlation coefficient: 0.7770\n",
      "Training epoch: 67 [0/900 (0%)]\tLoss: 0.000528\n",
      "Training epoch: 67 [160/900 (18%)]\tLoss: 0.001555\n",
      "Training epoch: 67 [320/900 (35%)]\tLoss: 0.000870\n",
      "Training epoch: 67 [480/900 (53%)]\tLoss: 0.004195\n",
      "Training epoch: 67 [640/900 (70%)]\tLoss: 0.002011\n",
      "Training epoch: 67 [800/900 (88%)]\tLoss: 0.000811\n",
      "=========> Epoch: 67 Average loss: 0.0034\n",
      "Correlation coefficient: 0.7760\n",
      "Training epoch: 68 [0/900 (0%)]\tLoss: 0.002460\n",
      "Training epoch: 68 [160/900 (18%)]\tLoss: 0.000476\n",
      "Training epoch: 68 [320/900 (35%)]\tLoss: 0.003276\n",
      "Training epoch: 68 [480/900 (53%)]\tLoss: 0.002228\n",
      "Training epoch: 68 [640/900 (70%)]\tLoss: 0.001563\n",
      "Training epoch: 68 [800/900 (88%)]\tLoss: 0.001688\n",
      "=========> Epoch: 68 Average loss: 0.0048\n",
      "Correlation coefficient: 0.7768\n",
      "Training epoch: 69 [0/900 (0%)]\tLoss: 0.007371\n",
      "Training epoch: 69 [160/900 (18%)]\tLoss: 0.002174\n",
      "Training epoch: 69 [320/900 (35%)]\tLoss: 0.002897\n",
      "Training epoch: 69 [480/900 (53%)]\tLoss: 0.001010\n",
      "Training epoch: 69 [640/900 (70%)]\tLoss: 0.013730\n",
      "Training epoch: 69 [800/900 (88%)]\tLoss: 0.007929\n",
      "=========> Epoch: 69 Average loss: 0.0055\n",
      "Correlation coefficient: 0.7755\n",
      "Training epoch: 70 [0/900 (0%)]\tLoss: 0.002027\n",
      "Training epoch: 70 [160/900 (18%)]\tLoss: 0.015297\n",
      "Training epoch: 70 [320/900 (35%)]\tLoss: 0.005756\n",
      "Training epoch: 70 [480/900 (53%)]\tLoss: 0.001575\n",
      "Training epoch: 70 [640/900 (70%)]\tLoss: 0.004400\n",
      "Training epoch: 70 [800/900 (88%)]\tLoss: 0.004048\n",
      "=========> Epoch: 70 Average loss: 0.0056\n",
      "Correlation coefficient: 0.7790\n",
      "Training epoch: 71 [0/900 (0%)]\tLoss: 0.002175\n",
      "Training epoch: 71 [160/900 (18%)]\tLoss: 0.004596\n",
      "Training epoch: 71 [320/900 (35%)]\tLoss: 0.001533\n",
      "Training epoch: 71 [480/900 (53%)]\tLoss: 0.004728\n",
      "Training epoch: 71 [640/900 (70%)]\tLoss: 0.001082\n",
      "Training epoch: 71 [800/900 (88%)]\tLoss: 0.009758\n",
      "=========> Epoch: 71 Average loss: 0.0058\n",
      "Correlation coefficient: 0.7768\n",
      "Training epoch: 72 [0/900 (0%)]\tLoss: 0.002925\n",
      "Training epoch: 72 [160/900 (18%)]\tLoss: 0.013744\n",
      "Training epoch: 72 [320/900 (35%)]\tLoss: 0.006921\n",
      "Training epoch: 72 [480/900 (53%)]\tLoss: 0.004878\n",
      "Training epoch: 72 [640/900 (70%)]\tLoss: 0.003632\n",
      "Training epoch: 72 [800/900 (88%)]\tLoss: 0.004933\n",
      "=========> Epoch: 72 Average loss: 0.0096\n",
      "Correlation coefficient: 0.7659\n",
      "Training epoch: 73 [0/900 (0%)]\tLoss: 0.011565\n",
      "Training epoch: 73 [160/900 (18%)]\tLoss: 0.017846\n",
      "Training epoch: 73 [320/900 (35%)]\tLoss: 0.015001\n",
      "Training epoch: 73 [480/900 (53%)]\tLoss: 0.006926\n",
      "Training epoch: 73 [640/900 (70%)]\tLoss: 0.014547\n",
      "Training epoch: 73 [800/900 (88%)]\tLoss: 0.015926\n",
      "=========> Epoch: 73 Average loss: 0.0164\n",
      "Correlation coefficient: 0.7767\n",
      "Training epoch: 74 [0/900 (0%)]\tLoss: 0.025639\n",
      "Training epoch: 74 [160/900 (18%)]\tLoss: 0.017821\n",
      "Training epoch: 74 [320/900 (35%)]\tLoss: 0.019733\n",
      "Training epoch: 74 [480/900 (53%)]\tLoss: 0.015935\n",
      "Training epoch: 74 [640/900 (70%)]\tLoss: 0.009262\n",
      "Training epoch: 74 [800/900 (88%)]\tLoss: 0.017443\n",
      "=========> Epoch: 74 Average loss: 0.0177\n",
      "Correlation coefficient: 0.7665\n",
      "Training epoch: 75 [0/900 (0%)]\tLoss: 0.007188\n",
      "Training epoch: 75 [160/900 (18%)]\tLoss: 0.021443\n",
      "Training epoch: 75 [320/900 (35%)]\tLoss: 0.013366\n",
      "Training epoch: 75 [480/900 (53%)]\tLoss: 0.007026\n",
      "Training epoch: 75 [640/900 (70%)]\tLoss: 0.005765\n",
      "Training epoch: 75 [800/900 (88%)]\tLoss: 0.044485\n",
      "=========> Epoch: 75 Average loss: 0.0151\n",
      "Correlation coefficient: 0.7722\n",
      "Training epoch: 76 [0/900 (0%)]\tLoss: 0.014680\n",
      "Training epoch: 76 [160/900 (18%)]\tLoss: 0.032765\n",
      "Training epoch: 76 [320/900 (35%)]\tLoss: 0.012672\n",
      "Training epoch: 76 [480/900 (53%)]\tLoss: 0.015598\n",
      "Training epoch: 76 [640/900 (70%)]\tLoss: 0.024403\n",
      "Training epoch: 76 [800/900 (88%)]\tLoss: 0.019022\n",
      "=========> Epoch: 76 Average loss: 0.0208\n",
      "Correlation coefficient: 0.7706\n",
      "⏹️  Epoch 76 early stopping (no improvement for 50 epochs)\n",
      "🏁 Fold 2 best correlation: 0.7844\n",
      "\n",
      "========== Cross-validation Fold 3/10 ==========\n",
      "🔄 Fold 3: Using random initialization\n",
      "Training epoch: 1 [0/900 (0%)]\tLoss: 1.233203\n",
      "Training epoch: 1 [160/900 (18%)]\tLoss: 1.141933\n",
      "Training epoch: 1 [320/900 (35%)]\tLoss: 0.721223\n",
      "Training epoch: 1 [480/900 (53%)]\tLoss: 0.505114\n",
      "Training epoch: 1 [640/900 (70%)]\tLoss: 0.744836\n",
      "Training epoch: 1 [800/900 (88%)]\tLoss: 0.546709\n",
      "=========> Epoch: 1 Average loss: 0.7281\n",
      "Correlation coefficient: 0.6591\n",
      "✅ Epoch 1: New best correlation = 0.6591\n",
      "Training epoch: 2 [0/900 (0%)]\tLoss: 0.471552\n",
      "Training epoch: 2 [160/900 (18%)]\tLoss: 0.423519\n",
      "Training epoch: 2 [320/900 (35%)]\tLoss: 0.345327\n",
      "Training epoch: 2 [480/900 (53%)]\tLoss: 0.280255\n",
      "Training epoch: 2 [640/900 (70%)]\tLoss: 0.113985\n",
      "Training epoch: 2 [800/900 (88%)]\tLoss: 0.137656\n",
      "=========> Epoch: 2 Average loss: 0.3230\n",
      "Correlation coefficient: 0.6805\n",
      "✅ Epoch 2: New best correlation = 0.6805\n",
      "Training epoch: 3 [0/900 (0%)]\tLoss: 0.122775\n",
      "Training epoch: 3 [160/900 (18%)]\tLoss: 0.214853\n",
      "Training epoch: 3 [320/900 (35%)]\tLoss: 0.163822\n",
      "Training epoch: 3 [480/900 (53%)]\tLoss: 0.117053\n",
      "Training epoch: 3 [640/900 (70%)]\tLoss: 0.241957\n",
      "Training epoch: 3 [800/900 (88%)]\tLoss: 0.186096\n",
      "=========> Epoch: 3 Average loss: 0.1192\n",
      "Correlation coefficient: 0.6273\n",
      "Training epoch: 4 [0/900 (0%)]\tLoss: 0.077668\n",
      "Training epoch: 4 [160/900 (18%)]\tLoss: 0.050827\n",
      "Training epoch: 4 [320/900 (35%)]\tLoss: 0.062105\n",
      "Training epoch: 4 [480/900 (53%)]\tLoss: 0.025492\n",
      "Training epoch: 4 [640/900 (70%)]\tLoss: 0.061305\n",
      "Training epoch: 4 [800/900 (88%)]\tLoss: 0.039086\n",
      "=========> Epoch: 4 Average loss: 0.0654\n",
      "Correlation coefficient: 0.6811\n",
      "✅ Epoch 4: New best correlation = 0.6811\n",
      "Training epoch: 5 [0/900 (0%)]\tLoss: 0.093720\n",
      "Training epoch: 5 [160/900 (18%)]\tLoss: 0.037976\n",
      "Training epoch: 5 [320/900 (35%)]\tLoss: 0.034546\n",
      "Training epoch: 5 [480/900 (53%)]\tLoss: 0.021840\n",
      "Training epoch: 5 [640/900 (70%)]\tLoss: 0.012333\n",
      "Training epoch: 5 [800/900 (88%)]\tLoss: 0.025668\n",
      "=========> Epoch: 5 Average loss: 0.0405\n",
      "Correlation coefficient: 0.6599\n",
      "Training epoch: 6 [0/900 (0%)]\tLoss: 0.024811\n",
      "Training epoch: 6 [160/900 (18%)]\tLoss: 0.009036\n",
      "Training epoch: 6 [320/900 (35%)]\tLoss: 0.021971\n",
      "Training epoch: 6 [480/900 (53%)]\tLoss: 0.009324\n",
      "Training epoch: 6 [640/900 (70%)]\tLoss: 0.028811\n",
      "Training epoch: 6 [800/900 (88%)]\tLoss: 0.012728\n",
      "=========> Epoch: 6 Average loss: 0.0223\n",
      "Correlation coefficient: 0.6893\n",
      "✅ Epoch 6: New best correlation = 0.6893\n",
      "Training epoch: 7 [0/900 (0%)]\tLoss: 0.008626\n",
      "Training epoch: 7 [160/900 (18%)]\tLoss: 0.007775\n",
      "Training epoch: 7 [320/900 (35%)]\tLoss: 0.006289\n",
      "Training epoch: 7 [480/900 (53%)]\tLoss: 0.008981\n",
      "Training epoch: 7 [640/900 (70%)]\tLoss: 0.007614\n",
      "Training epoch: 7 [800/900 (88%)]\tLoss: 0.018453\n",
      "=========> Epoch: 7 Average loss: 0.0120\n",
      "Correlation coefficient: 0.6747\n",
      "Training epoch: 8 [0/900 (0%)]\tLoss: 0.014608\n",
      "Training epoch: 8 [160/900 (18%)]\tLoss: 0.004275\n",
      "Training epoch: 8 [320/900 (35%)]\tLoss: 0.009175\n",
      "Training epoch: 8 [480/900 (53%)]\tLoss: 0.004352\n",
      "Training epoch: 8 [640/900 (70%)]\tLoss: 0.002095\n",
      "Training epoch: 8 [800/900 (88%)]\tLoss: 0.002022\n",
      "=========> Epoch: 8 Average loss: 0.0093\n",
      "Correlation coefficient: 0.6750\n",
      "Training epoch: 9 [0/900 (0%)]\tLoss: 0.014592\n",
      "Training epoch: 9 [160/900 (18%)]\tLoss: 0.005890\n",
      "Training epoch: 9 [320/900 (35%)]\tLoss: 0.009040\n",
      "Training epoch: 9 [480/900 (53%)]\tLoss: 0.004537\n",
      "Training epoch: 9 [640/900 (70%)]\tLoss: 0.002147\n",
      "Training epoch: 9 [800/900 (88%)]\tLoss: 0.019865\n",
      "=========> Epoch: 9 Average loss: 0.0073\n",
      "Correlation coefficient: 0.6754\n",
      "Training epoch: 10 [0/900 (0%)]\tLoss: 0.003372\n",
      "Training epoch: 10 [160/900 (18%)]\tLoss: 0.002080\n",
      "Training epoch: 10 [320/900 (35%)]\tLoss: 0.004660\n",
      "Training epoch: 10 [480/900 (53%)]\tLoss: 0.003581\n",
      "Training epoch: 10 [640/900 (70%)]\tLoss: 0.004684\n",
      "Training epoch: 10 [800/900 (88%)]\tLoss: 0.002591\n",
      "=========> Epoch: 10 Average loss: 0.0064\n",
      "Correlation coefficient: 0.6805\n",
      "Training epoch: 11 [0/900 (0%)]\tLoss: 0.004522\n",
      "Training epoch: 11 [160/900 (18%)]\tLoss: 0.002165\n",
      "Training epoch: 11 [320/900 (35%)]\tLoss: 0.005975\n",
      "Training epoch: 11 [480/900 (53%)]\tLoss: 0.005240\n",
      "Training epoch: 11 [640/900 (70%)]\tLoss: 0.002663\n",
      "Training epoch: 11 [800/900 (88%)]\tLoss: 0.005918\n",
      "=========> Epoch: 11 Average loss: 0.0072\n",
      "Correlation coefficient: 0.6681\n",
      "Training epoch: 12 [0/900 (0%)]\tLoss: 0.006241\n",
      "Training epoch: 12 [160/900 (18%)]\tLoss: 0.008802\n",
      "Training epoch: 12 [320/900 (35%)]\tLoss: 0.003975\n",
      "Training epoch: 12 [480/900 (53%)]\tLoss: 0.004821\n",
      "Training epoch: 12 [640/900 (70%)]\tLoss: 0.014170\n",
      "Training epoch: 12 [800/900 (88%)]\tLoss: 0.003901\n",
      "=========> Epoch: 12 Average loss: 0.0092\n",
      "Correlation coefficient: 0.6738\n",
      "Training epoch: 13 [0/900 (0%)]\tLoss: 0.013512\n",
      "Training epoch: 13 [160/900 (18%)]\tLoss: 0.011956\n",
      "Training epoch: 13 [320/900 (35%)]\tLoss: 0.005469\n",
      "Training epoch: 13 [480/900 (53%)]\tLoss: 0.023157\n",
      "Training epoch: 13 [640/900 (70%)]\tLoss: 0.007937\n",
      "Training epoch: 13 [800/900 (88%)]\tLoss: 0.006602\n",
      "=========> Epoch: 13 Average loss: 0.0120\n",
      "Correlation coefficient: 0.6808\n",
      "Training epoch: 14 [0/900 (0%)]\tLoss: 0.015537\n",
      "Training epoch: 14 [160/900 (18%)]\tLoss: 0.005708\n",
      "Training epoch: 14 [320/900 (35%)]\tLoss: 0.019466\n",
      "Training epoch: 14 [480/900 (53%)]\tLoss: 0.039675\n",
      "Training epoch: 14 [640/900 (70%)]\tLoss: 0.012581\n",
      "Training epoch: 14 [800/900 (88%)]\tLoss: 0.012785\n",
      "=========> Epoch: 14 Average loss: 0.0117\n",
      "Correlation coefficient: 0.6860\n",
      "Training epoch: 15 [0/900 (0%)]\tLoss: 0.016287\n",
      "Training epoch: 15 [160/900 (18%)]\tLoss: 0.015859\n",
      "Training epoch: 15 [320/900 (35%)]\tLoss: 0.006039\n",
      "Training epoch: 15 [480/900 (53%)]\tLoss: 0.009977\n",
      "Training epoch: 15 [640/900 (70%)]\tLoss: 0.008475\n",
      "Training epoch: 15 [800/900 (88%)]\tLoss: 0.012875\n",
      "=========> Epoch: 15 Average loss: 0.0159\n",
      "Correlation coefficient: 0.6729\n",
      "Training epoch: 16 [0/900 (0%)]\tLoss: 0.008411\n",
      "Training epoch: 16 [160/900 (18%)]\tLoss: 0.014669\n",
      "Training epoch: 16 [320/900 (35%)]\tLoss: 0.014790\n",
      "Training epoch: 16 [480/900 (53%)]\tLoss: 0.015336\n",
      "Training epoch: 16 [640/900 (70%)]\tLoss: 0.017032\n",
      "Training epoch: 16 [800/900 (88%)]\tLoss: 0.007765\n",
      "=========> Epoch: 16 Average loss: 0.0219\n",
      "Correlation coefficient: 0.6708\n",
      "Training epoch: 17 [0/900 (0%)]\tLoss: 0.013408\n",
      "Training epoch: 17 [160/900 (18%)]\tLoss: 0.091625\n",
      "Training epoch: 17 [320/900 (35%)]\tLoss: 0.016121\n",
      "Training epoch: 17 [480/900 (53%)]\tLoss: 0.034440\n",
      "Training epoch: 17 [640/900 (70%)]\tLoss: 0.008985\n",
      "Training epoch: 17 [800/900 (88%)]\tLoss: 0.022569\n",
      "=========> Epoch: 17 Average loss: 0.0548\n",
      "Correlation coefficient: 0.6588\n",
      "Training epoch: 18 [0/900 (0%)]\tLoss: 0.021043\n",
      "Training epoch: 18 [160/900 (18%)]\tLoss: 0.034251\n",
      "Training epoch: 18 [320/900 (35%)]\tLoss: 0.062565\n",
      "Training epoch: 18 [480/900 (53%)]\tLoss: 0.033489\n",
      "Training epoch: 18 [640/900 (70%)]\tLoss: 0.111897\n",
      "Training epoch: 18 [800/900 (88%)]\tLoss: 0.016506\n",
      "=========> Epoch: 18 Average loss: 0.0582\n",
      "Correlation coefficient: 0.6705\n",
      "Training epoch: 19 [0/900 (0%)]\tLoss: 0.074502\n",
      "Training epoch: 19 [160/900 (18%)]\tLoss: 0.009611\n",
      "Training epoch: 19 [320/900 (35%)]\tLoss: 0.078944\n",
      "Training epoch: 19 [480/900 (53%)]\tLoss: 0.021394\n",
      "Training epoch: 19 [640/900 (70%)]\tLoss: 0.079562\n",
      "Training epoch: 19 [800/900 (88%)]\tLoss: 0.027648\n",
      "=========> Epoch: 19 Average loss: 0.0341\n",
      "Correlation coefficient: 0.6557\n",
      "Training epoch: 20 [0/900 (0%)]\tLoss: 0.036302\n",
      "Training epoch: 20 [160/900 (18%)]\tLoss: 0.028360\n",
      "Training epoch: 20 [320/900 (35%)]\tLoss: 0.008741\n",
      "Training epoch: 20 [480/900 (53%)]\tLoss: 0.021881\n",
      "Training epoch: 20 [640/900 (70%)]\tLoss: 0.009299\n",
      "Training epoch: 20 [800/900 (88%)]\tLoss: 0.011284\n",
      "=========> Epoch: 20 Average loss: 0.0181\n",
      "Correlation coefficient: 0.6686\n",
      "Training epoch: 21 [0/900 (0%)]\tLoss: 0.012128\n",
      "Training epoch: 21 [160/900 (18%)]\tLoss: 0.013671\n",
      "Training epoch: 21 [320/900 (35%)]\tLoss: 0.010595\n",
      "Training epoch: 21 [480/900 (53%)]\tLoss: 0.009256\n",
      "Training epoch: 21 [640/900 (70%)]\tLoss: 0.029268\n",
      "Training epoch: 21 [800/900 (88%)]\tLoss: 0.007104\n",
      "=========> Epoch: 21 Average loss: 0.0138\n",
      "Correlation coefficient: 0.6846\n",
      "Training epoch: 22 [0/900 (0%)]\tLoss: 0.012460\n",
      "Training epoch: 22 [160/900 (18%)]\tLoss: 0.017359\n",
      "Training epoch: 22 [320/900 (35%)]\tLoss: 0.017321\n",
      "Training epoch: 22 [480/900 (53%)]\tLoss: 0.010100\n",
      "Training epoch: 22 [640/900 (70%)]\tLoss: 0.025850\n",
      "Training epoch: 22 [800/900 (88%)]\tLoss: 0.004327\n",
      "=========> Epoch: 22 Average loss: 0.0108\n",
      "Correlation coefficient: 0.6682\n",
      "Training epoch: 23 [0/900 (0%)]\tLoss: 0.005537\n",
      "Training epoch: 23 [160/900 (18%)]\tLoss: 0.003155\n",
      "Training epoch: 23 [320/900 (35%)]\tLoss: 0.004202\n",
      "Training epoch: 23 [480/900 (53%)]\tLoss: 0.003493\n",
      "Training epoch: 23 [640/900 (70%)]\tLoss: 0.003438\n",
      "Training epoch: 23 [800/900 (88%)]\tLoss: 0.003306\n",
      "=========> Epoch: 23 Average loss: 0.0072\n",
      "Correlation coefficient: 0.6713\n",
      "Training epoch: 24 [0/900 (0%)]\tLoss: 0.002884\n",
      "Training epoch: 24 [160/900 (18%)]\tLoss: 0.002156\n",
      "Training epoch: 24 [320/900 (35%)]\tLoss: 0.005146\n",
      "Training epoch: 24 [480/900 (53%)]\tLoss: 0.002736\n",
      "Training epoch: 24 [640/900 (70%)]\tLoss: 0.002888\n",
      "Training epoch: 24 [800/900 (88%)]\tLoss: 0.001867\n",
      "=========> Epoch: 24 Average loss: 0.0046\n",
      "Correlation coefficient: 0.6721\n",
      "Training epoch: 25 [0/900 (0%)]\tLoss: 0.002929\n",
      "Training epoch: 25 [160/900 (18%)]\tLoss: 0.003809\n",
      "Training epoch: 25 [320/900 (35%)]\tLoss: 0.002059\n",
      "Training epoch: 25 [480/900 (53%)]\tLoss: 0.004316\n",
      "Training epoch: 25 [640/900 (70%)]\tLoss: 0.001054\n",
      "Training epoch: 25 [800/900 (88%)]\tLoss: 0.001801\n",
      "=========> Epoch: 25 Average loss: 0.0039\n",
      "Correlation coefficient: 0.6747\n",
      "Training epoch: 26 [0/900 (0%)]\tLoss: 0.004852\n",
      "Training epoch: 26 [160/900 (18%)]\tLoss: 0.000985\n",
      "Training epoch: 26 [320/900 (35%)]\tLoss: 0.002021\n",
      "Training epoch: 26 [480/900 (53%)]\tLoss: 0.002591\n",
      "Training epoch: 26 [640/900 (70%)]\tLoss: 0.003814\n",
      "Training epoch: 26 [800/900 (88%)]\tLoss: 0.003388\n",
      "=========> Epoch: 26 Average loss: 0.0031\n",
      "Correlation coefficient: 0.6771\n",
      "Training epoch: 27 [0/900 (0%)]\tLoss: 0.003480\n",
      "Training epoch: 27 [160/900 (18%)]\tLoss: 0.001993\n",
      "Training epoch: 27 [320/900 (35%)]\tLoss: 0.001110\n",
      "Training epoch: 27 [480/900 (53%)]\tLoss: 0.001606\n",
      "Training epoch: 27 [640/900 (70%)]\tLoss: 0.002405\n",
      "Training epoch: 27 [800/900 (88%)]\tLoss: 0.001639\n",
      "=========> Epoch: 27 Average loss: 0.0025\n",
      "Correlation coefficient: 0.6805\n",
      "Training epoch: 28 [0/900 (0%)]\tLoss: 0.002017\n",
      "Training epoch: 28 [160/900 (18%)]\tLoss: 0.002123\n",
      "Training epoch: 28 [320/900 (35%)]\tLoss: 0.002506\n",
      "Training epoch: 28 [480/900 (53%)]\tLoss: 0.002369\n",
      "Training epoch: 28 [640/900 (70%)]\tLoss: 0.005620\n",
      "Training epoch: 28 [800/900 (88%)]\tLoss: 0.002118\n",
      "=========> Epoch: 28 Average loss: 0.0021\n",
      "Correlation coefficient: 0.6777\n",
      "Training epoch: 29 [0/900 (0%)]\tLoss: 0.001122\n",
      "Training epoch: 29 [160/900 (18%)]\tLoss: 0.003133\n",
      "Training epoch: 29 [320/900 (35%)]\tLoss: 0.001797\n",
      "Training epoch: 29 [480/900 (53%)]\tLoss: 0.001603\n",
      "Training epoch: 29 [640/900 (70%)]\tLoss: 0.001619\n",
      "Training epoch: 29 [800/900 (88%)]\tLoss: 0.003397\n",
      "=========> Epoch: 29 Average loss: 0.0018\n",
      "Correlation coefficient: 0.6788\n",
      "Training epoch: 30 [0/900 (0%)]\tLoss: 0.000811\n",
      "Training epoch: 30 [160/900 (18%)]\tLoss: 0.001307\n",
      "Training epoch: 30 [320/900 (35%)]\tLoss: 0.003578\n",
      "Training epoch: 30 [480/900 (53%)]\tLoss: 0.001507\n",
      "Training epoch: 30 [640/900 (70%)]\tLoss: 0.001564\n",
      "Training epoch: 30 [800/900 (88%)]\tLoss: 0.001121\n",
      "=========> Epoch: 30 Average loss: 0.0020\n",
      "Correlation coefficient: 0.6745\n",
      "Training epoch: 31 [0/900 (0%)]\tLoss: 0.003088\n",
      "Training epoch: 31 [160/900 (18%)]\tLoss: 0.001168\n",
      "Training epoch: 31 [320/900 (35%)]\tLoss: 0.000756\n",
      "Training epoch: 31 [480/900 (53%)]\tLoss: 0.002293\n",
      "Training epoch: 31 [640/900 (70%)]\tLoss: 0.003643\n",
      "Training epoch: 31 [800/900 (88%)]\tLoss: 0.004740\n",
      "=========> Epoch: 31 Average loss: 0.0023\n",
      "Correlation coefficient: 0.6798\n",
      "Training epoch: 32 [0/900 (0%)]\tLoss: 0.002368\n",
      "Training epoch: 32 [160/900 (18%)]\tLoss: 0.006060\n",
      "Training epoch: 32 [320/900 (35%)]\tLoss: 0.003661\n",
      "Training epoch: 32 [480/900 (53%)]\tLoss: 0.002697\n",
      "Training epoch: 32 [640/900 (70%)]\tLoss: 0.001312\n",
      "Training epoch: 32 [800/900 (88%)]\tLoss: 0.004188\n",
      "=========> Epoch: 32 Average loss: 0.0026\n",
      "Correlation coefficient: 0.6768\n",
      "Training epoch: 33 [0/900 (0%)]\tLoss: 0.002269\n",
      "Training epoch: 33 [160/900 (18%)]\tLoss: 0.002624\n",
      "Training epoch: 33 [320/900 (35%)]\tLoss: 0.002725\n",
      "Training epoch: 33 [480/900 (53%)]\tLoss: 0.003113\n",
      "Training epoch: 33 [640/900 (70%)]\tLoss: 0.002006\n",
      "Training epoch: 33 [800/900 (88%)]\tLoss: 0.003457\n",
      "=========> Epoch: 33 Average loss: 0.0029\n",
      "Correlation coefficient: 0.6754\n",
      "Training epoch: 34 [0/900 (0%)]\tLoss: 0.002690\n",
      "Training epoch: 34 [160/900 (18%)]\tLoss: 0.002865\n",
      "Training epoch: 34 [320/900 (35%)]\tLoss: 0.002159\n",
      "Training epoch: 34 [480/900 (53%)]\tLoss: 0.002481\n",
      "Training epoch: 34 [640/900 (70%)]\tLoss: 0.003463\n",
      "Training epoch: 34 [800/900 (88%)]\tLoss: 0.000373\n",
      "=========> Epoch: 34 Average loss: 0.0043\n",
      "Correlation coefficient: 0.6783\n",
      "Training epoch: 35 [0/900 (0%)]\tLoss: 0.005047\n",
      "Training epoch: 35 [160/900 (18%)]\tLoss: 0.003330\n",
      "Training epoch: 35 [320/900 (35%)]\tLoss: 0.009932\n",
      "Training epoch: 35 [480/900 (53%)]\tLoss: 0.011694\n",
      "Training epoch: 35 [640/900 (70%)]\tLoss: 0.020995\n",
      "Training epoch: 35 [800/900 (88%)]\tLoss: 0.004200\n",
      "=========> Epoch: 35 Average loss: 0.0068\n",
      "Correlation coefficient: 0.6797\n",
      "Training epoch: 36 [0/900 (0%)]\tLoss: 0.012792\n",
      "Training epoch: 36 [160/900 (18%)]\tLoss: 0.009134\n",
      "Training epoch: 36 [320/900 (35%)]\tLoss: 0.018436\n",
      "Training epoch: 36 [480/900 (53%)]\tLoss: 0.005578\n",
      "Training epoch: 36 [640/900 (70%)]\tLoss: 0.015618\n",
      "Training epoch: 36 [800/900 (88%)]\tLoss: 0.020752\n",
      "=========> Epoch: 36 Average loss: 0.0115\n",
      "Correlation coefficient: 0.6777\n",
      "Training epoch: 37 [0/900 (0%)]\tLoss: 0.008114\n",
      "Training epoch: 37 [160/900 (18%)]\tLoss: 0.028834\n",
      "Training epoch: 37 [320/900 (35%)]\tLoss: 0.018521\n",
      "Training epoch: 37 [480/900 (53%)]\tLoss: 0.027953\n",
      "Training epoch: 37 [640/900 (70%)]\tLoss: 0.017724\n",
      "Training epoch: 37 [800/900 (88%)]\tLoss: 0.037386\n",
      "=========> Epoch: 37 Average loss: 0.0182\n",
      "Correlation coefficient: 0.6721\n",
      "Training epoch: 38 [0/900 (0%)]\tLoss: 0.015120\n",
      "Training epoch: 38 [160/900 (18%)]\tLoss: 0.018562\n",
      "Training epoch: 38 [320/900 (35%)]\tLoss: 0.168216\n",
      "Training epoch: 38 [480/900 (53%)]\tLoss: 0.014764\n",
      "Training epoch: 38 [640/900 (70%)]\tLoss: 0.063826\n",
      "Training epoch: 38 [800/900 (88%)]\tLoss: 0.024942\n",
      "=========> Epoch: 38 Average loss: 0.0318\n",
      "Correlation coefficient: 0.6804\n",
      "Training epoch: 39 [0/900 (0%)]\tLoss: 0.015094\n",
      "Training epoch: 39 [160/900 (18%)]\tLoss: 0.024670\n",
      "Training epoch: 39 [320/900 (35%)]\tLoss: 0.022515\n",
      "Training epoch: 39 [480/900 (53%)]\tLoss: 0.086781\n",
      "Training epoch: 39 [640/900 (70%)]\tLoss: 0.027643\n",
      "Training epoch: 39 [800/900 (88%)]\tLoss: 0.025313\n",
      "=========> Epoch: 39 Average loss: 0.0377\n",
      "Correlation coefficient: 0.6536\n",
      "Training epoch: 40 [0/900 (0%)]\tLoss: 0.038038\n",
      "Training epoch: 40 [160/900 (18%)]\tLoss: 0.075523\n",
      "Training epoch: 40 [320/900 (35%)]\tLoss: 0.019136\n",
      "Training epoch: 40 [480/900 (53%)]\tLoss: 0.011485\n",
      "Training epoch: 40 [640/900 (70%)]\tLoss: 0.022491\n",
      "Training epoch: 40 [800/900 (88%)]\tLoss: 0.019684\n",
      "=========> Epoch: 40 Average loss: 0.0274\n",
      "Correlation coefficient: 0.6828\n",
      "Training epoch: 41 [0/900 (0%)]\tLoss: 0.022541\n",
      "Training epoch: 41 [160/900 (18%)]\tLoss: 0.032873\n",
      "Training epoch: 41 [320/900 (35%)]\tLoss: 0.023499\n",
      "Training epoch: 41 [480/900 (53%)]\tLoss: 0.028333\n",
      "Training epoch: 41 [640/900 (70%)]\tLoss: 0.009744\n",
      "Training epoch: 41 [800/900 (88%)]\tLoss: 0.026007\n",
      "=========> Epoch: 41 Average loss: 0.0240\n",
      "Correlation coefficient: 0.6579\n",
      "Training epoch: 42 [0/900 (0%)]\tLoss: 0.015089\n",
      "Training epoch: 42 [160/900 (18%)]\tLoss: 0.015895\n",
      "Training epoch: 42 [320/900 (35%)]\tLoss: 0.012694\n",
      "Training epoch: 42 [480/900 (53%)]\tLoss: 0.023217\n",
      "Training epoch: 42 [640/900 (70%)]\tLoss: 0.014613\n",
      "Training epoch: 42 [800/900 (88%)]\tLoss: 0.013543\n",
      "=========> Epoch: 42 Average loss: 0.0209\n",
      "Correlation coefficient: 0.6751\n",
      "Training epoch: 43 [0/900 (0%)]\tLoss: 0.023747\n",
      "Training epoch: 43 [160/900 (18%)]\tLoss: 0.025474\n",
      "Training epoch: 43 [320/900 (35%)]\tLoss: 0.022096\n",
      "Training epoch: 43 [480/900 (53%)]\tLoss: 0.009705\n",
      "Training epoch: 43 [640/900 (70%)]\tLoss: 0.010670\n",
      "Training epoch: 43 [800/900 (88%)]\tLoss: 0.021118\n",
      "=========> Epoch: 43 Average loss: 0.0158\n",
      "Correlation coefficient: 0.6758\n",
      "Training epoch: 44 [0/900 (0%)]\tLoss: 0.008566\n",
      "Training epoch: 44 [160/900 (18%)]\tLoss: 0.011081\n",
      "Training epoch: 44 [320/900 (35%)]\tLoss: 0.012928\n",
      "Training epoch: 44 [480/900 (53%)]\tLoss: 0.009229\n",
      "Training epoch: 44 [640/900 (70%)]\tLoss: 0.010992\n",
      "Training epoch: 44 [800/900 (88%)]\tLoss: 0.010559\n",
      "=========> Epoch: 44 Average loss: 0.0121\n",
      "Correlation coefficient: 0.6652\n",
      "Training epoch: 45 [0/900 (0%)]\tLoss: 0.013041\n",
      "Training epoch: 45 [160/900 (18%)]\tLoss: 0.008161\n",
      "Training epoch: 45 [320/900 (35%)]\tLoss: 0.010714\n",
      "Training epoch: 45 [480/900 (53%)]\tLoss: 0.007788\n",
      "Training epoch: 45 [640/900 (70%)]\tLoss: 0.010919\n",
      "Training epoch: 45 [800/900 (88%)]\tLoss: 0.002680\n",
      "=========> Epoch: 45 Average loss: 0.0096\n",
      "Correlation coefficient: 0.6670\n",
      "Training epoch: 46 [0/900 (0%)]\tLoss: 0.003486\n",
      "Training epoch: 46 [160/900 (18%)]\tLoss: 0.002748\n",
      "Training epoch: 46 [320/900 (35%)]\tLoss: 0.024618\n",
      "Training epoch: 46 [480/900 (53%)]\tLoss: 0.012506\n",
      "Training epoch: 46 [640/900 (70%)]\tLoss: 0.001025\n",
      "Training epoch: 46 [800/900 (88%)]\tLoss: 0.004362\n",
      "=========> Epoch: 46 Average loss: 0.0089\n",
      "Correlation coefficient: 0.6758\n",
      "Training epoch: 47 [0/900 (0%)]\tLoss: 0.002974\n",
      "Training epoch: 47 [160/900 (18%)]\tLoss: 0.008816\n",
      "Training epoch: 47 [320/900 (35%)]\tLoss: 0.004161\n",
      "Training epoch: 47 [480/900 (53%)]\tLoss: 0.004856\n",
      "Training epoch: 47 [640/900 (70%)]\tLoss: 0.007942\n",
      "Training epoch: 47 [800/900 (88%)]\tLoss: 0.007102\n",
      "=========> Epoch: 47 Average loss: 0.0063\n",
      "Correlation coefficient: 0.6760\n",
      "Training epoch: 48 [0/900 (0%)]\tLoss: 0.004938\n",
      "Training epoch: 48 [160/900 (18%)]\tLoss: 0.007933\n",
      "Training epoch: 48 [320/900 (35%)]\tLoss: 0.004915\n",
      "Training epoch: 48 [480/900 (53%)]\tLoss: 0.010123\n",
      "Training epoch: 48 [640/900 (70%)]\tLoss: 0.008595\n",
      "Training epoch: 48 [800/900 (88%)]\tLoss: 0.003271\n",
      "=========> Epoch: 48 Average loss: 0.0056\n",
      "Correlation coefficient: 0.6764\n",
      "Training epoch: 49 [0/900 (0%)]\tLoss: 0.005463\n",
      "Training epoch: 49 [160/900 (18%)]\tLoss: 0.006066\n",
      "Training epoch: 49 [320/900 (35%)]\tLoss: 0.012507\n",
      "Training epoch: 49 [480/900 (53%)]\tLoss: 0.006912\n",
      "Training epoch: 49 [640/900 (70%)]\tLoss: 0.002837\n",
      "Training epoch: 49 [800/900 (88%)]\tLoss: 0.005315\n",
      "=========> Epoch: 49 Average loss: 0.0064\n",
      "Correlation coefficient: 0.6697\n",
      "Training epoch: 50 [0/900 (0%)]\tLoss: 0.008235\n",
      "Training epoch: 50 [160/900 (18%)]\tLoss: 0.003905\n",
      "Training epoch: 50 [320/900 (35%)]\tLoss: 0.006706\n",
      "Training epoch: 50 [480/900 (53%)]\tLoss: 0.004593\n",
      "Training epoch: 50 [640/900 (70%)]\tLoss: 0.002810\n",
      "Training epoch: 50 [800/900 (88%)]\tLoss: 0.002885\n",
      "=========> Epoch: 50 Average loss: 0.0068\n",
      "Correlation coefficient: 0.6764\n",
      "Training epoch: 51 [0/900 (0%)]\tLoss: 0.004123\n",
      "Training epoch: 51 [160/900 (18%)]\tLoss: 0.013885\n",
      "Training epoch: 51 [320/900 (35%)]\tLoss: 0.002864\n",
      "Training epoch: 51 [480/900 (53%)]\tLoss: 0.001432\n",
      "Training epoch: 51 [640/900 (70%)]\tLoss: 0.003294\n",
      "Training epoch: 51 [800/900 (88%)]\tLoss: 0.003678\n",
      "=========> Epoch: 51 Average loss: 0.0051\n",
      "Correlation coefficient: 0.6803\n",
      "Training epoch: 52 [0/900 (0%)]\tLoss: 0.008571\n",
      "Training epoch: 52 [160/900 (18%)]\tLoss: 0.001895\n",
      "Training epoch: 52 [320/900 (35%)]\tLoss: 0.003278\n",
      "Training epoch: 52 [480/900 (53%)]\tLoss: 0.008673\n",
      "Training epoch: 52 [640/900 (70%)]\tLoss: 0.006324\n",
      "Training epoch: 52 [800/900 (88%)]\tLoss: 0.004715\n",
      "=========> Epoch: 52 Average loss: 0.0044\n",
      "Correlation coefficient: 0.6701\n",
      "Training epoch: 53 [0/900 (0%)]\tLoss: 0.003843\n",
      "Training epoch: 53 [160/900 (18%)]\tLoss: 0.002187\n",
      "Training epoch: 53 [320/900 (35%)]\tLoss: 0.001754\n",
      "Training epoch: 53 [480/900 (53%)]\tLoss: 0.002424\n",
      "Training epoch: 53 [640/900 (70%)]\tLoss: 0.002488\n",
      "Training epoch: 53 [800/900 (88%)]\tLoss: 0.002794\n",
      "=========> Epoch: 53 Average loss: 0.0034\n",
      "Correlation coefficient: 0.6786\n",
      "Training epoch: 54 [0/900 (0%)]\tLoss: 0.002960\n",
      "Training epoch: 54 [160/900 (18%)]\tLoss: 0.002947\n",
      "Training epoch: 54 [320/900 (35%)]\tLoss: 0.005567\n",
      "Training epoch: 54 [480/900 (53%)]\tLoss: 0.002349\n",
      "Training epoch: 54 [640/900 (70%)]\tLoss: 0.000915\n",
      "Training epoch: 54 [800/900 (88%)]\tLoss: 0.004243\n",
      "=========> Epoch: 54 Average loss: 0.0031\n",
      "Correlation coefficient: 0.6753\n",
      "Training epoch: 55 [0/900 (0%)]\tLoss: 0.002304\n",
      "Training epoch: 55 [160/900 (18%)]\tLoss: 0.002389\n",
      "Training epoch: 55 [320/900 (35%)]\tLoss: 0.002027\n",
      "Training epoch: 55 [480/900 (53%)]\tLoss: 0.001092\n",
      "Training epoch: 55 [640/900 (70%)]\tLoss: 0.001203\n",
      "Training epoch: 55 [800/900 (88%)]\tLoss: 0.002498\n",
      "=========> Epoch: 55 Average loss: 0.0027\n",
      "Correlation coefficient: 0.6781\n",
      "Training epoch: 56 [0/900 (0%)]\tLoss: 0.001531\n",
      "Training epoch: 56 [160/900 (18%)]\tLoss: 0.001807\n",
      "Training epoch: 56 [320/900 (35%)]\tLoss: 0.002188\n",
      "Training epoch: 56 [480/900 (53%)]\tLoss: 0.002506\n",
      "Training epoch: 56 [640/900 (70%)]\tLoss: 0.001016\n",
      "Training epoch: 56 [800/900 (88%)]\tLoss: 0.002777\n",
      "=========> Epoch: 56 Average loss: 0.0021\n",
      "Correlation coefficient: 0.6745\n",
      "⏹️  Epoch 56 early stopping (no improvement for 50 epochs)\n",
      "🏁 Fold 3 best correlation: 0.6893\n",
      "\n",
      "========== Cross-validation Fold 4/10 ==========\n",
      "🔄 Fold 4: Using random initialization\n",
      "Training epoch: 1 [0/900 (0%)]\tLoss: 0.882991\n",
      "Training epoch: 1 [160/900 (18%)]\tLoss: 0.938406\n",
      "Training epoch: 1 [320/900 (35%)]\tLoss: 1.163005\n",
      "Training epoch: 1 [480/900 (53%)]\tLoss: 1.160232\n",
      "Training epoch: 1 [640/900 (70%)]\tLoss: 0.430929\n",
      "Training epoch: 1 [800/900 (88%)]\tLoss: 0.494627\n",
      "=========> Epoch: 1 Average loss: 0.7676\n",
      "Correlation coefficient: 0.6213\n",
      "✅ Epoch 1: New best correlation = 0.6213\n",
      "Training epoch: 2 [0/900 (0%)]\tLoss: 0.278007\n",
      "Training epoch: 2 [160/900 (18%)]\tLoss: 0.518851\n",
      "Training epoch: 2 [320/900 (35%)]\tLoss: 0.213617\n",
      "Training epoch: 2 [480/900 (53%)]\tLoss: 0.407954\n",
      "Training epoch: 2 [640/900 (70%)]\tLoss: 0.685854\n",
      "Training epoch: 2 [800/900 (88%)]\tLoss: 0.391921\n",
      "=========> Epoch: 2 Average loss: 0.3691\n",
      "Correlation coefficient: 0.5955\n",
      "Training epoch: 3 [0/900 (0%)]\tLoss: 0.085659\n",
      "Training epoch: 3 [160/900 (18%)]\tLoss: 0.067996\n",
      "Training epoch: 3 [320/900 (35%)]\tLoss: 0.128924\n",
      "Training epoch: 3 [480/900 (53%)]\tLoss: 0.063795\n",
      "Training epoch: 3 [640/900 (70%)]\tLoss: 0.070849\n",
      "Training epoch: 3 [800/900 (88%)]\tLoss: 0.093959\n",
      "=========> Epoch: 3 Average loss: 0.1277\n",
      "Correlation coefficient: 0.5604\n",
      "Training epoch: 4 [0/900 (0%)]\tLoss: 0.021070\n",
      "Training epoch: 4 [160/900 (18%)]\tLoss: 0.024672\n",
      "Training epoch: 4 [320/900 (35%)]\tLoss: 0.051980\n",
      "Training epoch: 4 [480/900 (53%)]\tLoss: 0.028002\n",
      "Training epoch: 4 [640/900 (70%)]\tLoss: 0.128957\n",
      "Training epoch: 4 [800/900 (88%)]\tLoss: 0.060941\n",
      "=========> Epoch: 4 Average loss: 0.0603\n",
      "Correlation coefficient: 0.5555\n",
      "Training epoch: 5 [0/900 (0%)]\tLoss: 0.055879\n",
      "Training epoch: 5 [160/900 (18%)]\tLoss: 0.103541\n",
      "Training epoch: 5 [320/900 (35%)]\tLoss: 0.014968\n",
      "Training epoch: 5 [480/900 (53%)]\tLoss: 0.030315\n",
      "Training epoch: 5 [640/900 (70%)]\tLoss: 0.031083\n",
      "Training epoch: 5 [800/900 (88%)]\tLoss: 0.020845\n",
      "=========> Epoch: 5 Average loss: 0.0322\n",
      "Correlation coefficient: 0.5674\n",
      "Training epoch: 6 [0/900 (0%)]\tLoss: 0.019964\n",
      "Training epoch: 6 [160/900 (18%)]\tLoss: 0.027458\n",
      "Training epoch: 6 [320/900 (35%)]\tLoss: 0.015764\n",
      "Training epoch: 6 [480/900 (53%)]\tLoss: 0.018086\n",
      "Training epoch: 6 [640/900 (70%)]\tLoss: 0.009972\n",
      "Training epoch: 6 [800/900 (88%)]\tLoss: 0.006488\n",
      "=========> Epoch: 6 Average loss: 0.0178\n",
      "Correlation coefficient: 0.5799\n",
      "Training epoch: 7 [0/900 (0%)]\tLoss: 0.004674\n",
      "Training epoch: 7 [160/900 (18%)]\tLoss: 0.020722\n",
      "Training epoch: 7 [320/900 (35%)]\tLoss: 0.007896\n",
      "Training epoch: 7 [480/900 (53%)]\tLoss: 0.007090\n",
      "Training epoch: 7 [640/900 (70%)]\tLoss: 0.009793\n",
      "Training epoch: 7 [800/900 (88%)]\tLoss: 0.009841\n",
      "=========> Epoch: 7 Average loss: 0.0136\n",
      "Correlation coefficient: 0.5757\n",
      "Training epoch: 8 [0/900 (0%)]\tLoss: 0.012328\n",
      "Training epoch: 8 [160/900 (18%)]\tLoss: 0.008154\n",
      "Training epoch: 8 [320/900 (35%)]\tLoss: 0.012791\n",
      "Training epoch: 8 [480/900 (53%)]\tLoss: 0.013499\n",
      "Training epoch: 8 [640/900 (70%)]\tLoss: 0.003024\n",
      "Training epoch: 8 [800/900 (88%)]\tLoss: 0.010836\n",
      "=========> Epoch: 8 Average loss: 0.0103\n",
      "Correlation coefficient: 0.5823\n",
      "Training epoch: 9 [0/900 (0%)]\tLoss: 0.006882\n",
      "Training epoch: 9 [160/900 (18%)]\tLoss: 0.004288\n",
      "Training epoch: 9 [320/900 (35%)]\tLoss: 0.009796\n",
      "Training epoch: 9 [480/900 (53%)]\tLoss: 0.003636\n",
      "Training epoch: 9 [640/900 (70%)]\tLoss: 0.005184\n",
      "Training epoch: 9 [800/900 (88%)]\tLoss: 0.004216\n",
      "=========> Epoch: 9 Average loss: 0.0079\n",
      "Correlation coefficient: 0.5883\n",
      "Training epoch: 10 [0/900 (0%)]\tLoss: 0.006212\n",
      "Training epoch: 10 [160/900 (18%)]\tLoss: 0.004704\n",
      "Training epoch: 10 [320/900 (35%)]\tLoss: 0.009792\n",
      "Training epoch: 10 [480/900 (53%)]\tLoss: 0.006407\n",
      "Training epoch: 10 [640/900 (70%)]\tLoss: 0.002924\n",
      "Training epoch: 10 [800/900 (88%)]\tLoss: 0.002391\n",
      "=========> Epoch: 10 Average loss: 0.0059\n",
      "Correlation coefficient: 0.5802\n",
      "Training epoch: 11 [0/900 (0%)]\tLoss: 0.002628\n",
      "Training epoch: 11 [160/900 (18%)]\tLoss: 0.004082\n",
      "Training epoch: 11 [320/900 (35%)]\tLoss: 0.001447\n",
      "Training epoch: 11 [480/900 (53%)]\tLoss: 0.001154\n",
      "Training epoch: 11 [640/900 (70%)]\tLoss: 0.015633\n",
      "Training epoch: 11 [800/900 (88%)]\tLoss: 0.009287\n",
      "=========> Epoch: 11 Average loss: 0.0052\n",
      "Correlation coefficient: 0.5810\n",
      "Training epoch: 12 [0/900 (0%)]\tLoss: 0.006745\n",
      "Training epoch: 12 [160/900 (18%)]\tLoss: 0.003304\n",
      "Training epoch: 12 [320/900 (35%)]\tLoss: 0.006048\n",
      "Training epoch: 12 [480/900 (53%)]\tLoss: 0.007841\n",
      "Training epoch: 12 [640/900 (70%)]\tLoss: 0.007416\n",
      "Training epoch: 12 [800/900 (88%)]\tLoss: 0.002622\n",
      "=========> Epoch: 12 Average loss: 0.0061\n",
      "Correlation coefficient: 0.5917\n",
      "Training epoch: 13 [0/900 (0%)]\tLoss: 0.004605\n",
      "Training epoch: 13 [160/900 (18%)]\tLoss: 0.006479\n",
      "Training epoch: 13 [320/900 (35%)]\tLoss: 0.014059\n",
      "Training epoch: 13 [480/900 (53%)]\tLoss: 0.023890\n",
      "Training epoch: 13 [640/900 (70%)]\tLoss: 0.006414\n",
      "Training epoch: 13 [800/900 (88%)]\tLoss: 0.009229\n",
      "=========> Epoch: 13 Average loss: 0.0077\n",
      "Correlation coefficient: 0.5757\n",
      "Training epoch: 14 [0/900 (0%)]\tLoss: 0.008512\n",
      "Training epoch: 14 [160/900 (18%)]\tLoss: 0.006722\n",
      "Training epoch: 14 [320/900 (35%)]\tLoss: 0.013939\n",
      "Training epoch: 14 [480/900 (53%)]\tLoss: 0.009901\n",
      "Training epoch: 14 [640/900 (70%)]\tLoss: 0.017094\n",
      "Training epoch: 14 [800/900 (88%)]\tLoss: 0.004850\n",
      "=========> Epoch: 14 Average loss: 0.0101\n",
      "Correlation coefficient: 0.5829\n",
      "Training epoch: 15 [0/900 (0%)]\tLoss: 0.020454\n",
      "Training epoch: 15 [160/900 (18%)]\tLoss: 0.028559\n",
      "Training epoch: 15 [320/900 (35%)]\tLoss: 0.009634\n",
      "Training epoch: 15 [480/900 (53%)]\tLoss: 0.005752\n",
      "Training epoch: 15 [640/900 (70%)]\tLoss: 0.030297\n",
      "Training epoch: 15 [800/900 (88%)]\tLoss: 0.013489\n",
      "=========> Epoch: 15 Average loss: 0.0235\n",
      "Correlation coefficient: 0.5760\n",
      "Training epoch: 16 [0/900 (0%)]\tLoss: 0.054339\n",
      "Training epoch: 16 [160/900 (18%)]\tLoss: 0.020699\n",
      "Training epoch: 16 [320/900 (35%)]\tLoss: 0.027275\n",
      "Training epoch: 16 [480/900 (53%)]\tLoss: 0.025107\n",
      "Training epoch: 16 [640/900 (70%)]\tLoss: 0.019963\n",
      "Training epoch: 16 [800/900 (88%)]\tLoss: 0.031809\n",
      "=========> Epoch: 16 Average loss: 0.0326\n",
      "Correlation coefficient: 0.5995\n",
      "Training epoch: 17 [0/900 (0%)]\tLoss: 0.028009\n",
      "Training epoch: 17 [160/900 (18%)]\tLoss: 0.026509\n",
      "Training epoch: 17 [320/900 (35%)]\tLoss: 0.032903\n",
      "Training epoch: 17 [480/900 (53%)]\tLoss: 0.221491\n",
      "Training epoch: 17 [640/900 (70%)]\tLoss: 0.010759\n",
      "Training epoch: 17 [800/900 (88%)]\tLoss: 0.014525\n",
      "=========> Epoch: 17 Average loss: 0.0327\n",
      "Correlation coefficient: 0.5758\n",
      "Training epoch: 18 [0/900 (0%)]\tLoss: 0.011565\n",
      "Training epoch: 18 [160/900 (18%)]\tLoss: 0.005597\n",
      "Training epoch: 18 [320/900 (35%)]\tLoss: 0.015382\n",
      "Training epoch: 18 [480/900 (53%)]\tLoss: 0.016814\n",
      "Training epoch: 18 [640/900 (70%)]\tLoss: 0.022331\n",
      "Training epoch: 18 [800/900 (88%)]\tLoss: 0.027157\n",
      "=========> Epoch: 18 Average loss: 0.0240\n",
      "Correlation coefficient: 0.5881\n",
      "Training epoch: 19 [0/900 (0%)]\tLoss: 0.017487\n",
      "Training epoch: 19 [160/900 (18%)]\tLoss: 0.019728\n",
      "Training epoch: 19 [320/900 (35%)]\tLoss: 0.034243\n",
      "Training epoch: 19 [480/900 (53%)]\tLoss: 0.017228\n",
      "Training epoch: 19 [640/900 (70%)]\tLoss: 0.020316\n",
      "Training epoch: 19 [800/900 (88%)]\tLoss: 0.014664\n",
      "=========> Epoch: 19 Average loss: 0.0213\n",
      "Correlation coefficient: 0.5743\n",
      "Training epoch: 20 [0/900 (0%)]\tLoss: 0.010067\n",
      "Training epoch: 20 [160/900 (18%)]\tLoss: 0.013909\n",
      "Training epoch: 20 [320/900 (35%)]\tLoss: 0.012202\n",
      "Training epoch: 20 [480/900 (53%)]\tLoss: 0.006055\n",
      "Training epoch: 20 [640/900 (70%)]\tLoss: 0.015159\n",
      "Training epoch: 20 [800/900 (88%)]\tLoss: 0.031275\n",
      "=========> Epoch: 20 Average loss: 0.0141\n",
      "Correlation coefficient: 0.5834\n",
      "Training epoch: 21 [0/900 (0%)]\tLoss: 0.010682\n",
      "Training epoch: 21 [160/900 (18%)]\tLoss: 0.007876\n",
      "Training epoch: 21 [320/900 (35%)]\tLoss: 0.007142\n",
      "Training epoch: 21 [480/900 (53%)]\tLoss: 0.014461\n",
      "Training epoch: 21 [640/900 (70%)]\tLoss: 0.014686\n",
      "Training epoch: 21 [800/900 (88%)]\tLoss: 0.010273\n",
      "=========> Epoch: 21 Average loss: 0.0098\n",
      "Correlation coefficient: 0.5792\n",
      "Training epoch: 22 [0/900 (0%)]\tLoss: 0.011786\n",
      "Training epoch: 22 [160/900 (18%)]\tLoss: 0.006225\n",
      "Training epoch: 22 [320/900 (35%)]\tLoss: 0.003336\n",
      "Training epoch: 22 [480/900 (53%)]\tLoss: 0.012568\n",
      "Training epoch: 22 [640/900 (70%)]\tLoss: 0.002786\n",
      "Training epoch: 22 [800/900 (88%)]\tLoss: 0.002320\n",
      "=========> Epoch: 22 Average loss: 0.0070\n",
      "Correlation coefficient: 0.5701\n",
      "Training epoch: 23 [0/900 (0%)]\tLoss: 0.001302\n",
      "Training epoch: 23 [160/900 (18%)]\tLoss: 0.007711\n",
      "Training epoch: 23 [320/900 (35%)]\tLoss: 0.005088\n",
      "Training epoch: 23 [480/900 (53%)]\tLoss: 0.005776\n",
      "Training epoch: 23 [640/900 (70%)]\tLoss: 0.003050\n",
      "Training epoch: 23 [800/900 (88%)]\tLoss: 0.002649\n",
      "=========> Epoch: 23 Average loss: 0.0058\n",
      "Correlation coefficient: 0.5821\n",
      "Training epoch: 24 [0/900 (0%)]\tLoss: 0.007534\n",
      "Training epoch: 24 [160/900 (18%)]\tLoss: 0.006641\n",
      "Training epoch: 24 [320/900 (35%)]\tLoss: 0.011719\n",
      "Training epoch: 24 [480/900 (53%)]\tLoss: 0.004448\n",
      "Training epoch: 24 [640/900 (70%)]\tLoss: 0.004065\n",
      "Training epoch: 24 [800/900 (88%)]\tLoss: 0.002186\n",
      "=========> Epoch: 24 Average loss: 0.0053\n",
      "Correlation coefficient: 0.5838\n",
      "Training epoch: 25 [0/900 (0%)]\tLoss: 0.002323\n",
      "Training epoch: 25 [160/900 (18%)]\tLoss: 0.003096\n",
      "Training epoch: 25 [320/900 (35%)]\tLoss: 0.000887\n",
      "Training epoch: 25 [480/900 (53%)]\tLoss: 0.004708\n",
      "Training epoch: 25 [640/900 (70%)]\tLoss: 0.003818\n",
      "Training epoch: 25 [800/900 (88%)]\tLoss: 0.003839\n",
      "=========> Epoch: 25 Average loss: 0.0062\n",
      "Correlation coefficient: 0.5849\n",
      "Training epoch: 26 [0/900 (0%)]\tLoss: 0.004596\n",
      "Training epoch: 26 [160/900 (18%)]\tLoss: 0.006312\n",
      "Training epoch: 26 [320/900 (35%)]\tLoss: 0.006419\n",
      "Training epoch: 26 [480/900 (53%)]\tLoss: 0.004139\n",
      "Training epoch: 26 [640/900 (70%)]\tLoss: 0.012939\n",
      "Training epoch: 26 [800/900 (88%)]\tLoss: 0.001667\n",
      "=========> Epoch: 26 Average loss: 0.0056\n",
      "Correlation coefficient: 0.5755\n",
      "Training epoch: 27 [0/900 (0%)]\tLoss: 0.005028\n",
      "Training epoch: 27 [160/900 (18%)]\tLoss: 0.008276\n",
      "Training epoch: 27 [320/900 (35%)]\tLoss: 0.005752\n",
      "Training epoch: 27 [480/900 (53%)]\tLoss: 0.004277\n",
      "Training epoch: 27 [640/900 (70%)]\tLoss: 0.001776\n",
      "Training epoch: 27 [800/900 (88%)]\tLoss: 0.004175\n",
      "=========> Epoch: 27 Average loss: 0.0068\n",
      "Correlation coefficient: 0.5987\n",
      "Training epoch: 28 [0/900 (0%)]\tLoss: 0.003901\n",
      "Training epoch: 28 [160/900 (18%)]\tLoss: 0.003933\n",
      "Training epoch: 28 [320/900 (35%)]\tLoss: 0.005831\n",
      "Training epoch: 28 [480/900 (53%)]\tLoss: 0.005124\n",
      "Training epoch: 28 [640/900 (70%)]\tLoss: 0.004504\n",
      "Training epoch: 28 [800/900 (88%)]\tLoss: 0.002660\n",
      "=========> Epoch: 28 Average loss: 0.0057\n",
      "Correlation coefficient: 0.5766\n",
      "Training epoch: 29 [0/900 (0%)]\tLoss: 0.006513\n",
      "Training epoch: 29 [160/900 (18%)]\tLoss: 0.004747\n",
      "Training epoch: 29 [320/900 (35%)]\tLoss: 0.003737\n",
      "Training epoch: 29 [480/900 (53%)]\tLoss: 0.005012\n",
      "Training epoch: 29 [640/900 (70%)]\tLoss: 0.005900\n",
      "Training epoch: 29 [800/900 (88%)]\tLoss: 0.002267\n",
      "=========> Epoch: 29 Average loss: 0.0057\n",
      "Correlation coefficient: 0.5827\n",
      "Training epoch: 30 [0/900 (0%)]\tLoss: 0.002313\n",
      "Training epoch: 30 [160/900 (18%)]\tLoss: 0.011666\n",
      "Training epoch: 30 [320/900 (35%)]\tLoss: 0.010483\n",
      "Training epoch: 30 [480/900 (53%)]\tLoss: 0.037090\n",
      "Training epoch: 30 [640/900 (70%)]\tLoss: 0.002620\n",
      "Training epoch: 30 [800/900 (88%)]\tLoss: 0.002990\n",
      "=========> Epoch: 30 Average loss: 0.0066\n",
      "Correlation coefficient: 0.5888\n",
      "Training epoch: 31 [0/900 (0%)]\tLoss: 0.005833\n",
      "Training epoch: 31 [160/900 (18%)]\tLoss: 0.021305\n",
      "Training epoch: 31 [320/900 (35%)]\tLoss: 0.006305\n",
      "Training epoch: 31 [480/900 (53%)]\tLoss: 0.006722\n",
      "Training epoch: 31 [640/900 (70%)]\tLoss: 0.001041\n",
      "Training epoch: 31 [800/900 (88%)]\tLoss: 0.011297\n",
      "=========> Epoch: 31 Average loss: 0.0087\n",
      "Correlation coefficient: 0.5830\n",
      "Training epoch: 32 [0/900 (0%)]\tLoss: 0.008607\n",
      "Training epoch: 32 [160/900 (18%)]\tLoss: 0.009421\n",
      "Training epoch: 32 [320/900 (35%)]\tLoss: 0.014527\n",
      "Training epoch: 32 [480/900 (53%)]\tLoss: 0.003253\n",
      "Training epoch: 32 [640/900 (70%)]\tLoss: 0.011215\n",
      "Training epoch: 32 [800/900 (88%)]\tLoss: 0.025441\n",
      "=========> Epoch: 32 Average loss: 0.0138\n",
      "Correlation coefficient: 0.5947\n",
      "Training epoch: 33 [0/900 (0%)]\tLoss: 0.012197\n",
      "Training epoch: 33 [160/900 (18%)]\tLoss: 0.007748\n",
      "Training epoch: 33 [320/900 (35%)]\tLoss: 0.011710\n",
      "Training epoch: 33 [480/900 (53%)]\tLoss: 0.027380\n",
      "Training epoch: 33 [640/900 (70%)]\tLoss: 0.020687\n",
      "Training epoch: 33 [800/900 (88%)]\tLoss: 0.006130\n",
      "=========> Epoch: 33 Average loss: 0.0173\n",
      "Correlation coefficient: 0.5749\n",
      "Training epoch: 34 [0/900 (0%)]\tLoss: 0.004620\n",
      "Training epoch: 34 [160/900 (18%)]\tLoss: 0.009734\n",
      "Training epoch: 34 [320/900 (35%)]\tLoss: 0.019202\n",
      "Training epoch: 34 [480/900 (53%)]\tLoss: 0.011984\n",
      "Training epoch: 34 [640/900 (70%)]\tLoss: 0.012666\n",
      "Training epoch: 34 [800/900 (88%)]\tLoss: 0.022356\n",
      "=========> Epoch: 34 Average loss: 0.0213\n",
      "Correlation coefficient: 0.5905\n",
      "Training epoch: 35 [0/900 (0%)]\tLoss: 0.033160\n",
      "Training epoch: 35 [160/900 (18%)]\tLoss: 0.014037\n",
      "Training epoch: 35 [320/900 (35%)]\tLoss: 0.010850\n",
      "Training epoch: 35 [480/900 (53%)]\tLoss: 0.015541\n",
      "Training epoch: 35 [640/900 (70%)]\tLoss: 0.054893\n",
      "Training epoch: 35 [800/900 (88%)]\tLoss: 0.006432\n",
      "=========> Epoch: 35 Average loss: 0.0187\n",
      "Correlation coefficient: 0.5987\n",
      "Training epoch: 36 [0/900 (0%)]\tLoss: 0.010466\n",
      "Training epoch: 36 [160/900 (18%)]\tLoss: 0.017035\n",
      "Training epoch: 36 [320/900 (35%)]\tLoss: 0.036769\n",
      "Training epoch: 36 [480/900 (53%)]\tLoss: 0.014043\n",
      "Training epoch: 36 [640/900 (70%)]\tLoss: 0.017652\n",
      "Training epoch: 36 [800/900 (88%)]\tLoss: 0.045998\n",
      "=========> Epoch: 36 Average loss: 0.0253\n",
      "Correlation coefficient: 0.5850\n",
      "Training epoch: 37 [0/900 (0%)]\tLoss: 0.033425\n",
      "Training epoch: 37 [160/900 (18%)]\tLoss: 0.020805\n",
      "Training epoch: 37 [320/900 (35%)]\tLoss: 0.015267\n",
      "Training epoch: 37 [480/900 (53%)]\tLoss: 0.022160\n",
      "Training epoch: 37 [640/900 (70%)]\tLoss: 0.039424\n",
      "Training epoch: 37 [800/900 (88%)]\tLoss: 0.013962\n",
      "=========> Epoch: 37 Average loss: 0.0228\n",
      "Correlation coefficient: 0.5859\n",
      "Training epoch: 38 [0/900 (0%)]\tLoss: 0.020103\n",
      "Training epoch: 38 [160/900 (18%)]\tLoss: 0.014375\n",
      "Training epoch: 38 [320/900 (35%)]\tLoss: 0.009090\n",
      "Training epoch: 38 [480/900 (53%)]\tLoss: 0.036754\n",
      "Training epoch: 38 [640/900 (70%)]\tLoss: 0.025084\n",
      "Training epoch: 38 [800/900 (88%)]\tLoss: 0.022781\n",
      "=========> Epoch: 38 Average loss: 0.0211\n",
      "Correlation coefficient: 0.5709\n",
      "Training epoch: 39 [0/900 (0%)]\tLoss: 0.009850\n",
      "Training epoch: 39 [160/900 (18%)]\tLoss: 0.028682\n",
      "Training epoch: 39 [320/900 (35%)]\tLoss: 0.058766\n",
      "Training epoch: 39 [480/900 (53%)]\tLoss: 0.023874\n",
      "Training epoch: 39 [640/900 (70%)]\tLoss: 0.025908\n",
      "Training epoch: 39 [800/900 (88%)]\tLoss: 0.010715\n",
      "=========> Epoch: 39 Average loss: 0.0249\n",
      "Correlation coefficient: 0.5731\n",
      "Training epoch: 40 [0/900 (0%)]\tLoss: 0.023737\n",
      "Training epoch: 40 [160/900 (18%)]\tLoss: 0.105158\n",
      "Training epoch: 40 [320/900 (35%)]\tLoss: 0.026664\n",
      "Training epoch: 40 [480/900 (53%)]\tLoss: 0.020762\n",
      "Training epoch: 40 [640/900 (70%)]\tLoss: 0.007792\n",
      "Training epoch: 40 [800/900 (88%)]\tLoss: 0.008552\n",
      "=========> Epoch: 40 Average loss: 0.0233\n",
      "Correlation coefficient: 0.5961\n",
      "Training epoch: 41 [0/900 (0%)]\tLoss: 0.019221\n",
      "Training epoch: 41 [160/900 (18%)]\tLoss: 0.014101\n",
      "Training epoch: 41 [320/900 (35%)]\tLoss: 0.022887\n",
      "Training epoch: 41 [480/900 (53%)]\tLoss: 0.013357\n",
      "Training epoch: 41 [640/900 (70%)]\tLoss: 0.010891\n",
      "Training epoch: 41 [800/900 (88%)]\tLoss: 0.006867\n",
      "=========> Epoch: 41 Average loss: 0.0182\n",
      "Correlation coefficient: 0.5821\n",
      "Training epoch: 42 [0/900 (0%)]\tLoss: 0.008697\n",
      "Training epoch: 42 [160/900 (18%)]\tLoss: 0.012241\n",
      "Training epoch: 42 [320/900 (35%)]\tLoss: 0.010885\n",
      "Training epoch: 42 [480/900 (53%)]\tLoss: 0.016189\n",
      "Training epoch: 42 [640/900 (70%)]\tLoss: 0.025262\n",
      "Training epoch: 42 [800/900 (88%)]\tLoss: 0.020691\n",
      "=========> Epoch: 42 Average loss: 0.0139\n",
      "Correlation coefficient: 0.5968\n",
      "Training epoch: 43 [0/900 (0%)]\tLoss: 0.010009\n",
      "Training epoch: 43 [160/900 (18%)]\tLoss: 0.004587\n",
      "Training epoch: 43 [320/900 (35%)]\tLoss: 0.005435\n",
      "Training epoch: 43 [480/900 (53%)]\tLoss: 0.008209\n",
      "Training epoch: 43 [640/900 (70%)]\tLoss: 0.018984\n",
      "Training epoch: 43 [800/900 (88%)]\tLoss: 0.002758\n",
      "=========> Epoch: 43 Average loss: 0.0112\n",
      "Correlation coefficient: 0.5963\n",
      "Training epoch: 44 [0/900 (0%)]\tLoss: 0.002735\n",
      "Training epoch: 44 [160/900 (18%)]\tLoss: 0.005175\n",
      "Training epoch: 44 [320/900 (35%)]\tLoss: 0.005100\n",
      "Training epoch: 44 [480/900 (53%)]\tLoss: 0.006912\n",
      "Training epoch: 44 [640/900 (70%)]\tLoss: 0.005042\n",
      "Training epoch: 44 [800/900 (88%)]\tLoss: 0.010530\n",
      "=========> Epoch: 44 Average loss: 0.0088\n",
      "Correlation coefficient: 0.5974\n",
      "Training epoch: 45 [0/900 (0%)]\tLoss: 0.004351\n",
      "Training epoch: 45 [160/900 (18%)]\tLoss: 0.008778\n",
      "Training epoch: 45 [320/900 (35%)]\tLoss: 0.001838\n",
      "Training epoch: 45 [480/900 (53%)]\tLoss: 0.008048\n",
      "Training epoch: 45 [640/900 (70%)]\tLoss: 0.003466\n",
      "Training epoch: 45 [800/900 (88%)]\tLoss: 0.000770\n",
      "=========> Epoch: 45 Average loss: 0.0088\n",
      "Correlation coefficient: 0.5908\n",
      "Training epoch: 46 [0/900 (0%)]\tLoss: 0.006681\n",
      "Training epoch: 46 [160/900 (18%)]\tLoss: 0.008091\n",
      "Training epoch: 46 [320/900 (35%)]\tLoss: 0.006058\n",
      "Training epoch: 46 [480/900 (53%)]\tLoss: 0.011392\n",
      "Training epoch: 46 [640/900 (70%)]\tLoss: 0.010046\n",
      "Training epoch: 46 [800/900 (88%)]\tLoss: 0.004710\n",
      "=========> Epoch: 46 Average loss: 0.0067\n",
      "Correlation coefficient: 0.5964\n",
      "Training epoch: 47 [0/900 (0%)]\tLoss: 0.001758\n",
      "Training epoch: 47 [160/900 (18%)]\tLoss: 0.002097\n",
      "Training epoch: 47 [320/900 (35%)]\tLoss: 0.002338\n",
      "Training epoch: 47 [480/900 (53%)]\tLoss: 0.003315\n",
      "Training epoch: 47 [640/900 (70%)]\tLoss: 0.001911\n",
      "Training epoch: 47 [800/900 (88%)]\tLoss: 0.001657\n",
      "=========> Epoch: 47 Average loss: 0.0034\n",
      "Correlation coefficient: 0.5967\n",
      "Training epoch: 48 [0/900 (0%)]\tLoss: 0.000882\n",
      "Training epoch: 48 [160/900 (18%)]\tLoss: 0.003578\n",
      "Training epoch: 48 [320/900 (35%)]\tLoss: 0.006022\n",
      "Training epoch: 48 [480/900 (53%)]\tLoss: 0.001068\n",
      "Training epoch: 48 [640/900 (70%)]\tLoss: 0.004464\n",
      "Training epoch: 48 [800/900 (88%)]\tLoss: 0.002095\n",
      "=========> Epoch: 48 Average loss: 0.0029\n",
      "Correlation coefficient: 0.5930\n",
      "Training epoch: 49 [0/900 (0%)]\tLoss: 0.002081\n",
      "Training epoch: 49 [160/900 (18%)]\tLoss: 0.000946\n",
      "Training epoch: 49 [320/900 (35%)]\tLoss: 0.005694\n",
      "Training epoch: 49 [480/900 (53%)]\tLoss: 0.002992\n",
      "Training epoch: 49 [640/900 (70%)]\tLoss: 0.001277\n",
      "Training epoch: 49 [800/900 (88%)]\tLoss: 0.012807\n",
      "=========> Epoch: 49 Average loss: 0.0024\n",
      "Correlation coefficient: 0.5967\n",
      "Training epoch: 50 [0/900 (0%)]\tLoss: 0.003278\n",
      "Training epoch: 50 [160/900 (18%)]\tLoss: 0.002907\n",
      "Training epoch: 50 [320/900 (35%)]\tLoss: 0.004803\n",
      "Training epoch: 50 [480/900 (53%)]\tLoss: 0.005486\n",
      "Training epoch: 50 [640/900 (70%)]\tLoss: 0.001012\n",
      "Training epoch: 50 [800/900 (88%)]\tLoss: 0.001991\n",
      "=========> Epoch: 50 Average loss: 0.0032\n",
      "Correlation coefficient: 0.5925\n",
      "Training epoch: 51 [0/900 (0%)]\tLoss: 0.003096\n",
      "Training epoch: 51 [160/900 (18%)]\tLoss: 0.005048\n",
      "Training epoch: 51 [320/900 (35%)]\tLoss: 0.003263\n",
      "Training epoch: 51 [480/900 (53%)]\tLoss: 0.002543\n",
      "Training epoch: 51 [640/900 (70%)]\tLoss: 0.002918\n",
      "Training epoch: 51 [800/900 (88%)]\tLoss: 0.003557\n",
      "=========> Epoch: 51 Average loss: 0.0032\n",
      "Correlation coefficient: 0.5980\n",
      "⏹️  Epoch 51 early stopping (no improvement for 50 epochs)\n",
      "🏁 Fold 4 best correlation: 0.6213\n",
      "\n",
      "========== Cross-validation Fold 5/10 ==========\n",
      "🔄 Fold 5: Using random initialization\n",
      "Training epoch: 1 [0/900 (0%)]\tLoss: 1.472980\n",
      "Training epoch: 1 [160/900 (18%)]\tLoss: 0.797786\n",
      "Training epoch: 1 [320/900 (35%)]\tLoss: 0.979250\n",
      "Training epoch: 1 [480/900 (53%)]\tLoss: 0.783800\n",
      "Training epoch: 1 [640/900 (70%)]\tLoss: 0.535455\n",
      "Training epoch: 1 [800/900 (88%)]\tLoss: 0.632012\n",
      "=========> Epoch: 1 Average loss: 0.7359\n",
      "Correlation coefficient: 0.6267\n",
      "✅ Epoch 1: New best correlation = 0.6267\n",
      "Training epoch: 2 [0/900 (0%)]\tLoss: 0.290052\n",
      "Training epoch: 2 [160/900 (18%)]\tLoss: 0.251690\n",
      "Training epoch: 2 [320/900 (35%)]\tLoss: 0.218287\n",
      "Training epoch: 2 [480/900 (53%)]\tLoss: 0.295051\n",
      "Training epoch: 2 [640/900 (70%)]\tLoss: 0.204171\n",
      "Training epoch: 2 [800/900 (88%)]\tLoss: 0.248956\n",
      "=========> Epoch: 2 Average loss: 0.3302\n",
      "Correlation coefficient: 0.6268\n",
      "✅ Epoch 2: New best correlation = 0.6268\n",
      "Training epoch: 3 [0/900 (0%)]\tLoss: 0.092900\n",
      "Training epoch: 3 [160/900 (18%)]\tLoss: 0.063288\n",
      "Training epoch: 3 [320/900 (35%)]\tLoss: 0.054625\n",
      "Training epoch: 3 [480/900 (53%)]\tLoss: 0.105330\n",
      "Training epoch: 3 [640/900 (70%)]\tLoss: 0.128856\n",
      "Training epoch: 3 [800/900 (88%)]\tLoss: 0.128033\n",
      "=========> Epoch: 3 Average loss: 0.1148\n",
      "Correlation coefficient: 0.5931\n",
      "Training epoch: 4 [0/900 (0%)]\tLoss: 0.080689\n",
      "Training epoch: 4 [160/900 (18%)]\tLoss: 0.069729\n",
      "Training epoch: 4 [320/900 (35%)]\tLoss: 0.072937\n",
      "Training epoch: 4 [480/900 (53%)]\tLoss: 0.057336\n",
      "Training epoch: 4 [640/900 (70%)]\tLoss: 0.065007\n",
      "Training epoch: 4 [800/900 (88%)]\tLoss: 0.037893\n",
      "=========> Epoch: 4 Average loss: 0.0608\n",
      "Correlation coefficient: 0.5888\n",
      "Training epoch: 5 [0/900 (0%)]\tLoss: 0.034229\n",
      "Training epoch: 5 [160/900 (18%)]\tLoss: 0.020733\n",
      "Training epoch: 5 [320/900 (35%)]\tLoss: 0.014556\n",
      "Training epoch: 5 [480/900 (53%)]\tLoss: 0.031582\n",
      "Training epoch: 5 [640/900 (70%)]\tLoss: 0.029247\n",
      "Training epoch: 5 [800/900 (88%)]\tLoss: 0.038523\n",
      "=========> Epoch: 5 Average loss: 0.0340\n",
      "Correlation coefficient: 0.6106\n",
      "Training epoch: 6 [0/900 (0%)]\tLoss: 0.035722\n",
      "Training epoch: 6 [160/900 (18%)]\tLoss: 0.007867\n",
      "Training epoch: 6 [320/900 (35%)]\tLoss: 0.017545\n",
      "Training epoch: 6 [480/900 (53%)]\tLoss: 0.012186\n",
      "Training epoch: 6 [640/900 (70%)]\tLoss: 0.036341\n",
      "Training epoch: 6 [800/900 (88%)]\tLoss: 0.022383\n",
      "=========> Epoch: 6 Average loss: 0.0237\n",
      "Correlation coefficient: 0.6109\n",
      "Training epoch: 7 [0/900 (0%)]\tLoss: 0.017075\n",
      "Training epoch: 7 [160/900 (18%)]\tLoss: 0.059876\n",
      "Training epoch: 7 [320/900 (35%)]\tLoss: 0.020733\n",
      "Training epoch: 7 [480/900 (53%)]\tLoss: 0.014609\n",
      "Training epoch: 7 [640/900 (70%)]\tLoss: 0.008905\n",
      "Training epoch: 7 [800/900 (88%)]\tLoss: 0.006262\n",
      "=========> Epoch: 7 Average loss: 0.0182\n",
      "Correlation coefficient: 0.6082\n",
      "Training epoch: 8 [0/900 (0%)]\tLoss: 0.033782\n",
      "Training epoch: 8 [160/900 (18%)]\tLoss: 0.027007\n",
      "Training epoch: 8 [320/900 (35%)]\tLoss: 0.009008\n",
      "Training epoch: 8 [480/900 (53%)]\tLoss: 0.027359\n",
      "Training epoch: 8 [640/900 (70%)]\tLoss: 0.013134\n",
      "Training epoch: 8 [800/900 (88%)]\tLoss: 0.011598\n",
      "=========> Epoch: 8 Average loss: 0.0149\n",
      "Correlation coefficient: 0.6194\n",
      "Training epoch: 9 [0/900 (0%)]\tLoss: 0.004109\n",
      "Training epoch: 9 [160/900 (18%)]\tLoss: 0.007429\n",
      "Training epoch: 9 [320/900 (35%)]\tLoss: 0.008817\n",
      "Training epoch: 9 [480/900 (53%)]\tLoss: 0.016245\n",
      "Training epoch: 9 [640/900 (70%)]\tLoss: 0.019286\n",
      "Training epoch: 9 [800/900 (88%)]\tLoss: 0.006134\n",
      "=========> Epoch: 9 Average loss: 0.0101\n",
      "Correlation coefficient: 0.6180\n",
      "Training epoch: 10 [0/900 (0%)]\tLoss: 0.003576\n",
      "Training epoch: 10 [160/900 (18%)]\tLoss: 0.006512\n",
      "Training epoch: 10 [320/900 (35%)]\tLoss: 0.008307\n",
      "Training epoch: 10 [480/900 (53%)]\tLoss: 0.007339\n",
      "Training epoch: 10 [640/900 (70%)]\tLoss: 0.010001\n",
      "Training epoch: 10 [800/900 (88%)]\tLoss: 0.005820\n",
      "=========> Epoch: 10 Average loss: 0.0084\n",
      "Correlation coefficient: 0.6128\n",
      "Training epoch: 11 [0/900 (0%)]\tLoss: 0.009229\n",
      "Training epoch: 11 [160/900 (18%)]\tLoss: 0.011336\n",
      "Training epoch: 11 [320/900 (35%)]\tLoss: 0.003922\n",
      "Training epoch: 11 [480/900 (53%)]\tLoss: 0.008441\n",
      "Training epoch: 11 [640/900 (70%)]\tLoss: 0.006677\n",
      "Training epoch: 11 [800/900 (88%)]\tLoss: 0.006526\n",
      "=========> Epoch: 11 Average loss: 0.0090\n",
      "Correlation coefficient: 0.6079\n",
      "Training epoch: 12 [0/900 (0%)]\tLoss: 0.014277\n",
      "Training epoch: 12 [160/900 (18%)]\tLoss: 0.014605\n",
      "Training epoch: 12 [320/900 (35%)]\tLoss: 0.013663\n",
      "Training epoch: 12 [480/900 (53%)]\tLoss: 0.004620\n",
      "Training epoch: 12 [640/900 (70%)]\tLoss: 0.004914\n",
      "Training epoch: 12 [800/900 (88%)]\tLoss: 0.002868\n",
      "=========> Epoch: 12 Average loss: 0.0087\n",
      "Correlation coefficient: 0.6199\n",
      "Training epoch: 13 [0/900 (0%)]\tLoss: 0.008919\n",
      "Training epoch: 13 [160/900 (18%)]\tLoss: 0.005979\n",
      "Training epoch: 13 [320/900 (35%)]\tLoss: 0.003942\n",
      "Training epoch: 13 [480/900 (53%)]\tLoss: 0.009299\n",
      "Training epoch: 13 [640/900 (70%)]\tLoss: 0.008873\n",
      "Training epoch: 13 [800/900 (88%)]\tLoss: 0.002747\n",
      "=========> Epoch: 13 Average loss: 0.0075\n",
      "Correlation coefficient: 0.6155\n",
      "Training epoch: 14 [0/900 (0%)]\tLoss: 0.006943\n",
      "Training epoch: 14 [160/900 (18%)]\tLoss: 0.005636\n",
      "Training epoch: 14 [320/900 (35%)]\tLoss: 0.006793\n",
      "Training epoch: 14 [480/900 (53%)]\tLoss: 0.013480\n",
      "Training epoch: 14 [640/900 (70%)]\tLoss: 0.006451\n",
      "Training epoch: 14 [800/900 (88%)]\tLoss: 0.004161\n",
      "=========> Epoch: 14 Average loss: 0.0075\n",
      "Correlation coefficient: 0.6189\n",
      "Training epoch: 15 [0/900 (0%)]\tLoss: 0.004215\n",
      "Training epoch: 15 [160/900 (18%)]\tLoss: 0.007813\n",
      "Training epoch: 15 [320/900 (35%)]\tLoss: 0.006442\n",
      "Training epoch: 15 [480/900 (53%)]\tLoss: 0.007881\n",
      "Training epoch: 15 [640/900 (70%)]\tLoss: 0.015492\n",
      "Training epoch: 15 [800/900 (88%)]\tLoss: 0.001253\n",
      "=========> Epoch: 15 Average loss: 0.0080\n",
      "Correlation coefficient: 0.6151\n",
      "Training epoch: 16 [0/900 (0%)]\tLoss: 0.023291\n",
      "Training epoch: 16 [160/900 (18%)]\tLoss: 0.009205\n",
      "Training epoch: 16 [320/900 (35%)]\tLoss: 0.013282\n",
      "Training epoch: 16 [480/900 (53%)]\tLoss: 0.008813\n",
      "Training epoch: 16 [640/900 (70%)]\tLoss: 0.009037\n",
      "Training epoch: 16 [800/900 (88%)]\tLoss: 0.005604\n",
      "=========> Epoch: 16 Average loss: 0.0085\n",
      "Correlation coefficient: 0.6084\n",
      "Training epoch: 17 [0/900 (0%)]\tLoss: 0.012213\n",
      "Training epoch: 17 [160/900 (18%)]\tLoss: 0.008084\n",
      "Training epoch: 17 [320/900 (35%)]\tLoss: 0.031975\n",
      "Training epoch: 17 [480/900 (53%)]\tLoss: 0.010616\n",
      "Training epoch: 17 [640/900 (70%)]\tLoss: 0.007913\n",
      "Training epoch: 17 [800/900 (88%)]\tLoss: 0.005579\n",
      "=========> Epoch: 17 Average loss: 0.0089\n",
      "Correlation coefficient: 0.6180\n",
      "Training epoch: 18 [0/900 (0%)]\tLoss: 0.008820\n",
      "Training epoch: 18 [160/900 (18%)]\tLoss: 0.009611\n",
      "Training epoch: 18 [320/900 (35%)]\tLoss: 0.010821\n",
      "Training epoch: 18 [480/900 (53%)]\tLoss: 0.027258\n",
      "Training epoch: 18 [640/900 (70%)]\tLoss: 0.006560\n",
      "Training epoch: 18 [800/900 (88%)]\tLoss: 0.007436\n",
      "=========> Epoch: 18 Average loss: 0.0108\n",
      "Correlation coefficient: 0.6133\n",
      "Training epoch: 19 [0/900 (0%)]\tLoss: 0.014526\n",
      "Training epoch: 19 [160/900 (18%)]\tLoss: 0.024045\n",
      "Training epoch: 19 [320/900 (35%)]\tLoss: 0.011272\n",
      "Training epoch: 19 [480/900 (53%)]\tLoss: 0.006739\n",
      "Training epoch: 19 [640/900 (70%)]\tLoss: 0.014715\n",
      "Training epoch: 19 [800/900 (88%)]\tLoss: 0.008157\n",
      "=========> Epoch: 19 Average loss: 0.0121\n",
      "Correlation coefficient: 0.6192\n",
      "Training epoch: 20 [0/900 (0%)]\tLoss: 0.005679\n",
      "Training epoch: 20 [160/900 (18%)]\tLoss: 0.020680\n",
      "Training epoch: 20 [320/900 (35%)]\tLoss: 0.013256\n",
      "Training epoch: 20 [480/900 (53%)]\tLoss: 0.012914\n",
      "Training epoch: 20 [640/900 (70%)]\tLoss: 0.016638\n",
      "Training epoch: 20 [800/900 (88%)]\tLoss: 0.010792\n",
      "=========> Epoch: 20 Average loss: 0.0139\n",
      "Correlation coefficient: 0.6146\n",
      "Training epoch: 21 [0/900 (0%)]\tLoss: 0.016138\n",
      "Training epoch: 21 [160/900 (18%)]\tLoss: 0.028794\n",
      "Training epoch: 21 [320/900 (35%)]\tLoss: 0.010019\n",
      "Training epoch: 21 [480/900 (53%)]\tLoss: 0.010342\n",
      "Training epoch: 21 [640/900 (70%)]\tLoss: 0.012115\n",
      "Training epoch: 21 [800/900 (88%)]\tLoss: 0.012642\n",
      "=========> Epoch: 21 Average loss: 0.0144\n",
      "Correlation coefficient: 0.6151\n",
      "Training epoch: 22 [0/900 (0%)]\tLoss: 0.016498\n",
      "Training epoch: 22 [160/900 (18%)]\tLoss: 0.008778\n",
      "Training epoch: 22 [320/900 (35%)]\tLoss: 0.017369\n",
      "Training epoch: 22 [480/900 (53%)]\tLoss: 0.015043\n",
      "Training epoch: 22 [640/900 (70%)]\tLoss: 0.013958\n",
      "Training epoch: 22 [800/900 (88%)]\tLoss: 0.018588\n",
      "=========> Epoch: 22 Average loss: 0.0146\n",
      "Correlation coefficient: 0.6336\n",
      "✅ Epoch 22: New best correlation = 0.6336\n",
      "Training epoch: 23 [0/900 (0%)]\tLoss: 0.035054\n",
      "Training epoch: 23 [160/900 (18%)]\tLoss: 0.009775\n",
      "Training epoch: 23 [320/900 (35%)]\tLoss: 0.010201\n",
      "Training epoch: 23 [480/900 (53%)]\tLoss: 0.007423\n",
      "Training epoch: 23 [640/900 (70%)]\tLoss: 0.009333\n",
      "Training epoch: 23 [800/900 (88%)]\tLoss: 0.017113\n",
      "=========> Epoch: 23 Average loss: 0.0187\n",
      "Correlation coefficient: 0.6220\n",
      "Training epoch: 24 [0/900 (0%)]\tLoss: 0.014932\n",
      "Training epoch: 24 [160/900 (18%)]\tLoss: 0.015961\n",
      "Training epoch: 24 [320/900 (35%)]\tLoss: 0.014622\n",
      "Training epoch: 24 [480/900 (53%)]\tLoss: 0.023957\n",
      "Training epoch: 24 [640/900 (70%)]\tLoss: 0.011899\n",
      "Training epoch: 24 [800/900 (88%)]\tLoss: 0.016145\n",
      "=========> Epoch: 24 Average loss: 0.0218\n",
      "Correlation coefficient: 0.6089\n",
      "Training epoch: 25 [0/900 (0%)]\tLoss: 0.023680\n",
      "Training epoch: 25 [160/900 (18%)]\tLoss: 0.026339\n",
      "Training epoch: 25 [320/900 (35%)]\tLoss: 0.037502\n",
      "Training epoch: 25 [480/900 (53%)]\tLoss: 0.016672\n",
      "Training epoch: 25 [640/900 (70%)]\tLoss: 0.016459\n",
      "Training epoch: 25 [800/900 (88%)]\tLoss: 0.034437\n",
      "=========> Epoch: 25 Average loss: 0.0257\n",
      "Correlation coefficient: 0.6172\n",
      "Training epoch: 26 [0/900 (0%)]\tLoss: 0.016628\n",
      "Training epoch: 26 [160/900 (18%)]\tLoss: 0.013739\n",
      "Training epoch: 26 [320/900 (35%)]\tLoss: 0.021103\n",
      "Training epoch: 26 [480/900 (53%)]\tLoss: 0.033056\n",
      "Training epoch: 26 [640/900 (70%)]\tLoss: 0.016732\n",
      "Training epoch: 26 [800/900 (88%)]\tLoss: 0.007856\n",
      "=========> Epoch: 26 Average loss: 0.0205\n",
      "Correlation coefficient: 0.6177\n",
      "Training epoch: 27 [0/900 (0%)]\tLoss: 0.022669\n",
      "Training epoch: 27 [160/900 (18%)]\tLoss: 0.030645\n",
      "Training epoch: 27 [320/900 (35%)]\tLoss: 0.021182\n",
      "Training epoch: 27 [480/900 (53%)]\tLoss: 0.017340\n",
      "Training epoch: 27 [640/900 (70%)]\tLoss: 0.012429\n",
      "Training epoch: 27 [800/900 (88%)]\tLoss: 0.013955\n",
      "=========> Epoch: 27 Average loss: 0.0215\n",
      "Correlation coefficient: 0.6148\n",
      "Training epoch: 28 [0/900 (0%)]\tLoss: 0.008893\n",
      "Training epoch: 28 [160/900 (18%)]\tLoss: 0.005978\n",
      "Training epoch: 28 [320/900 (35%)]\tLoss: 0.011713\n",
      "Training epoch: 28 [480/900 (53%)]\tLoss: 0.032551\n",
      "Training epoch: 28 [640/900 (70%)]\tLoss: 0.019454\n",
      "Training epoch: 28 [800/900 (88%)]\tLoss: 0.009398\n",
      "=========> Epoch: 28 Average loss: 0.0145\n",
      "Correlation coefficient: 0.6215\n",
      "Training epoch: 29 [0/900 (0%)]\tLoss: 0.005386\n",
      "Training epoch: 29 [160/900 (18%)]\tLoss: 0.006703\n",
      "Training epoch: 29 [320/900 (35%)]\tLoss: 0.004556\n",
      "Training epoch: 29 [480/900 (53%)]\tLoss: 0.008657\n",
      "Training epoch: 29 [640/900 (70%)]\tLoss: 0.007450\n",
      "Training epoch: 29 [800/900 (88%)]\tLoss: 0.002552\n",
      "=========> Epoch: 29 Average loss: 0.0094\n",
      "Correlation coefficient: 0.6196\n",
      "Training epoch: 30 [0/900 (0%)]\tLoss: 0.001314\n",
      "Training epoch: 30 [160/900 (18%)]\tLoss: 0.013840\n",
      "Training epoch: 30 [320/900 (35%)]\tLoss: 0.005397\n",
      "Training epoch: 30 [480/900 (53%)]\tLoss: 0.008490\n",
      "Training epoch: 30 [640/900 (70%)]\tLoss: 0.018153\n",
      "Training epoch: 30 [800/900 (88%)]\tLoss: 0.003984\n",
      "=========> Epoch: 30 Average loss: 0.0082\n",
      "Correlation coefficient: 0.6228\n",
      "Training epoch: 31 [0/900 (0%)]\tLoss: 0.011103\n",
      "Training epoch: 31 [160/900 (18%)]\tLoss: 0.008128\n",
      "Training epoch: 31 [320/900 (35%)]\tLoss: 0.008320\n",
      "Training epoch: 31 [480/900 (53%)]\tLoss: 0.040195\n",
      "Training epoch: 31 [640/900 (70%)]\tLoss: 0.009437\n",
      "Training epoch: 31 [800/900 (88%)]\tLoss: 0.006653\n",
      "=========> Epoch: 31 Average loss: 0.0079\n",
      "Correlation coefficient: 0.6222\n",
      "Training epoch: 32 [0/900 (0%)]\tLoss: 0.011328\n",
      "Training epoch: 32 [160/900 (18%)]\tLoss: 0.017809\n",
      "Training epoch: 32 [320/900 (35%)]\tLoss: 0.002305\n",
      "Training epoch: 32 [480/900 (53%)]\tLoss: 0.011972\n",
      "Training epoch: 32 [640/900 (70%)]\tLoss: 0.006682\n",
      "Training epoch: 32 [800/900 (88%)]\tLoss: 0.010305\n",
      "=========> Epoch: 32 Average loss: 0.0097\n",
      "Correlation coefficient: 0.6259\n",
      "Training epoch: 33 [0/900 (0%)]\tLoss: 0.003770\n",
      "Training epoch: 33 [160/900 (18%)]\tLoss: 0.011107\n",
      "Training epoch: 33 [320/900 (35%)]\tLoss: 0.029389\n",
      "Training epoch: 33 [480/900 (53%)]\tLoss: 0.006578\n",
      "Training epoch: 33 [640/900 (70%)]\tLoss: 0.011028\n",
      "Training epoch: 33 [800/900 (88%)]\tLoss: 0.017748\n",
      "=========> Epoch: 33 Average loss: 0.0115\n",
      "Correlation coefficient: 0.6219\n",
      "Training epoch: 34 [0/900 (0%)]\tLoss: 0.003761\n",
      "Training epoch: 34 [160/900 (18%)]\tLoss: 0.039718\n",
      "Training epoch: 34 [320/900 (35%)]\tLoss: 0.004349\n",
      "Training epoch: 34 [480/900 (53%)]\tLoss: 0.013051\n",
      "Training epoch: 34 [640/900 (70%)]\tLoss: 0.024461\n",
      "Training epoch: 34 [800/900 (88%)]\tLoss: 0.022319\n",
      "=========> Epoch: 34 Average loss: 0.0122\n",
      "Correlation coefficient: 0.6302\n",
      "Training epoch: 35 [0/900 (0%)]\tLoss: 0.003455\n",
      "Training epoch: 35 [160/900 (18%)]\tLoss: 0.010151\n",
      "Training epoch: 35 [320/900 (35%)]\tLoss: 0.019174\n",
      "Training epoch: 35 [480/900 (53%)]\tLoss: 0.003233\n",
      "Training epoch: 35 [640/900 (70%)]\tLoss: 0.008501\n",
      "Training epoch: 35 [800/900 (88%)]\tLoss: 0.011624\n",
      "=========> Epoch: 35 Average loss: 0.0110\n",
      "Correlation coefficient: 0.6229\n",
      "Training epoch: 36 [0/900 (0%)]\tLoss: 0.012685\n",
      "Training epoch: 36 [160/900 (18%)]\tLoss: 0.006729\n",
      "Training epoch: 36 [320/900 (35%)]\tLoss: 0.004281\n",
      "Training epoch: 36 [480/900 (53%)]\tLoss: 0.002817\n",
      "Training epoch: 36 [640/900 (70%)]\tLoss: 0.021317\n",
      "Training epoch: 36 [800/900 (88%)]\tLoss: 0.003671\n",
      "=========> Epoch: 36 Average loss: 0.0073\n",
      "Correlation coefficient: 0.6346\n",
      "✅ Epoch 36: New best correlation = 0.6346\n",
      "Training epoch: 37 [0/900 (0%)]\tLoss: 0.003528\n",
      "Training epoch: 37 [160/900 (18%)]\tLoss: 0.004587\n",
      "Training epoch: 37 [320/900 (35%)]\tLoss: 0.002444\n",
      "Training epoch: 37 [480/900 (53%)]\tLoss: 0.006110\n",
      "Training epoch: 37 [640/900 (70%)]\tLoss: 0.006958\n",
      "Training epoch: 37 [800/900 (88%)]\tLoss: 0.003774\n",
      "=========> Epoch: 37 Average loss: 0.0045\n",
      "Correlation coefficient: 0.6320\n",
      "Training epoch: 38 [0/900 (0%)]\tLoss: 0.003670\n",
      "Training epoch: 38 [160/900 (18%)]\tLoss: 0.006160\n",
      "Training epoch: 38 [320/900 (35%)]\tLoss: 0.003467\n",
      "Training epoch: 38 [480/900 (53%)]\tLoss: 0.003343\n",
      "Training epoch: 38 [640/900 (70%)]\tLoss: 0.002373\n",
      "Training epoch: 38 [800/900 (88%)]\tLoss: 0.009016\n",
      "=========> Epoch: 38 Average loss: 0.0042\n",
      "Correlation coefficient: 0.6267\n",
      "Training epoch: 39 [0/900 (0%)]\tLoss: 0.002615\n",
      "Training epoch: 39 [160/900 (18%)]\tLoss: 0.005365\n",
      "Training epoch: 39 [320/900 (35%)]\tLoss: 0.007096\n",
      "Training epoch: 39 [480/900 (53%)]\tLoss: 0.014682\n",
      "Training epoch: 39 [640/900 (70%)]\tLoss: 0.006455\n",
      "Training epoch: 39 [800/900 (88%)]\tLoss: 0.001668\n",
      "=========> Epoch: 39 Average loss: 0.0045\n",
      "Correlation coefficient: 0.6282\n",
      "Training epoch: 40 [0/900 (0%)]\tLoss: 0.004144\n",
      "Training epoch: 40 [160/900 (18%)]\tLoss: 0.001120\n",
      "Training epoch: 40 [320/900 (35%)]\tLoss: 0.009671\n",
      "Training epoch: 40 [480/900 (53%)]\tLoss: 0.001928\n",
      "Training epoch: 40 [640/900 (70%)]\tLoss: 0.009040\n",
      "Training epoch: 40 [800/900 (88%)]\tLoss: 0.004776\n",
      "=========> Epoch: 40 Average loss: 0.0048\n",
      "Correlation coefficient: 0.6312\n",
      "Training epoch: 41 [0/900 (0%)]\tLoss: 0.005481\n",
      "Training epoch: 41 [160/900 (18%)]\tLoss: 0.001939\n",
      "Training epoch: 41 [320/900 (35%)]\tLoss: 0.004520\n",
      "Training epoch: 41 [480/900 (53%)]\tLoss: 0.001599\n",
      "Training epoch: 41 [640/900 (70%)]\tLoss: 0.008117\n",
      "Training epoch: 41 [800/900 (88%)]\tLoss: 0.002471\n",
      "=========> Epoch: 41 Average loss: 0.0051\n",
      "Correlation coefficient: 0.6303\n",
      "Training epoch: 42 [0/900 (0%)]\tLoss: 0.004684\n",
      "Training epoch: 42 [160/900 (18%)]\tLoss: 0.003028\n",
      "Training epoch: 42 [320/900 (35%)]\tLoss: 0.006246\n",
      "Training epoch: 42 [480/900 (53%)]\tLoss: 0.003684\n",
      "Training epoch: 42 [640/900 (70%)]\tLoss: 0.005123\n",
      "Training epoch: 42 [800/900 (88%)]\tLoss: 0.007896\n",
      "=========> Epoch: 42 Average loss: 0.0049\n",
      "Correlation coefficient: 0.6295\n",
      "Training epoch: 43 [0/900 (0%)]\tLoss: 0.001856\n",
      "Training epoch: 43 [160/900 (18%)]\tLoss: 0.011817\n",
      "Training epoch: 43 [320/900 (35%)]\tLoss: 0.003338\n",
      "Training epoch: 43 [480/900 (53%)]\tLoss: 0.006087\n",
      "Training epoch: 43 [640/900 (70%)]\tLoss: 0.005595\n",
      "Training epoch: 43 [800/900 (88%)]\tLoss: 0.003420\n",
      "=========> Epoch: 43 Average loss: 0.0062\n",
      "Correlation coefficient: 0.6230\n",
      "Training epoch: 44 [0/900 (0%)]\tLoss: 0.005765\n",
      "Training epoch: 44 [160/900 (18%)]\tLoss: 0.002478\n",
      "Training epoch: 44 [320/900 (35%)]\tLoss: 0.014861\n",
      "Training epoch: 44 [480/900 (53%)]\tLoss: 0.008564\n",
      "Training epoch: 44 [640/900 (70%)]\tLoss: 0.005397\n",
      "Training epoch: 44 [800/900 (88%)]\tLoss: 0.010097\n",
      "=========> Epoch: 44 Average loss: 0.0075\n",
      "Correlation coefficient: 0.6224\n",
      "Training epoch: 45 [0/900 (0%)]\tLoss: 0.007509\n",
      "Training epoch: 45 [160/900 (18%)]\tLoss: 0.025544\n",
      "Training epoch: 45 [320/900 (35%)]\tLoss: 0.007013\n",
      "Training epoch: 45 [480/900 (53%)]\tLoss: 0.004808\n",
      "Training epoch: 45 [640/900 (70%)]\tLoss: 0.009659\n",
      "Training epoch: 45 [800/900 (88%)]\tLoss: 0.007182\n",
      "=========> Epoch: 45 Average loss: 0.0109\n",
      "Correlation coefficient: 0.6402\n",
      "✅ Epoch 45: New best correlation = 0.6402\n",
      "Training epoch: 46 [0/900 (0%)]\tLoss: 0.004296\n",
      "Training epoch: 46 [160/900 (18%)]\tLoss: 0.006343\n",
      "Training epoch: 46 [320/900 (35%)]\tLoss: 0.011446\n",
      "Training epoch: 46 [480/900 (53%)]\tLoss: 0.005281\n",
      "Training epoch: 46 [640/900 (70%)]\tLoss: 0.006636\n",
      "Training epoch: 46 [800/900 (88%)]\tLoss: 0.012168\n",
      "=========> Epoch: 46 Average loss: 0.0091\n",
      "Correlation coefficient: 0.6217\n",
      "Training epoch: 47 [0/900 (0%)]\tLoss: 0.007565\n",
      "Training epoch: 47 [160/900 (18%)]\tLoss: 0.009031\n",
      "Training epoch: 47 [320/900 (35%)]\tLoss: 0.009040\n",
      "Training epoch: 47 [480/900 (53%)]\tLoss: 0.029081\n",
      "Training epoch: 47 [640/900 (70%)]\tLoss: 0.013385\n",
      "Training epoch: 47 [800/900 (88%)]\tLoss: 0.006951\n",
      "=========> Epoch: 47 Average loss: 0.0088\n",
      "Correlation coefficient: 0.6194\n",
      "Training epoch: 48 [0/900 (0%)]\tLoss: 0.009554\n",
      "Training epoch: 48 [160/900 (18%)]\tLoss: 0.009424\n",
      "Training epoch: 48 [320/900 (35%)]\tLoss: 0.009450\n",
      "Training epoch: 48 [480/900 (53%)]\tLoss: 0.010353\n",
      "Training epoch: 48 [640/900 (70%)]\tLoss: 0.011262\n",
      "Training epoch: 48 [800/900 (88%)]\tLoss: 0.013547\n",
      "=========> Epoch: 48 Average loss: 0.0095\n",
      "Correlation coefficient: 0.6297\n",
      "Training epoch: 49 [0/900 (0%)]\tLoss: 0.006709\n",
      "Training epoch: 49 [160/900 (18%)]\tLoss: 0.003113\n",
      "Training epoch: 49 [320/900 (35%)]\tLoss: 0.006463\n",
      "Training epoch: 49 [480/900 (53%)]\tLoss: 0.005407\n",
      "Training epoch: 49 [640/900 (70%)]\tLoss: 0.003138\n",
      "Training epoch: 49 [800/900 (88%)]\tLoss: 0.011319\n",
      "=========> Epoch: 49 Average loss: 0.0087\n",
      "Correlation coefficient: 0.6285\n",
      "Training epoch: 50 [0/900 (0%)]\tLoss: 0.019493\n",
      "Training epoch: 50 [160/900 (18%)]\tLoss: 0.010822\n",
      "Training epoch: 50 [320/900 (35%)]\tLoss: 0.008096\n",
      "Training epoch: 50 [480/900 (53%)]\tLoss: 0.036638\n",
      "Training epoch: 50 [640/900 (70%)]\tLoss: 0.006737\n",
      "Training epoch: 50 [800/900 (88%)]\tLoss: 0.004512\n",
      "=========> Epoch: 50 Average loss: 0.0135\n",
      "Correlation coefficient: 0.6191\n",
      "Training epoch: 51 [0/900 (0%)]\tLoss: 0.016224\n",
      "Training epoch: 51 [160/900 (18%)]\tLoss: 0.034094\n",
      "Training epoch: 51 [320/900 (35%)]\tLoss: 0.007402\n",
      "Training epoch: 51 [480/900 (53%)]\tLoss: 0.018475\n",
      "Training epoch: 51 [640/900 (70%)]\tLoss: 0.008912\n",
      "Training epoch: 51 [800/900 (88%)]\tLoss: 0.064554\n",
      "=========> Epoch: 51 Average loss: 0.0208\n",
      "Correlation coefficient: 0.6245\n",
      "Training epoch: 52 [0/900 (0%)]\tLoss: 0.021921\n",
      "Training epoch: 52 [160/900 (18%)]\tLoss: 0.029320\n",
      "Training epoch: 52 [320/900 (35%)]\tLoss: 0.009175\n",
      "Training epoch: 52 [480/900 (53%)]\tLoss: 0.014407\n",
      "Training epoch: 52 [640/900 (70%)]\tLoss: 0.010643\n",
      "Training epoch: 52 [800/900 (88%)]\tLoss: 0.014671\n",
      "=========> Epoch: 52 Average loss: 0.0257\n",
      "Correlation coefficient: 0.6173\n",
      "Training epoch: 53 [0/900 (0%)]\tLoss: 0.018084\n",
      "Training epoch: 53 [160/900 (18%)]\tLoss: 0.040149\n",
      "Training epoch: 53 [320/900 (35%)]\tLoss: 0.046239\n",
      "Training epoch: 53 [480/900 (53%)]\tLoss: 0.016378\n",
      "Training epoch: 53 [640/900 (70%)]\tLoss: 0.009528\n",
      "Training epoch: 53 [800/900 (88%)]\tLoss: 0.029386\n",
      "=========> Epoch: 53 Average loss: 0.0310\n",
      "Correlation coefficient: 0.6275\n",
      "Training epoch: 54 [0/900 (0%)]\tLoss: 0.026602\n",
      "Training epoch: 54 [160/900 (18%)]\tLoss: 0.035109\n",
      "Training epoch: 54 [320/900 (35%)]\tLoss: 0.022349\n",
      "Training epoch: 54 [480/900 (53%)]\tLoss: 0.015876\n",
      "Training epoch: 54 [640/900 (70%)]\tLoss: 0.039455\n",
      "Training epoch: 54 [800/900 (88%)]\tLoss: 0.023644\n",
      "=========> Epoch: 54 Average loss: 0.0295\n",
      "Correlation coefficient: 0.6388\n",
      "Training epoch: 55 [0/900 (0%)]\tLoss: 0.044968\n",
      "Training epoch: 55 [160/900 (18%)]\tLoss: 0.013105\n",
      "Training epoch: 55 [320/900 (35%)]\tLoss: 0.010896\n",
      "Training epoch: 55 [480/900 (53%)]\tLoss: 0.070405\n",
      "Training epoch: 55 [640/900 (70%)]\tLoss: 0.030428\n",
      "Training epoch: 55 [800/900 (88%)]\tLoss: 0.029129\n",
      "=========> Epoch: 55 Average loss: 0.0302\n",
      "Correlation coefficient: 0.6022\n",
      "Training epoch: 56 [0/900 (0%)]\tLoss: 0.024138\n",
      "Training epoch: 56 [160/900 (18%)]\tLoss: 0.112333\n",
      "Training epoch: 56 [320/900 (35%)]\tLoss: 0.016034\n",
      "Training epoch: 56 [480/900 (53%)]\tLoss: 0.014467\n",
      "Training epoch: 56 [640/900 (70%)]\tLoss: 0.046747\n",
      "Training epoch: 56 [800/900 (88%)]\tLoss: 0.020808\n",
      "=========> Epoch: 56 Average loss: 0.0308\n",
      "Correlation coefficient: 0.6294\n",
      "Training epoch: 57 [0/900 (0%)]\tLoss: 0.015081\n",
      "Training epoch: 57 [160/900 (18%)]\tLoss: 0.026235\n",
      "Training epoch: 57 [320/900 (35%)]\tLoss: 0.050834\n",
      "Training epoch: 57 [480/900 (53%)]\tLoss: 0.032232\n",
      "Training epoch: 57 [640/900 (70%)]\tLoss: 0.017839\n",
      "Training epoch: 57 [800/900 (88%)]\tLoss: 0.013758\n",
      "=========> Epoch: 57 Average loss: 0.0191\n",
      "Correlation coefficient: 0.6395\n",
      "Training epoch: 58 [0/900 (0%)]\tLoss: 0.011069\n",
      "Training epoch: 58 [160/900 (18%)]\tLoss: 0.006113\n",
      "Training epoch: 58 [320/900 (35%)]\tLoss: 0.012626\n",
      "Training epoch: 58 [480/900 (53%)]\tLoss: 0.007263\n",
      "Training epoch: 58 [640/900 (70%)]\tLoss: 0.007646\n",
      "Training epoch: 58 [800/900 (88%)]\tLoss: 0.020013\n",
      "=========> Epoch: 58 Average loss: 0.0131\n",
      "Correlation coefficient: 0.6340\n",
      "Training epoch: 59 [0/900 (0%)]\tLoss: 0.005204\n",
      "Training epoch: 59 [160/900 (18%)]\tLoss: 0.002727\n",
      "Training epoch: 59 [320/900 (35%)]\tLoss: 0.005076\n",
      "Training epoch: 59 [480/900 (53%)]\tLoss: 0.007225\n",
      "Training epoch: 59 [640/900 (70%)]\tLoss: 0.065975\n",
      "Training epoch: 59 [800/900 (88%)]\tLoss: 0.016013\n",
      "=========> Epoch: 59 Average loss: 0.0079\n",
      "Correlation coefficient: 0.6187\n",
      "Training epoch: 60 [0/900 (0%)]\tLoss: 0.068224\n",
      "Training epoch: 60 [160/900 (18%)]\tLoss: 0.005864\n",
      "Training epoch: 60 [320/900 (35%)]\tLoss: 0.002638\n",
      "Training epoch: 60 [480/900 (53%)]\tLoss: 0.003102\n",
      "Training epoch: 60 [640/900 (70%)]\tLoss: 0.014829\n",
      "Training epoch: 60 [800/900 (88%)]\tLoss: 0.008741\n",
      "=========> Epoch: 60 Average loss: 0.0062\n",
      "Correlation coefficient: 0.6308\n",
      "Training epoch: 61 [0/900 (0%)]\tLoss: 0.008755\n",
      "Training epoch: 61 [160/900 (18%)]\tLoss: 0.004408\n",
      "Training epoch: 61 [320/900 (35%)]\tLoss: 0.004474\n",
      "Training epoch: 61 [480/900 (53%)]\tLoss: 0.008017\n",
      "Training epoch: 61 [640/900 (70%)]\tLoss: 0.005592\n",
      "Training epoch: 61 [800/900 (88%)]\tLoss: 0.003849\n",
      "=========> Epoch: 61 Average loss: 0.0036\n",
      "Correlation coefficient: 0.6268\n",
      "Training epoch: 62 [0/900 (0%)]\tLoss: 0.002341\n",
      "Training epoch: 62 [160/900 (18%)]\tLoss: 0.001269\n",
      "Training epoch: 62 [320/900 (35%)]\tLoss: 0.002397\n",
      "Training epoch: 62 [480/900 (53%)]\tLoss: 0.002648\n",
      "Training epoch: 62 [640/900 (70%)]\tLoss: 0.001114\n",
      "Training epoch: 62 [800/900 (88%)]\tLoss: 0.001243\n",
      "=========> Epoch: 62 Average loss: 0.0026\n",
      "Correlation coefficient: 0.6270\n",
      "Training epoch: 63 [0/900 (0%)]\tLoss: 0.001495\n",
      "Training epoch: 63 [160/900 (18%)]\tLoss: 0.004931\n",
      "Training epoch: 63 [320/900 (35%)]\tLoss: 0.001542\n",
      "Training epoch: 63 [480/900 (53%)]\tLoss: 0.002211\n",
      "Training epoch: 63 [640/900 (70%)]\tLoss: 0.001684\n",
      "Training epoch: 63 [800/900 (88%)]\tLoss: 0.004519\n",
      "=========> Epoch: 63 Average loss: 0.0020\n",
      "Correlation coefficient: 0.6278\n",
      "Training epoch: 64 [0/900 (0%)]\tLoss: 0.002443\n",
      "Training epoch: 64 [160/900 (18%)]\tLoss: 0.002195\n",
      "Training epoch: 64 [320/900 (35%)]\tLoss: 0.001647\n",
      "Training epoch: 64 [480/900 (53%)]\tLoss: 0.001214\n",
      "Training epoch: 64 [640/900 (70%)]\tLoss: 0.001173\n",
      "Training epoch: 64 [800/900 (88%)]\tLoss: 0.001179\n",
      "=========> Epoch: 64 Average loss: 0.0018\n",
      "Correlation coefficient: 0.6287\n",
      "Training epoch: 65 [0/900 (0%)]\tLoss: 0.002269\n",
      "Training epoch: 65 [160/900 (18%)]\tLoss: 0.001996\n",
      "Training epoch: 65 [320/900 (35%)]\tLoss: 0.001350\n",
      "Training epoch: 65 [480/900 (53%)]\tLoss: 0.001473\n",
      "Training epoch: 65 [640/900 (70%)]\tLoss: 0.001055\n",
      "Training epoch: 65 [800/900 (88%)]\tLoss: 0.003062\n",
      "=========> Epoch: 65 Average loss: 0.0026\n",
      "Correlation coefficient: 0.6245\n",
      "Training epoch: 66 [0/900 (0%)]\tLoss: 0.000990\n",
      "Training epoch: 66 [160/900 (18%)]\tLoss: 0.001874\n",
      "Training epoch: 66 [320/900 (35%)]\tLoss: 0.004528\n",
      "Training epoch: 66 [480/900 (53%)]\tLoss: 0.003562\n",
      "Training epoch: 66 [640/900 (70%)]\tLoss: 0.001127\n",
      "Training epoch: 66 [800/900 (88%)]\tLoss: 0.002707\n",
      "=========> Epoch: 66 Average loss: 0.0031\n",
      "Correlation coefficient: 0.6300\n",
      "Training epoch: 67 [0/900 (0%)]\tLoss: 0.003428\n",
      "Training epoch: 67 [160/900 (18%)]\tLoss: 0.012741\n",
      "Training epoch: 67 [320/900 (35%)]\tLoss: 0.004793\n",
      "Training epoch: 67 [480/900 (53%)]\tLoss: 0.003119\n",
      "Training epoch: 67 [640/900 (70%)]\tLoss: 0.001141\n",
      "Training epoch: 67 [800/900 (88%)]\tLoss: 0.002214\n",
      "=========> Epoch: 67 Average loss: 0.0027\n",
      "Correlation coefficient: 0.6279\n",
      "Training epoch: 68 [0/900 (0%)]\tLoss: 0.000975\n",
      "Training epoch: 68 [160/900 (18%)]\tLoss: 0.001299\n",
      "Training epoch: 68 [320/900 (35%)]\tLoss: 0.001270\n",
      "Training epoch: 68 [480/900 (53%)]\tLoss: 0.001248\n",
      "Training epoch: 68 [640/900 (70%)]\tLoss: 0.000979\n",
      "Training epoch: 68 [800/900 (88%)]\tLoss: 0.006002\n",
      "=========> Epoch: 68 Average loss: 0.0022\n",
      "Correlation coefficient: 0.6274\n",
      "Training epoch: 69 [0/900 (0%)]\tLoss: 0.003365\n",
      "Training epoch: 69 [160/900 (18%)]\tLoss: 0.000936\n",
      "Training epoch: 69 [320/900 (35%)]\tLoss: 0.001480\n",
      "Training epoch: 69 [480/900 (53%)]\tLoss: 0.001260\n",
      "Training epoch: 69 [640/900 (70%)]\tLoss: 0.001130\n",
      "Training epoch: 69 [800/900 (88%)]\tLoss: 0.001020\n",
      "=========> Epoch: 69 Average loss: 0.0017\n",
      "Correlation coefficient: 0.6258\n",
      "Training epoch: 70 [0/900 (0%)]\tLoss: 0.000606\n",
      "Training epoch: 70 [160/900 (18%)]\tLoss: 0.002119\n",
      "Training epoch: 70 [320/900 (35%)]\tLoss: 0.000944\n",
      "Training epoch: 70 [480/900 (53%)]\tLoss: 0.001748\n",
      "Training epoch: 70 [640/900 (70%)]\tLoss: 0.000772\n",
      "Training epoch: 70 [800/900 (88%)]\tLoss: 0.001798\n",
      "=========> Epoch: 70 Average loss: 0.0015\n",
      "Correlation coefficient: 0.6244\n",
      "Training epoch: 71 [0/900 (0%)]\tLoss: 0.001720\n",
      "Training epoch: 71 [160/900 (18%)]\tLoss: 0.001534\n",
      "Training epoch: 71 [320/900 (35%)]\tLoss: 0.001728\n",
      "Training epoch: 71 [480/900 (53%)]\tLoss: 0.000701\n",
      "Training epoch: 71 [640/900 (70%)]\tLoss: 0.002356\n",
      "Training epoch: 71 [800/900 (88%)]\tLoss: 0.003624\n",
      "=========> Epoch: 71 Average loss: 0.0022\n",
      "Correlation coefficient: 0.6269\n",
      "Training epoch: 72 [0/900 (0%)]\tLoss: 0.002171\n",
      "Training epoch: 72 [160/900 (18%)]\tLoss: 0.003921\n",
      "Training epoch: 72 [320/900 (35%)]\tLoss: 0.002380\n",
      "Training epoch: 72 [480/900 (53%)]\tLoss: 0.002358\n",
      "Training epoch: 72 [640/900 (70%)]\tLoss: 0.007570\n",
      "Training epoch: 72 [800/900 (88%)]\tLoss: 0.001054\n",
      "=========> Epoch: 72 Average loss: 0.0034\n",
      "Correlation coefficient: 0.6251\n",
      "Training epoch: 73 [0/900 (0%)]\tLoss: 0.001621\n",
      "Training epoch: 73 [160/900 (18%)]\tLoss: 0.003723\n",
      "Training epoch: 73 [320/900 (35%)]\tLoss: 0.001728\n",
      "Training epoch: 73 [480/900 (53%)]\tLoss: 0.001537\n",
      "Training epoch: 73 [640/900 (70%)]\tLoss: 0.004279\n",
      "Training epoch: 73 [800/900 (88%)]\tLoss: 0.002094\n",
      "=========> Epoch: 73 Average loss: 0.0033\n",
      "Correlation coefficient: 0.6302\n",
      "Training epoch: 74 [0/900 (0%)]\tLoss: 0.003192\n",
      "Training epoch: 74 [160/900 (18%)]\tLoss: 0.002396\n",
      "Training epoch: 74 [320/900 (35%)]\tLoss: 0.004787\n",
      "Training epoch: 74 [480/900 (53%)]\tLoss: 0.001942\n",
      "Training epoch: 74 [640/900 (70%)]\tLoss: 0.003003\n",
      "Training epoch: 74 [800/900 (88%)]\tLoss: 0.002611\n",
      "=========> Epoch: 74 Average loss: 0.0031\n",
      "Correlation coefficient: 0.6263\n",
      "Training epoch: 75 [0/900 (0%)]\tLoss: 0.001519\n",
      "Training epoch: 75 [160/900 (18%)]\tLoss: 0.003634\n",
      "Training epoch: 75 [320/900 (35%)]\tLoss: 0.002960\n",
      "Training epoch: 75 [480/900 (53%)]\tLoss: 0.001629\n",
      "Training epoch: 75 [640/900 (70%)]\tLoss: 0.001890\n",
      "Training epoch: 75 [800/900 (88%)]\tLoss: 0.004659\n",
      "=========> Epoch: 75 Average loss: 0.0045\n",
      "Correlation coefficient: 0.6335\n",
      "Training epoch: 76 [0/900 (0%)]\tLoss: 0.003013\n",
      "Training epoch: 76 [160/900 (18%)]\tLoss: 0.004844\n",
      "Training epoch: 76 [320/900 (35%)]\tLoss: 0.014992\n",
      "Training epoch: 76 [480/900 (53%)]\tLoss: 0.006398\n",
      "Training epoch: 76 [640/900 (70%)]\tLoss: 0.007517\n",
      "Training epoch: 76 [800/900 (88%)]\tLoss: 0.004324\n",
      "=========> Epoch: 76 Average loss: 0.0088\n",
      "Correlation coefficient: 0.6239\n",
      "Training epoch: 77 [0/900 (0%)]\tLoss: 0.006050\n",
      "Training epoch: 77 [160/900 (18%)]\tLoss: 0.005696\n",
      "Training epoch: 77 [320/900 (35%)]\tLoss: 0.010549\n",
      "Training epoch: 77 [480/900 (53%)]\tLoss: 0.014254\n",
      "Training epoch: 77 [640/900 (70%)]\tLoss: 0.029776\n",
      "Training epoch: 77 [800/900 (88%)]\tLoss: 0.009383\n",
      "=========> Epoch: 77 Average loss: 0.0111\n",
      "Correlation coefficient: 0.6329\n",
      "Training epoch: 78 [0/900 (0%)]\tLoss: 0.023823\n",
      "Training epoch: 78 [160/900 (18%)]\tLoss: 0.020756\n",
      "Training epoch: 78 [320/900 (35%)]\tLoss: 0.023453\n",
      "Training epoch: 78 [480/900 (53%)]\tLoss: 0.007483\n",
      "Training epoch: 78 [640/900 (70%)]\tLoss: 0.015408\n",
      "Training epoch: 78 [800/900 (88%)]\tLoss: 0.010672\n",
      "=========> Epoch: 78 Average loss: 0.0120\n",
      "Correlation coefficient: 0.6116\n",
      "Training epoch: 79 [0/900 (0%)]\tLoss: 0.010665\n",
      "Training epoch: 79 [160/900 (18%)]\tLoss: 0.020130\n",
      "Training epoch: 79 [320/900 (35%)]\tLoss: 0.013510\n",
      "Training epoch: 79 [480/900 (53%)]\tLoss: 0.024932\n",
      "Training epoch: 79 [640/900 (70%)]\tLoss: 0.031485\n",
      "Training epoch: 79 [800/900 (88%)]\tLoss: 0.007358\n",
      "=========> Epoch: 79 Average loss: 0.0152\n",
      "Correlation coefficient: 0.6306\n",
      "Training epoch: 80 [0/900 (0%)]\tLoss: 0.005659\n",
      "Training epoch: 80 [160/900 (18%)]\tLoss: 0.011452\n",
      "Training epoch: 80 [320/900 (35%)]\tLoss: 0.015136\n",
      "Training epoch: 80 [480/900 (53%)]\tLoss: 0.024252\n",
      "Training epoch: 80 [640/900 (70%)]\tLoss: 0.018280\n",
      "Training epoch: 80 [800/900 (88%)]\tLoss: 0.004129\n",
      "=========> Epoch: 80 Average loss: 0.0197\n",
      "Correlation coefficient: 0.6220\n",
      "Training epoch: 81 [0/900 (0%)]\tLoss: 0.054537\n",
      "Training epoch: 81 [160/900 (18%)]\tLoss: 0.012869\n",
      "Training epoch: 81 [320/900 (35%)]\tLoss: 0.011090\n",
      "Training epoch: 81 [480/900 (53%)]\tLoss: 0.020411\n",
      "Training epoch: 81 [640/900 (70%)]\tLoss: 0.011928\n",
      "Training epoch: 81 [800/900 (88%)]\tLoss: 0.007221\n",
      "=========> Epoch: 81 Average loss: 0.0219\n",
      "Correlation coefficient: 0.6420\n",
      "✅ Epoch 81: New best correlation = 0.6420\n",
      "Training epoch: 82 [0/900 (0%)]\tLoss: 0.017593\n",
      "Training epoch: 82 [160/900 (18%)]\tLoss: 0.038534\n",
      "Training epoch: 82 [320/900 (35%)]\tLoss: 0.030903\n",
      "Training epoch: 82 [480/900 (53%)]\tLoss: 0.050863\n",
      "Training epoch: 82 [640/900 (70%)]\tLoss: 0.016046\n",
      "Training epoch: 82 [800/900 (88%)]\tLoss: 0.012423\n",
      "=========> Epoch: 82 Average loss: 0.0201\n",
      "Correlation coefficient: 0.6139\n",
      "Training epoch: 83 [0/900 (0%)]\tLoss: 0.018577\n",
      "Training epoch: 83 [160/900 (18%)]\tLoss: 0.011912\n",
      "Training epoch: 83 [320/900 (35%)]\tLoss: 0.011552\n",
      "Training epoch: 83 [480/900 (53%)]\tLoss: 0.007264\n",
      "Training epoch: 83 [640/900 (70%)]\tLoss: 0.009319\n",
      "Training epoch: 83 [800/900 (88%)]\tLoss: 0.012709\n",
      "=========> Epoch: 83 Average loss: 0.0154\n",
      "Correlation coefficient: 0.6298\n",
      "Training epoch: 84 [0/900 (0%)]\tLoss: 0.009934\n",
      "Training epoch: 84 [160/900 (18%)]\tLoss: 0.015693\n",
      "Training epoch: 84 [320/900 (35%)]\tLoss: 0.024152\n",
      "Training epoch: 84 [480/900 (53%)]\tLoss: 0.014351\n",
      "Training epoch: 84 [640/900 (70%)]\tLoss: 0.005539\n",
      "Training epoch: 84 [800/900 (88%)]\tLoss: 0.009295\n",
      "=========> Epoch: 84 Average loss: 0.0147\n",
      "Correlation coefficient: 0.6283\n",
      "Training epoch: 85 [0/900 (0%)]\tLoss: 0.007390\n",
      "Training epoch: 85 [160/900 (18%)]\tLoss: 0.007419\n",
      "Training epoch: 85 [320/900 (35%)]\tLoss: 0.006468\n",
      "Training epoch: 85 [480/900 (53%)]\tLoss: 0.012163\n",
      "Training epoch: 85 [640/900 (70%)]\tLoss: 0.008390\n",
      "Training epoch: 85 [800/900 (88%)]\tLoss: 0.013828\n",
      "=========> Epoch: 85 Average loss: 0.0166\n",
      "Correlation coefficient: 0.6090\n",
      "Training epoch: 86 [0/900 (0%)]\tLoss: 0.006070\n",
      "Training epoch: 86 [160/900 (18%)]\tLoss: 0.008651\n",
      "Training epoch: 86 [320/900 (35%)]\tLoss: 0.015320\n",
      "Training epoch: 86 [480/900 (53%)]\tLoss: 0.015192\n",
      "Training epoch: 86 [640/900 (70%)]\tLoss: 0.008692\n",
      "Training epoch: 86 [800/900 (88%)]\tLoss: 0.006810\n",
      "=========> Epoch: 86 Average loss: 0.0176\n",
      "Correlation coefficient: 0.6266\n",
      "Training epoch: 87 [0/900 (0%)]\tLoss: 0.006121\n",
      "Training epoch: 87 [160/900 (18%)]\tLoss: 0.033250\n",
      "Training epoch: 87 [320/900 (35%)]\tLoss: 0.027089\n",
      "Training epoch: 87 [480/900 (53%)]\tLoss: 0.007396\n",
      "Training epoch: 87 [640/900 (70%)]\tLoss: 0.017019\n",
      "Training epoch: 87 [800/900 (88%)]\tLoss: 0.013710\n",
      "=========> Epoch: 87 Average loss: 0.0170\n",
      "Correlation coefficient: 0.6165\n",
      "Training epoch: 88 [0/900 (0%)]\tLoss: 0.010962\n",
      "Training epoch: 88 [160/900 (18%)]\tLoss: 0.008782\n",
      "Training epoch: 88 [320/900 (35%)]\tLoss: 0.013539\n",
      "Training epoch: 88 [480/900 (53%)]\tLoss: 0.004206\n",
      "Training epoch: 88 [640/900 (70%)]\tLoss: 0.006812\n",
      "Training epoch: 88 [800/900 (88%)]\tLoss: 0.011052\n",
      "=========> Epoch: 88 Average loss: 0.0136\n",
      "Correlation coefficient: 0.6306\n",
      "Training epoch: 89 [0/900 (0%)]\tLoss: 0.004674\n",
      "Training epoch: 89 [160/900 (18%)]\tLoss: 0.032279\n",
      "Training epoch: 89 [320/900 (35%)]\tLoss: 0.010782\n",
      "Training epoch: 89 [480/900 (53%)]\tLoss: 0.007020\n",
      "Training epoch: 89 [640/900 (70%)]\tLoss: 0.002519\n",
      "Training epoch: 89 [800/900 (88%)]\tLoss: 0.002508\n",
      "=========> Epoch: 89 Average loss: 0.0094\n",
      "Correlation coefficient: 0.6171\n",
      "Training epoch: 90 [0/900 (0%)]\tLoss: 0.003348\n",
      "Training epoch: 90 [160/900 (18%)]\tLoss: 0.004873\n",
      "Training epoch: 90 [320/900 (35%)]\tLoss: 0.010323\n",
      "Training epoch: 90 [480/900 (53%)]\tLoss: 0.004059\n",
      "Training epoch: 90 [640/900 (70%)]\tLoss: 0.002257\n",
      "Training epoch: 90 [800/900 (88%)]\tLoss: 0.003566\n",
      "=========> Epoch: 90 Average loss: 0.0058\n",
      "Correlation coefficient: 0.6340\n",
      "Training epoch: 91 [0/900 (0%)]\tLoss: 0.003558\n",
      "Training epoch: 91 [160/900 (18%)]\tLoss: 0.002015\n",
      "Training epoch: 91 [320/900 (35%)]\tLoss: 0.004208\n",
      "Training epoch: 91 [480/900 (53%)]\tLoss: 0.003538\n",
      "Training epoch: 91 [640/900 (70%)]\tLoss: 0.002758\n",
      "Training epoch: 91 [800/900 (88%)]\tLoss: 0.001718\n",
      "=========> Epoch: 91 Average loss: 0.0034\n",
      "Correlation coefficient: 0.6329\n",
      "Training epoch: 92 [0/900 (0%)]\tLoss: 0.004879\n",
      "Training epoch: 92 [160/900 (18%)]\tLoss: 0.002358\n",
      "Training epoch: 92 [320/900 (35%)]\tLoss: 0.001225\n",
      "Training epoch: 92 [480/900 (53%)]\tLoss: 0.002571\n",
      "Training epoch: 92 [640/900 (70%)]\tLoss: 0.000979\n",
      "Training epoch: 92 [800/900 (88%)]\tLoss: 0.001941\n",
      "=========> Epoch: 92 Average loss: 0.0024\n",
      "Correlation coefficient: 0.6268\n",
      "Training epoch: 93 [0/900 (0%)]\tLoss: 0.001895\n",
      "Training epoch: 93 [160/900 (18%)]\tLoss: 0.001701\n",
      "Training epoch: 93 [320/900 (35%)]\tLoss: 0.001654\n",
      "Training epoch: 93 [480/900 (53%)]\tLoss: 0.001449\n",
      "Training epoch: 93 [640/900 (70%)]\tLoss: 0.000805\n",
      "Training epoch: 93 [800/900 (88%)]\tLoss: 0.004448\n",
      "=========> Epoch: 93 Average loss: 0.0017\n",
      "Correlation coefficient: 0.6286\n",
      "Training epoch: 94 [0/900 (0%)]\tLoss: 0.000853\n",
      "Training epoch: 94 [160/900 (18%)]\tLoss: 0.000969\n",
      "Training epoch: 94 [320/900 (35%)]\tLoss: 0.001657\n",
      "Training epoch: 94 [480/900 (53%)]\tLoss: 0.001735\n",
      "Training epoch: 94 [640/900 (70%)]\tLoss: 0.001688\n",
      "Training epoch: 94 [800/900 (88%)]\tLoss: 0.001237\n",
      "=========> Epoch: 94 Average loss: 0.0013\n",
      "Correlation coefficient: 0.6268\n",
      "Training epoch: 95 [0/900 (0%)]\tLoss: 0.001081\n",
      "Training epoch: 95 [160/900 (18%)]\tLoss: 0.001540\n",
      "Training epoch: 95 [320/900 (35%)]\tLoss: 0.000797\n",
      "Training epoch: 95 [480/900 (53%)]\tLoss: 0.000608\n",
      "Training epoch: 95 [640/900 (70%)]\tLoss: 0.000464\n",
      "Training epoch: 95 [800/900 (88%)]\tLoss: 0.003340\n",
      "=========> Epoch: 95 Average loss: 0.0011\n",
      "Correlation coefficient: 0.6255\n",
      "Training epoch: 96 [0/900 (0%)]\tLoss: 0.000707\n",
      "Training epoch: 96 [160/900 (18%)]\tLoss: 0.001249\n",
      "Training epoch: 96 [320/900 (35%)]\tLoss: 0.000332\n",
      "Training epoch: 96 [480/900 (53%)]\tLoss: 0.000663\n",
      "Training epoch: 96 [640/900 (70%)]\tLoss: 0.000706\n",
      "Training epoch: 96 [800/900 (88%)]\tLoss: 0.000397\n",
      "=========> Epoch: 96 Average loss: 0.0007\n",
      "Correlation coefficient: 0.6292\n",
      "Training epoch: 97 [0/900 (0%)]\tLoss: 0.001118\n",
      "Training epoch: 97 [160/900 (18%)]\tLoss: 0.000500\n",
      "Training epoch: 97 [320/900 (35%)]\tLoss: 0.001054\n",
      "Training epoch: 97 [480/900 (53%)]\tLoss: 0.000513\n",
      "Training epoch: 97 [640/900 (70%)]\tLoss: 0.000413\n",
      "Training epoch: 97 [800/900 (88%)]\tLoss: 0.000619\n",
      "=========> Epoch: 97 Average loss: 0.0006\n",
      "Correlation coefficient: 0.6268\n",
      "Training epoch: 98 [0/900 (0%)]\tLoss: 0.000930\n",
      "Training epoch: 98 [160/900 (18%)]\tLoss: 0.000406\n",
      "Training epoch: 98 [320/900 (35%)]\tLoss: 0.000583\n",
      "Training epoch: 98 [480/900 (53%)]\tLoss: 0.000238\n",
      "Training epoch: 98 [640/900 (70%)]\tLoss: 0.000186\n",
      "Training epoch: 98 [800/900 (88%)]\tLoss: 0.001059\n",
      "=========> Epoch: 98 Average loss: 0.0006\n",
      "Correlation coefficient: 0.6283\n",
      "Training epoch: 99 [0/900 (0%)]\tLoss: 0.000591\n",
      "Training epoch: 99 [160/900 (18%)]\tLoss: 0.000476\n",
      "Training epoch: 99 [320/900 (35%)]\tLoss: 0.000437\n",
      "Training epoch: 99 [480/900 (53%)]\tLoss: 0.000361\n",
      "Training epoch: 99 [640/900 (70%)]\tLoss: 0.000396\n",
      "Training epoch: 99 [800/900 (88%)]\tLoss: 0.001094\n",
      "=========> Epoch: 99 Average loss: 0.0007\n",
      "Correlation coefficient: 0.6273\n",
      "Training epoch: 100 [0/900 (0%)]\tLoss: 0.000262\n",
      "Training epoch: 100 [160/900 (18%)]\tLoss: 0.000633\n",
      "Training epoch: 100 [320/900 (35%)]\tLoss: 0.000965\n",
      "Training epoch: 100 [480/900 (53%)]\tLoss: 0.000354\n",
      "Training epoch: 100 [640/900 (70%)]\tLoss: 0.000180\n",
      "Training epoch: 100 [800/900 (88%)]\tLoss: 0.000954\n",
      "=========> Epoch: 100 Average loss: 0.0009\n",
      "Correlation coefficient: 0.6280\n",
      "Training epoch: 101 [0/900 (0%)]\tLoss: 0.000779\n",
      "Training epoch: 101 [160/900 (18%)]\tLoss: 0.001252\n",
      "Training epoch: 101 [320/900 (35%)]\tLoss: 0.000378\n",
      "Training epoch: 101 [480/900 (53%)]\tLoss: 0.001874\n",
      "Training epoch: 101 [640/900 (70%)]\tLoss: 0.001183\n",
      "Training epoch: 101 [800/900 (88%)]\tLoss: 0.001979\n",
      "=========> Epoch: 101 Average loss: 0.0014\n",
      "Correlation coefficient: 0.6311\n",
      "Training epoch: 102 [0/900 (0%)]\tLoss: 0.000962\n",
      "Training epoch: 102 [160/900 (18%)]\tLoss: 0.001518\n",
      "Training epoch: 102 [320/900 (35%)]\tLoss: 0.000684\n",
      "Training epoch: 102 [480/900 (53%)]\tLoss: 0.001173\n",
      "Training epoch: 102 [640/900 (70%)]\tLoss: 0.002077\n",
      "Training epoch: 102 [800/900 (88%)]\tLoss: 0.000991\n",
      "=========> Epoch: 102 Average loss: 0.0018\n",
      "Correlation coefficient: 0.6287\n",
      "Training epoch: 103 [0/900 (0%)]\tLoss: 0.002462\n",
      "Training epoch: 103 [160/900 (18%)]\tLoss: 0.003567\n",
      "Training epoch: 103 [320/900 (35%)]\tLoss: 0.001653\n",
      "Training epoch: 103 [480/900 (53%)]\tLoss: 0.004959\n",
      "Training epoch: 103 [640/900 (70%)]\tLoss: 0.000987\n",
      "Training epoch: 103 [800/900 (88%)]\tLoss: 0.003385\n",
      "=========> Epoch: 103 Average loss: 0.0029\n",
      "Correlation coefficient: 0.6241\n",
      "Training epoch: 104 [0/900 (0%)]\tLoss: 0.003223\n",
      "Training epoch: 104 [160/900 (18%)]\tLoss: 0.001756\n",
      "Training epoch: 104 [320/900 (35%)]\tLoss: 0.004712\n",
      "Training epoch: 104 [480/900 (53%)]\tLoss: 0.005801\n",
      "Training epoch: 104 [640/900 (70%)]\tLoss: 0.001769\n",
      "Training epoch: 104 [800/900 (88%)]\tLoss: 0.006759\n",
      "=========> Epoch: 104 Average loss: 0.0054\n",
      "Correlation coefficient: 0.6217\n",
      "Training epoch: 105 [0/900 (0%)]\tLoss: 0.011464\n",
      "Training epoch: 105 [160/900 (18%)]\tLoss: 0.005852\n",
      "Training epoch: 105 [320/900 (35%)]\tLoss: 0.003843\n",
      "Training epoch: 105 [480/900 (53%)]\tLoss: 0.010372\n",
      "Training epoch: 105 [640/900 (70%)]\tLoss: 0.004264\n",
      "Training epoch: 105 [800/900 (88%)]\tLoss: 0.003556\n",
      "=========> Epoch: 105 Average loss: 0.0091\n",
      "Correlation coefficient: 0.6306\n",
      "Training epoch: 106 [0/900 (0%)]\tLoss: 0.009914\n",
      "Training epoch: 106 [160/900 (18%)]\tLoss: 0.002423\n",
      "Training epoch: 106 [320/900 (35%)]\tLoss: 0.025180\n",
      "Training epoch: 106 [480/900 (53%)]\tLoss: 0.059051\n",
      "Training epoch: 106 [640/900 (70%)]\tLoss: 0.009204\n",
      "Training epoch: 106 [800/900 (88%)]\tLoss: 0.006869\n",
      "=========> Epoch: 106 Average loss: 0.0145\n",
      "Correlation coefficient: 0.6119\n",
      "Training epoch: 107 [0/900 (0%)]\tLoss: 0.017111\n",
      "Training epoch: 107 [160/900 (18%)]\tLoss: 0.024208\n",
      "Training epoch: 107 [320/900 (35%)]\tLoss: 0.023818\n",
      "Training epoch: 107 [480/900 (53%)]\tLoss: 0.005099\n",
      "Training epoch: 107 [640/900 (70%)]\tLoss: 0.012118\n",
      "Training epoch: 107 [800/900 (88%)]\tLoss: 0.004452\n",
      "=========> Epoch: 107 Average loss: 0.0164\n",
      "Correlation coefficient: 0.6299\n",
      "Training epoch: 108 [0/900 (0%)]\tLoss: 0.012687\n",
      "Training epoch: 108 [160/900 (18%)]\tLoss: 0.008174\n",
      "Training epoch: 108 [320/900 (35%)]\tLoss: 0.011819\n",
      "Training epoch: 108 [480/900 (53%)]\tLoss: 0.022170\n",
      "Training epoch: 108 [640/900 (70%)]\tLoss: 0.011140\n",
      "Training epoch: 108 [800/900 (88%)]\tLoss: 0.009381\n",
      "=========> Epoch: 108 Average loss: 0.0134\n",
      "Correlation coefficient: 0.5968\n",
      "Training epoch: 109 [0/900 (0%)]\tLoss: 0.026051\n",
      "Training epoch: 109 [160/900 (18%)]\tLoss: 0.005931\n",
      "Training epoch: 109 [320/900 (35%)]\tLoss: 0.004887\n",
      "Training epoch: 109 [480/900 (53%)]\tLoss: 0.006938\n",
      "Training epoch: 109 [640/900 (70%)]\tLoss: 0.013119\n",
      "Training epoch: 109 [800/900 (88%)]\tLoss: 0.015417\n",
      "=========> Epoch: 109 Average loss: 0.0130\n",
      "Correlation coefficient: 0.6154\n",
      "Training epoch: 110 [0/900 (0%)]\tLoss: 0.008710\n",
      "Training epoch: 110 [160/900 (18%)]\tLoss: 0.016532\n",
      "Training epoch: 110 [320/900 (35%)]\tLoss: 0.013289\n",
      "Training epoch: 110 [480/900 (53%)]\tLoss: 0.003266\n",
      "Training epoch: 110 [640/900 (70%)]\tLoss: 0.011219\n",
      "Training epoch: 110 [800/900 (88%)]\tLoss: 0.006016\n",
      "=========> Epoch: 110 Average loss: 0.0117\n",
      "Correlation coefficient: 0.6192\n",
      "Training epoch: 111 [0/900 (0%)]\tLoss: 0.009723\n",
      "Training epoch: 111 [160/900 (18%)]\tLoss: 0.005220\n",
      "Training epoch: 111 [320/900 (35%)]\tLoss: 0.007661\n",
      "Training epoch: 111 [480/900 (53%)]\tLoss: 0.003682\n",
      "Training epoch: 111 [640/900 (70%)]\tLoss: 0.005133\n",
      "Training epoch: 111 [800/900 (88%)]\tLoss: 0.020056\n",
      "=========> Epoch: 111 Average loss: 0.0098\n",
      "Correlation coefficient: 0.6378\n",
      "Training epoch: 112 [0/900 (0%)]\tLoss: 0.008442\n",
      "Training epoch: 112 [160/900 (18%)]\tLoss: 0.003614\n",
      "Training epoch: 112 [320/900 (35%)]\tLoss: 0.004508\n",
      "Training epoch: 112 [480/900 (53%)]\tLoss: 0.008952\n",
      "Training epoch: 112 [640/900 (70%)]\tLoss: 0.002847\n",
      "Training epoch: 112 [800/900 (88%)]\tLoss: 0.007943\n",
      "=========> Epoch: 112 Average loss: 0.0066\n",
      "Correlation coefficient: 0.6193\n",
      "Training epoch: 113 [0/900 (0%)]\tLoss: 0.004041\n",
      "Training epoch: 113 [160/900 (18%)]\tLoss: 0.004288\n",
      "Training epoch: 113 [320/900 (35%)]\tLoss: 0.002629\n",
      "Training epoch: 113 [480/900 (53%)]\tLoss: 0.005524\n",
      "Training epoch: 113 [640/900 (70%)]\tLoss: 0.005075\n",
      "Training epoch: 113 [800/900 (88%)]\tLoss: 0.006223\n",
      "=========> Epoch: 113 Average loss: 0.0059\n",
      "Correlation coefficient: 0.6337\n",
      "Training epoch: 114 [0/900 (0%)]\tLoss: 0.011641\n",
      "Training epoch: 114 [160/900 (18%)]\tLoss: 0.005126\n",
      "Training epoch: 114 [320/900 (35%)]\tLoss: 0.005408\n",
      "Training epoch: 114 [480/900 (53%)]\tLoss: 0.004211\n",
      "Training epoch: 114 [640/900 (70%)]\tLoss: 0.002835\n",
      "Training epoch: 114 [800/900 (88%)]\tLoss: 0.004183\n",
      "=========> Epoch: 114 Average loss: 0.0057\n",
      "Correlation coefficient: 0.6301\n",
      "Training epoch: 115 [0/900 (0%)]\tLoss: 0.002788\n",
      "Training epoch: 115 [160/900 (18%)]\tLoss: 0.005475\n",
      "Training epoch: 115 [320/900 (35%)]\tLoss: 0.002955\n",
      "Training epoch: 115 [480/900 (53%)]\tLoss: 0.005273\n",
      "Training epoch: 115 [640/900 (70%)]\tLoss: 0.001829\n",
      "Training epoch: 115 [800/900 (88%)]\tLoss: 0.003848\n",
      "=========> Epoch: 115 Average loss: 0.0042\n",
      "Correlation coefficient: 0.6247\n",
      "Training epoch: 116 [0/900 (0%)]\tLoss: 0.001035\n",
      "Training epoch: 116 [160/900 (18%)]\tLoss: 0.001531\n",
      "Training epoch: 116 [320/900 (35%)]\tLoss: 0.002573\n",
      "Training epoch: 116 [480/900 (53%)]\tLoss: 0.005097\n",
      "Training epoch: 116 [640/900 (70%)]\tLoss: 0.002758\n",
      "Training epoch: 116 [800/900 (88%)]\tLoss: 0.001873\n",
      "=========> Epoch: 116 Average loss: 0.0022\n",
      "Correlation coefficient: 0.6238\n",
      "Training epoch: 117 [0/900 (0%)]\tLoss: 0.001546\n",
      "Training epoch: 117 [160/900 (18%)]\tLoss: 0.000507\n",
      "Training epoch: 117 [320/900 (35%)]\tLoss: 0.001800\n",
      "Training epoch: 117 [480/900 (53%)]\tLoss: 0.001607\n",
      "Training epoch: 117 [640/900 (70%)]\tLoss: 0.000413\n",
      "Training epoch: 117 [800/900 (88%)]\tLoss: 0.010239\n",
      "=========> Epoch: 117 Average loss: 0.0016\n",
      "Correlation coefficient: 0.6276\n",
      "Training epoch: 118 [0/900 (0%)]\tLoss: 0.001415\n",
      "Training epoch: 118 [160/900 (18%)]\tLoss: 0.004578\n",
      "Training epoch: 118 [320/900 (35%)]\tLoss: 0.000770\n",
      "Training epoch: 118 [480/900 (53%)]\tLoss: 0.001949\n",
      "Training epoch: 118 [640/900 (70%)]\tLoss: 0.000889\n",
      "Training epoch: 118 [800/900 (88%)]\tLoss: 0.000606\n",
      "=========> Epoch: 118 Average loss: 0.0013\n",
      "Correlation coefficient: 0.6287\n",
      "Training epoch: 119 [0/900 (0%)]\tLoss: 0.000481\n",
      "Training epoch: 119 [160/900 (18%)]\tLoss: 0.001218\n",
      "Training epoch: 119 [320/900 (35%)]\tLoss: 0.000835\n",
      "Training epoch: 119 [480/900 (53%)]\tLoss: 0.001116\n",
      "Training epoch: 119 [640/900 (70%)]\tLoss: 0.001071\n",
      "Training epoch: 119 [800/900 (88%)]\tLoss: 0.003067\n",
      "=========> Epoch: 119 Average loss: 0.0012\n",
      "Correlation coefficient: 0.6330\n",
      "Training epoch: 120 [0/900 (0%)]\tLoss: 0.000985\n",
      "Training epoch: 120 [160/900 (18%)]\tLoss: 0.001653\n",
      "Training epoch: 120 [320/900 (35%)]\tLoss: 0.001368\n",
      "Training epoch: 120 [480/900 (53%)]\tLoss: 0.001240\n",
      "Training epoch: 120 [640/900 (70%)]\tLoss: 0.001066\n",
      "Training epoch: 120 [800/900 (88%)]\tLoss: 0.001566\n",
      "=========> Epoch: 120 Average loss: 0.0010\n",
      "Correlation coefficient: 0.6263\n",
      "Training epoch: 121 [0/900 (0%)]\tLoss: 0.002262\n",
      "Training epoch: 121 [160/900 (18%)]\tLoss: 0.004065\n",
      "Training epoch: 121 [320/900 (35%)]\tLoss: 0.003057\n",
      "Training epoch: 121 [480/900 (53%)]\tLoss: 0.001486\n",
      "Training epoch: 121 [640/900 (70%)]\tLoss: 0.001423\n",
      "Training epoch: 121 [800/900 (88%)]\tLoss: 0.000302\n",
      "=========> Epoch: 121 Average loss: 0.0015\n",
      "Correlation coefficient: 0.6318\n",
      "Training epoch: 122 [0/900 (0%)]\tLoss: 0.000428\n",
      "Training epoch: 122 [160/900 (18%)]\tLoss: 0.000922\n",
      "Training epoch: 122 [320/900 (35%)]\tLoss: 0.002431\n",
      "Training epoch: 122 [480/900 (53%)]\tLoss: 0.001215\n",
      "Training epoch: 122 [640/900 (70%)]\tLoss: 0.002647\n",
      "Training epoch: 122 [800/900 (88%)]\tLoss: 0.000696\n",
      "=========> Epoch: 122 Average loss: 0.0016\n",
      "Correlation coefficient: 0.6324\n",
      "Training epoch: 123 [0/900 (0%)]\tLoss: 0.001727\n",
      "Training epoch: 123 [160/900 (18%)]\tLoss: 0.003431\n",
      "Training epoch: 123 [320/900 (35%)]\tLoss: 0.001694\n",
      "Training epoch: 123 [480/900 (53%)]\tLoss: 0.001698\n",
      "Training epoch: 123 [640/900 (70%)]\tLoss: 0.000848\n",
      "Training epoch: 123 [800/900 (88%)]\tLoss: 0.000993\n",
      "=========> Epoch: 123 Average loss: 0.0019\n",
      "Correlation coefficient: 0.6290\n",
      "Training epoch: 124 [0/900 (0%)]\tLoss: 0.002640\n",
      "Training epoch: 124 [160/900 (18%)]\tLoss: 0.002597\n",
      "Training epoch: 124 [320/900 (35%)]\tLoss: 0.001564\n",
      "Training epoch: 124 [480/900 (53%)]\tLoss: 0.001455\n",
      "Training epoch: 124 [640/900 (70%)]\tLoss: 0.001005\n",
      "Training epoch: 124 [800/900 (88%)]\tLoss: 0.000561\n",
      "=========> Epoch: 124 Average loss: 0.0020\n",
      "Correlation coefficient: 0.6324\n",
      "Training epoch: 125 [0/900 (0%)]\tLoss: 0.002441\n",
      "Training epoch: 125 [160/900 (18%)]\tLoss: 0.000434\n",
      "Training epoch: 125 [320/900 (35%)]\tLoss: 0.001046\n",
      "Training epoch: 125 [480/900 (53%)]\tLoss: 0.001610\n",
      "Training epoch: 125 [640/900 (70%)]\tLoss: 0.001132\n",
      "Training epoch: 125 [800/900 (88%)]\tLoss: 0.003084\n",
      "=========> Epoch: 125 Average loss: 0.0018\n",
      "Correlation coefficient: 0.6310\n",
      "Training epoch: 126 [0/900 (0%)]\tLoss: 0.001019\n",
      "Training epoch: 126 [160/900 (18%)]\tLoss: 0.003131\n",
      "Training epoch: 126 [320/900 (35%)]\tLoss: 0.010135\n",
      "Training epoch: 126 [480/900 (53%)]\tLoss: 0.004553\n",
      "Training epoch: 126 [640/900 (70%)]\tLoss: 0.001283\n",
      "Training epoch: 126 [800/900 (88%)]\tLoss: 0.005627\n",
      "=========> Epoch: 126 Average loss: 0.0056\n",
      "Correlation coefficient: 0.6225\n",
      "Training epoch: 127 [0/900 (0%)]\tLoss: 0.008422\n",
      "Training epoch: 127 [160/900 (18%)]\tLoss: 0.003029\n",
      "Training epoch: 127 [320/900 (35%)]\tLoss: 0.006181\n",
      "Training epoch: 127 [480/900 (53%)]\tLoss: 0.005805\n",
      "Training epoch: 127 [640/900 (70%)]\tLoss: 0.011035\n",
      "Training epoch: 127 [800/900 (88%)]\tLoss: 0.007194\n",
      "=========> Epoch: 127 Average loss: 0.0123\n",
      "Correlation coefficient: 0.6160\n",
      "Training epoch: 128 [0/900 (0%)]\tLoss: 0.009618\n",
      "Training epoch: 128 [160/900 (18%)]\tLoss: 0.008129\n",
      "Training epoch: 128 [320/900 (35%)]\tLoss: 0.016106\n",
      "Training epoch: 128 [480/900 (53%)]\tLoss: 0.006859\n",
      "Training epoch: 128 [640/900 (70%)]\tLoss: 0.011088\n",
      "Training epoch: 128 [800/900 (88%)]\tLoss: 0.005164\n",
      "=========> Epoch: 128 Average loss: 0.0118\n",
      "Correlation coefficient: 0.6293\n",
      "Training epoch: 129 [0/900 (0%)]\tLoss: 0.014269\n",
      "Training epoch: 129 [160/900 (18%)]\tLoss: 0.011361\n",
      "Training epoch: 129 [320/900 (35%)]\tLoss: 0.012429\n",
      "Training epoch: 129 [480/900 (53%)]\tLoss: 0.019421\n",
      "Training epoch: 129 [640/900 (70%)]\tLoss: 0.009811\n",
      "Training epoch: 129 [800/900 (88%)]\tLoss: 0.016796\n",
      "=========> Epoch: 129 Average loss: 0.0145\n",
      "Correlation coefficient: 0.6347\n",
      "Training epoch: 130 [0/900 (0%)]\tLoss: 0.010039\n",
      "Training epoch: 130 [160/900 (18%)]\tLoss: 0.013574\n",
      "Training epoch: 130 [320/900 (35%)]\tLoss: 0.010279\n",
      "Training epoch: 130 [480/900 (53%)]\tLoss: 0.024299\n",
      "Training epoch: 130 [640/900 (70%)]\tLoss: 0.013053\n",
      "Training epoch: 130 [800/900 (88%)]\tLoss: 0.009553\n",
      "=========> Epoch: 130 Average loss: 0.0148\n",
      "Correlation coefficient: 0.6368\n",
      "Training epoch: 131 [0/900 (0%)]\tLoss: 0.004564\n",
      "Training epoch: 131 [160/900 (18%)]\tLoss: 0.010824\n",
      "Training epoch: 131 [320/900 (35%)]\tLoss: 0.007180\n",
      "Training epoch: 131 [480/900 (53%)]\tLoss: 0.021387\n",
      "Training epoch: 131 [640/900 (70%)]\tLoss: 0.008422\n",
      "Training epoch: 131 [800/900 (88%)]\tLoss: 0.011797\n",
      "=========> Epoch: 131 Average loss: 0.0166\n",
      "Correlation coefficient: 0.6121\n",
      "⏹️  Epoch 131 early stopping (no improvement for 50 epochs)\n",
      "🏁 Fold 5 best correlation: 0.6420\n",
      "\n",
      "========== Cross-validation Fold 6/10 ==========\n",
      "🔄 Fold 6: Using random initialization\n",
      "Training epoch: 1 [0/900 (0%)]\tLoss: 0.600788\n",
      "Training epoch: 1 [160/900 (18%)]\tLoss: 1.058242\n",
      "Training epoch: 1 [320/900 (35%)]\tLoss: 0.604042\n",
      "Training epoch: 1 [480/900 (53%)]\tLoss: 0.405446\n",
      "Training epoch: 1 [640/900 (70%)]\tLoss: 0.769127\n",
      "Training epoch: 1 [800/900 (88%)]\tLoss: 0.636013\n",
      "=========> Epoch: 1 Average loss: 0.8019\n",
      "Correlation coefficient: 0.6832\n",
      "✅ Epoch 1: New best correlation = 0.6832\n",
      "Training epoch: 2 [0/900 (0%)]\tLoss: 0.762430\n",
      "Training epoch: 2 [160/900 (18%)]\tLoss: 0.609003\n",
      "Training epoch: 2 [320/900 (35%)]\tLoss: 0.440049\n",
      "Training epoch: 2 [480/900 (53%)]\tLoss: 0.350052\n",
      "Training epoch: 2 [640/900 (70%)]\tLoss: 0.232809\n",
      "Training epoch: 2 [800/900 (88%)]\tLoss: 0.753961\n",
      "=========> Epoch: 2 Average loss: 0.3743\n",
      "Correlation coefficient: 0.6682\n",
      "Training epoch: 3 [0/900 (0%)]\tLoss: 0.133166\n",
      "Training epoch: 3 [160/900 (18%)]\tLoss: 0.047923\n",
      "Training epoch: 3 [320/900 (35%)]\tLoss: 0.096132\n",
      "Training epoch: 3 [480/900 (53%)]\tLoss: 0.226342\n",
      "Training epoch: 3 [640/900 (70%)]\tLoss: 0.220788\n",
      "Training epoch: 3 [800/900 (88%)]\tLoss: 0.085450\n",
      "=========> Epoch: 3 Average loss: 0.1088\n",
      "Correlation coefficient: 0.6670\n",
      "Training epoch: 4 [0/900 (0%)]\tLoss: 0.042973\n",
      "Training epoch: 4 [160/900 (18%)]\tLoss: 0.047453\n",
      "Training epoch: 4 [320/900 (35%)]\tLoss: 0.049778\n",
      "Training epoch: 4 [480/900 (53%)]\tLoss: 0.045274\n",
      "Training epoch: 4 [640/900 (70%)]\tLoss: 0.098237\n",
      "Training epoch: 4 [800/900 (88%)]\tLoss: 0.048519\n",
      "=========> Epoch: 4 Average loss: 0.0530\n",
      "Correlation coefficient: 0.7031\n",
      "✅ Epoch 4: New best correlation = 0.7031\n",
      "Training epoch: 5 [0/900 (0%)]\tLoss: 0.038391\n",
      "Training epoch: 5 [160/900 (18%)]\tLoss: 0.028826\n",
      "Training epoch: 5 [320/900 (35%)]\tLoss: 0.036595\n",
      "Training epoch: 5 [480/900 (53%)]\tLoss: 0.056422\n",
      "Training epoch: 5 [640/900 (70%)]\tLoss: 0.015259\n",
      "Training epoch: 5 [800/900 (88%)]\tLoss: 0.045360\n",
      "=========> Epoch: 5 Average loss: 0.0337\n",
      "Correlation coefficient: 0.6745\n",
      "Training epoch: 6 [0/900 (0%)]\tLoss: 0.030032\n",
      "Training epoch: 6 [160/900 (18%)]\tLoss: 0.022005\n",
      "Training epoch: 6 [320/900 (35%)]\tLoss: 0.020221\n",
      "Training epoch: 6 [480/900 (53%)]\tLoss: 0.020850\n",
      "Training epoch: 6 [640/900 (70%)]\tLoss: 0.008104\n",
      "Training epoch: 6 [800/900 (88%)]\tLoss: 0.016339\n",
      "=========> Epoch: 6 Average loss: 0.0167\n",
      "Correlation coefficient: 0.6875\n",
      "Training epoch: 7 [0/900 (0%)]\tLoss: 0.011449\n",
      "Training epoch: 7 [160/900 (18%)]\tLoss: 0.009560\n",
      "Training epoch: 7 [320/900 (35%)]\tLoss: 0.014000\n",
      "Training epoch: 7 [480/900 (53%)]\tLoss: 0.007774\n",
      "Training epoch: 7 [640/900 (70%)]\tLoss: 0.027130\n",
      "Training epoch: 7 [800/900 (88%)]\tLoss: 0.011159\n",
      "=========> Epoch: 7 Average loss: 0.0117\n",
      "Correlation coefficient: 0.6835\n",
      "Training epoch: 8 [0/900 (0%)]\tLoss: 0.003504\n",
      "Training epoch: 8 [160/900 (18%)]\tLoss: 0.013974\n",
      "Training epoch: 8 [320/900 (35%)]\tLoss: 0.010472\n",
      "Training epoch: 8 [480/900 (53%)]\tLoss: 0.012575\n",
      "Training epoch: 8 [640/900 (70%)]\tLoss: 0.009944\n",
      "Training epoch: 8 [800/900 (88%)]\tLoss: 0.007934\n",
      "=========> Epoch: 8 Average loss: 0.0097\n",
      "Correlation coefficient: 0.6849\n",
      "Training epoch: 9 [0/900 (0%)]\tLoss: 0.022822\n",
      "Training epoch: 9 [160/900 (18%)]\tLoss: 0.010553\n",
      "Training epoch: 9 [320/900 (35%)]\tLoss: 0.012600\n",
      "Training epoch: 9 [480/900 (53%)]\tLoss: 0.006585\n",
      "Training epoch: 9 [640/900 (70%)]\tLoss: 0.010678\n",
      "Training epoch: 9 [800/900 (88%)]\tLoss: 0.007018\n",
      "=========> Epoch: 9 Average loss: 0.0125\n",
      "Correlation coefficient: 0.6868\n",
      "Training epoch: 10 [0/900 (0%)]\tLoss: 0.014055\n",
      "Training epoch: 10 [160/900 (18%)]\tLoss: 0.018388\n",
      "Training epoch: 10 [320/900 (35%)]\tLoss: 0.027034\n",
      "Training epoch: 10 [480/900 (53%)]\tLoss: 0.009540\n",
      "Training epoch: 10 [640/900 (70%)]\tLoss: 0.021564\n",
      "Training epoch: 10 [800/900 (88%)]\tLoss: 0.007358\n",
      "=========> Epoch: 10 Average loss: 0.0157\n",
      "Correlation coefficient: 0.6807\n",
      "Training epoch: 11 [0/900 (0%)]\tLoss: 0.009549\n",
      "Training epoch: 11 [160/900 (18%)]\tLoss: 0.012888\n",
      "Training epoch: 11 [320/900 (35%)]\tLoss: 0.012341\n",
      "Training epoch: 11 [480/900 (53%)]\tLoss: 0.015304\n",
      "Training epoch: 11 [640/900 (70%)]\tLoss: 0.015141\n",
      "Training epoch: 11 [800/900 (88%)]\tLoss: 0.008704\n",
      "=========> Epoch: 11 Average loss: 0.0165\n",
      "Correlation coefficient: 0.6951\n",
      "Training epoch: 12 [0/900 (0%)]\tLoss: 0.011697\n",
      "Training epoch: 12 [160/900 (18%)]\tLoss: 0.009946\n",
      "Training epoch: 12 [320/900 (35%)]\tLoss: 0.016373\n",
      "Training epoch: 12 [480/900 (53%)]\tLoss: 0.008467\n",
      "Training epoch: 12 [640/900 (70%)]\tLoss: 0.011413\n",
      "Training epoch: 12 [800/900 (88%)]\tLoss: 0.012926\n",
      "=========> Epoch: 12 Average loss: 0.0131\n",
      "Correlation coefficient: 0.6931\n",
      "Training epoch: 13 [0/900 (0%)]\tLoss: 0.006048\n",
      "Training epoch: 13 [160/900 (18%)]\tLoss: 0.013302\n",
      "Training epoch: 13 [320/900 (35%)]\tLoss: 0.008577\n",
      "Training epoch: 13 [480/900 (53%)]\tLoss: 0.012011\n",
      "Training epoch: 13 [640/900 (70%)]\tLoss: 0.006201\n",
      "Training epoch: 13 [800/900 (88%)]\tLoss: 0.013722\n",
      "=========> Epoch: 13 Average loss: 0.0095\n",
      "Correlation coefficient: 0.6958\n",
      "Training epoch: 14 [0/900 (0%)]\tLoss: 0.005559\n",
      "Training epoch: 14 [160/900 (18%)]\tLoss: 0.017821\n",
      "Training epoch: 14 [320/900 (35%)]\tLoss: 0.006925\n",
      "Training epoch: 14 [480/900 (53%)]\tLoss: 0.002007\n",
      "Training epoch: 14 [640/900 (70%)]\tLoss: 0.014114\n",
      "Training epoch: 14 [800/900 (88%)]\tLoss: 0.008308\n",
      "=========> Epoch: 14 Average loss: 0.0086\n",
      "Correlation coefficient: 0.6849\n",
      "Training epoch: 15 [0/900 (0%)]\tLoss: 0.006809\n",
      "Training epoch: 15 [160/900 (18%)]\tLoss: 0.013917\n",
      "Training epoch: 15 [320/900 (35%)]\tLoss: 0.005725\n",
      "Training epoch: 15 [480/900 (53%)]\tLoss: 0.009566\n",
      "Training epoch: 15 [640/900 (70%)]\tLoss: 0.026947\n",
      "Training epoch: 15 [800/900 (88%)]\tLoss: 0.005431\n",
      "=========> Epoch: 15 Average loss: 0.0078\n",
      "Correlation coefficient: 0.7069\n",
      "✅ Epoch 15: New best correlation = 0.7069\n",
      "Training epoch: 16 [0/900 (0%)]\tLoss: 0.004183\n",
      "Training epoch: 16 [160/900 (18%)]\tLoss: 0.010224\n",
      "Training epoch: 16 [320/900 (35%)]\tLoss: 0.007698\n",
      "Training epoch: 16 [480/900 (53%)]\tLoss: 0.019083\n",
      "Training epoch: 16 [640/900 (70%)]\tLoss: 0.015573\n",
      "Training epoch: 16 [800/900 (88%)]\tLoss: 0.008947\n",
      "=========> Epoch: 16 Average loss: 0.0081\n",
      "Correlation coefficient: 0.6859\n",
      "Training epoch: 17 [0/900 (0%)]\tLoss: 0.011384\n",
      "Training epoch: 17 [160/900 (18%)]\tLoss: 0.008353\n",
      "Training epoch: 17 [320/900 (35%)]\tLoss: 0.004692\n",
      "Training epoch: 17 [480/900 (53%)]\tLoss: 0.009257\n",
      "Training epoch: 17 [640/900 (70%)]\tLoss: 0.016675\n",
      "Training epoch: 17 [800/900 (88%)]\tLoss: 0.006879\n",
      "=========> Epoch: 17 Average loss: 0.0081\n",
      "Correlation coefficient: 0.6977\n",
      "Training epoch: 18 [0/900 (0%)]\tLoss: 0.003005\n",
      "Training epoch: 18 [160/900 (18%)]\tLoss: 0.003641\n",
      "Training epoch: 18 [320/900 (35%)]\tLoss: 0.006146\n",
      "Training epoch: 18 [480/900 (53%)]\tLoss: 0.012694\n",
      "Training epoch: 18 [640/900 (70%)]\tLoss: 0.005471\n",
      "Training epoch: 18 [800/900 (88%)]\tLoss: 0.003785\n",
      "=========> Epoch: 18 Average loss: 0.0083\n",
      "Correlation coefficient: 0.6841\n",
      "Training epoch: 19 [0/900 (0%)]\tLoss: 0.008257\n",
      "Training epoch: 19 [160/900 (18%)]\tLoss: 0.012106\n",
      "Training epoch: 19 [320/900 (35%)]\tLoss: 0.009821\n",
      "Training epoch: 19 [480/900 (53%)]\tLoss: 0.026780\n",
      "Training epoch: 19 [640/900 (70%)]\tLoss: 0.009947\n",
      "Training epoch: 19 [800/900 (88%)]\tLoss: 0.008919\n",
      "=========> Epoch: 19 Average loss: 0.0111\n",
      "Correlation coefficient: 0.6926\n",
      "Training epoch: 20 [0/900 (0%)]\tLoss: 0.006145\n",
      "Training epoch: 20 [160/900 (18%)]\tLoss: 0.009777\n",
      "Training epoch: 20 [320/900 (35%)]\tLoss: 0.019722\n",
      "Training epoch: 20 [480/900 (53%)]\tLoss: 0.016964\n",
      "Training epoch: 20 [640/900 (70%)]\tLoss: 0.014173\n",
      "Training epoch: 20 [800/900 (88%)]\tLoss: 0.014217\n",
      "=========> Epoch: 20 Average loss: 0.0133\n",
      "Correlation coefficient: 0.6801\n",
      "Training epoch: 21 [0/900 (0%)]\tLoss: 0.008025\n",
      "Training epoch: 21 [160/900 (18%)]\tLoss: 0.032957\n",
      "Training epoch: 21 [320/900 (35%)]\tLoss: 0.018136\n",
      "Training epoch: 21 [480/900 (53%)]\tLoss: 0.021094\n",
      "Training epoch: 21 [640/900 (70%)]\tLoss: 0.012923\n",
      "Training epoch: 21 [800/900 (88%)]\tLoss: 0.005495\n",
      "=========> Epoch: 21 Average loss: 0.0137\n",
      "Correlation coefficient: 0.6954\n",
      "Training epoch: 22 [0/900 (0%)]\tLoss: 0.018580\n",
      "Training epoch: 22 [160/900 (18%)]\tLoss: 0.010703\n",
      "Training epoch: 22 [320/900 (35%)]\tLoss: 0.016868\n",
      "Training epoch: 22 [480/900 (53%)]\tLoss: 0.006488\n",
      "Training epoch: 22 [640/900 (70%)]\tLoss: 0.016414\n",
      "Training epoch: 22 [800/900 (88%)]\tLoss: 0.018304\n",
      "=========> Epoch: 22 Average loss: 0.0150\n",
      "Correlation coefficient: 0.6922\n",
      "Training epoch: 23 [0/900 (0%)]\tLoss: 0.006311\n",
      "Training epoch: 23 [160/900 (18%)]\tLoss: 0.021783\n",
      "Training epoch: 23 [320/900 (35%)]\tLoss: 0.009761\n",
      "Training epoch: 23 [480/900 (53%)]\tLoss: 0.003540\n",
      "Training epoch: 23 [640/900 (70%)]\tLoss: 0.008345\n",
      "Training epoch: 23 [800/900 (88%)]\tLoss: 0.011837\n",
      "=========> Epoch: 23 Average loss: 0.0118\n",
      "Correlation coefficient: 0.6885\n",
      "Training epoch: 24 [0/900 (0%)]\tLoss: 0.005890\n",
      "Training epoch: 24 [160/900 (18%)]\tLoss: 0.027784\n",
      "Training epoch: 24 [320/900 (35%)]\tLoss: 0.009349\n",
      "Training epoch: 24 [480/900 (53%)]\tLoss: 0.018804\n",
      "Training epoch: 24 [640/900 (70%)]\tLoss: 0.013003\n",
      "Training epoch: 24 [800/900 (88%)]\tLoss: 0.015513\n",
      "=========> Epoch: 24 Average loss: 0.0135\n",
      "Correlation coefficient: 0.7013\n",
      "Training epoch: 25 [0/900 (0%)]\tLoss: 0.012979\n",
      "Training epoch: 25 [160/900 (18%)]\tLoss: 0.016719\n",
      "Training epoch: 25 [320/900 (35%)]\tLoss: 0.006555\n",
      "Training epoch: 25 [480/900 (53%)]\tLoss: 0.015109\n",
      "Training epoch: 25 [640/900 (70%)]\tLoss: 0.017697\n",
      "Training epoch: 25 [800/900 (88%)]\tLoss: 0.005793\n",
      "=========> Epoch: 25 Average loss: 0.0156\n",
      "Correlation coefficient: 0.6894\n",
      "Training epoch: 26 [0/900 (0%)]\tLoss: 0.026880\n",
      "Training epoch: 26 [160/900 (18%)]\tLoss: 0.020701\n",
      "Training epoch: 26 [320/900 (35%)]\tLoss: 0.012697\n",
      "Training epoch: 26 [480/900 (53%)]\tLoss: 0.018174\n",
      "Training epoch: 26 [640/900 (70%)]\tLoss: 0.018899\n",
      "Training epoch: 26 [800/900 (88%)]\tLoss: 0.007189\n",
      "=========> Epoch: 26 Average loss: 0.0168\n",
      "Correlation coefficient: 0.6959\n",
      "Training epoch: 27 [0/900 (0%)]\tLoss: 0.015794\n",
      "Training epoch: 27 [160/900 (18%)]\tLoss: 0.014190\n",
      "Training epoch: 27 [320/900 (35%)]\tLoss: 0.019935\n",
      "Training epoch: 27 [480/900 (53%)]\tLoss: 0.011835\n",
      "Training epoch: 27 [640/900 (70%)]\tLoss: 0.010950\n",
      "Training epoch: 27 [800/900 (88%)]\tLoss: 0.040381\n",
      "=========> Epoch: 27 Average loss: 0.0165\n",
      "Correlation coefficient: 0.6974\n",
      "Training epoch: 28 [0/900 (0%)]\tLoss: 0.016113\n",
      "Training epoch: 28 [160/900 (18%)]\tLoss: 0.012886\n",
      "Training epoch: 28 [320/900 (35%)]\tLoss: 0.013888\n",
      "Training epoch: 28 [480/900 (53%)]\tLoss: 0.061187\n",
      "Training epoch: 28 [640/900 (70%)]\tLoss: 0.008477\n",
      "Training epoch: 28 [800/900 (88%)]\tLoss: 0.015570\n",
      "=========> Epoch: 28 Average loss: 0.0135\n",
      "Correlation coefficient: 0.6943\n",
      "Training epoch: 29 [0/900 (0%)]\tLoss: 0.004973\n",
      "Training epoch: 29 [160/900 (18%)]\tLoss: 0.025530\n",
      "Training epoch: 29 [320/900 (35%)]\tLoss: 0.014213\n",
      "Training epoch: 29 [480/900 (53%)]\tLoss: 0.003570\n",
      "Training epoch: 29 [640/900 (70%)]\tLoss: 0.014432\n",
      "Training epoch: 29 [800/900 (88%)]\tLoss: 0.002918\n",
      "=========> Epoch: 29 Average loss: 0.0128\n",
      "Correlation coefficient: 0.6857\n",
      "Training epoch: 30 [0/900 (0%)]\tLoss: 0.012883\n",
      "Training epoch: 30 [160/900 (18%)]\tLoss: 0.012731\n",
      "Training epoch: 30 [320/900 (35%)]\tLoss: 0.038792\n",
      "Training epoch: 30 [480/900 (53%)]\tLoss: 0.008682\n",
      "Training epoch: 30 [640/900 (70%)]\tLoss: 0.010837\n",
      "Training epoch: 30 [800/900 (88%)]\tLoss: 0.013588\n",
      "=========> Epoch: 30 Average loss: 0.0140\n",
      "Correlation coefficient: 0.6878\n",
      "Training epoch: 31 [0/900 (0%)]\tLoss: 0.005594\n",
      "Training epoch: 31 [160/900 (18%)]\tLoss: 0.044592\n",
      "Training epoch: 31 [320/900 (35%)]\tLoss: 0.011090\n",
      "Training epoch: 31 [480/900 (53%)]\tLoss: 0.011358\n",
      "Training epoch: 31 [640/900 (70%)]\tLoss: 0.006886\n",
      "Training epoch: 31 [800/900 (88%)]\tLoss: 0.005966\n",
      "=========> Epoch: 31 Average loss: 0.0113\n",
      "Correlation coefficient: 0.6794\n",
      "Training epoch: 32 [0/900 (0%)]\tLoss: 0.005541\n",
      "Training epoch: 32 [160/900 (18%)]\tLoss: 0.023003\n",
      "Training epoch: 32 [320/900 (35%)]\tLoss: 0.007565\n",
      "Training epoch: 32 [480/900 (53%)]\tLoss: 0.007626\n",
      "Training epoch: 32 [640/900 (70%)]\tLoss: 0.006535\n",
      "Training epoch: 32 [800/900 (88%)]\tLoss: 0.007062\n",
      "=========> Epoch: 32 Average loss: 0.0095\n",
      "Correlation coefficient: 0.6960\n",
      "Training epoch: 33 [0/900 (0%)]\tLoss: 0.008988\n",
      "Training epoch: 33 [160/900 (18%)]\tLoss: 0.003969\n",
      "Training epoch: 33 [320/900 (35%)]\tLoss: 0.005360\n",
      "Training epoch: 33 [480/900 (53%)]\tLoss: 0.011336\n",
      "Training epoch: 33 [640/900 (70%)]\tLoss: 0.003142\n",
      "Training epoch: 33 [800/900 (88%)]\tLoss: 0.012256\n",
      "=========> Epoch: 33 Average loss: 0.0093\n",
      "Correlation coefficient: 0.6895\n",
      "Training epoch: 34 [0/900 (0%)]\tLoss: 0.005834\n",
      "Training epoch: 34 [160/900 (18%)]\tLoss: 0.004383\n",
      "Training epoch: 34 [320/900 (35%)]\tLoss: 0.015782\n",
      "Training epoch: 34 [480/900 (53%)]\tLoss: 0.013250\n",
      "Training epoch: 34 [640/900 (70%)]\tLoss: 0.007658\n",
      "Training epoch: 34 [800/900 (88%)]\tLoss: 0.011128\n",
      "=========> Epoch: 34 Average loss: 0.0111\n",
      "Correlation coefficient: 0.6836\n",
      "Training epoch: 35 [0/900 (0%)]\tLoss: 0.019530\n",
      "Training epoch: 35 [160/900 (18%)]\tLoss: 0.026607\n",
      "Training epoch: 35 [320/900 (35%)]\tLoss: 0.010402\n",
      "Training epoch: 35 [480/900 (53%)]\tLoss: 0.011846\n",
      "Training epoch: 35 [640/900 (70%)]\tLoss: 0.007069\n",
      "Training epoch: 35 [800/900 (88%)]\tLoss: 0.020017\n",
      "=========> Epoch: 35 Average loss: 0.0106\n",
      "Correlation coefficient: 0.6934\n",
      "Training epoch: 36 [0/900 (0%)]\tLoss: 0.009540\n",
      "Training epoch: 36 [160/900 (18%)]\tLoss: 0.005094\n",
      "Training epoch: 36 [320/900 (35%)]\tLoss: 0.021523\n",
      "Training epoch: 36 [480/900 (53%)]\tLoss: 0.006779\n",
      "Training epoch: 36 [640/900 (70%)]\tLoss: 0.010878\n",
      "Training epoch: 36 [800/900 (88%)]\tLoss: 0.005744\n",
      "=========> Epoch: 36 Average loss: 0.0108\n",
      "Correlation coefficient: 0.6911\n",
      "Training epoch: 37 [0/900 (0%)]\tLoss: 0.006249\n",
      "Training epoch: 37 [160/900 (18%)]\tLoss: 0.009013\n",
      "Training epoch: 37 [320/900 (35%)]\tLoss: 0.012787\n",
      "Training epoch: 37 [480/900 (53%)]\tLoss: 0.009075\n",
      "Training epoch: 37 [640/900 (70%)]\tLoss: 0.009172\n",
      "Training epoch: 37 [800/900 (88%)]\tLoss: 0.006319\n",
      "=========> Epoch: 37 Average loss: 0.0092\n",
      "Correlation coefficient: 0.6989\n",
      "Training epoch: 38 [0/900 (0%)]\tLoss: 0.003381\n",
      "Training epoch: 38 [160/900 (18%)]\tLoss: 0.003068\n",
      "Training epoch: 38 [320/900 (35%)]\tLoss: 0.009941\n",
      "Training epoch: 38 [480/900 (53%)]\tLoss: 0.008718\n",
      "Training epoch: 38 [640/900 (70%)]\tLoss: 0.006089\n",
      "Training epoch: 38 [800/900 (88%)]\tLoss: 0.007274\n",
      "=========> Epoch: 38 Average loss: 0.0101\n",
      "Correlation coefficient: 0.6971\n",
      "Training epoch: 39 [0/900 (0%)]\tLoss: 0.006164\n",
      "Training epoch: 39 [160/900 (18%)]\tLoss: 0.014395\n",
      "Training epoch: 39 [320/900 (35%)]\tLoss: 0.032461\n",
      "Training epoch: 39 [480/900 (53%)]\tLoss: 0.008703\n",
      "Training epoch: 39 [640/900 (70%)]\tLoss: 0.008116\n",
      "Training epoch: 39 [800/900 (88%)]\tLoss: 0.003882\n",
      "=========> Epoch: 39 Average loss: 0.0098\n",
      "Correlation coefficient: 0.6847\n",
      "Training epoch: 40 [0/900 (0%)]\tLoss: 0.012513\n",
      "Training epoch: 40 [160/900 (18%)]\tLoss: 0.015864\n",
      "Training epoch: 40 [320/900 (35%)]\tLoss: 0.005648\n",
      "Training epoch: 40 [480/900 (53%)]\tLoss: 0.003839\n",
      "Training epoch: 40 [640/900 (70%)]\tLoss: 0.003723\n",
      "Training epoch: 40 [800/900 (88%)]\tLoss: 0.003052\n",
      "=========> Epoch: 40 Average loss: 0.0110\n",
      "Correlation coefficient: 0.6914\n",
      "Training epoch: 41 [0/900 (0%)]\tLoss: 0.007968\n",
      "Training epoch: 41 [160/900 (18%)]\tLoss: 0.007999\n",
      "Training epoch: 41 [320/900 (35%)]\tLoss: 0.020053\n",
      "Training epoch: 41 [480/900 (53%)]\tLoss: 0.012835\n",
      "Training epoch: 41 [640/900 (70%)]\tLoss: 0.004560\n",
      "Training epoch: 41 [800/900 (88%)]\tLoss: 0.008318\n",
      "=========> Epoch: 41 Average loss: 0.0118\n",
      "Correlation coefficient: 0.7000\n",
      "Training epoch: 42 [0/900 (0%)]\tLoss: 0.002452\n",
      "Training epoch: 42 [160/900 (18%)]\tLoss: 0.006913\n",
      "Training epoch: 42 [320/900 (35%)]\tLoss: 0.008828\n",
      "Training epoch: 42 [480/900 (53%)]\tLoss: 0.006825\n",
      "Training epoch: 42 [640/900 (70%)]\tLoss: 0.008698\n",
      "Training epoch: 42 [800/900 (88%)]\tLoss: 0.020794\n",
      "=========> Epoch: 42 Average loss: 0.0104\n",
      "Correlation coefficient: 0.6741\n",
      "Training epoch: 43 [0/900 (0%)]\tLoss: 0.008425\n",
      "Training epoch: 43 [160/900 (18%)]\tLoss: 0.006849\n",
      "Training epoch: 43 [320/900 (35%)]\tLoss: 0.002118\n",
      "Training epoch: 43 [480/900 (53%)]\tLoss: 0.005174\n",
      "Training epoch: 43 [640/900 (70%)]\tLoss: 0.006820\n",
      "Training epoch: 43 [800/900 (88%)]\tLoss: 0.003952\n",
      "=========> Epoch: 43 Average loss: 0.0096\n",
      "Correlation coefficient: 0.6960\n",
      "Training epoch: 44 [0/900 (0%)]\tLoss: 0.003189\n",
      "Training epoch: 44 [160/900 (18%)]\tLoss: 0.007820\n",
      "Training epoch: 44 [320/900 (35%)]\tLoss: 0.007606\n",
      "Training epoch: 44 [480/900 (53%)]\tLoss: 0.004871\n",
      "Training epoch: 44 [640/900 (70%)]\tLoss: 0.003540\n",
      "Training epoch: 44 [800/900 (88%)]\tLoss: 0.002425\n",
      "=========> Epoch: 44 Average loss: 0.0072\n",
      "Correlation coefficient: 0.6856\n",
      "Training epoch: 45 [0/900 (0%)]\tLoss: 0.001075\n",
      "Training epoch: 45 [160/900 (18%)]\tLoss: 0.007506\n",
      "Training epoch: 45 [320/900 (35%)]\tLoss: 0.002751\n",
      "Training epoch: 45 [480/900 (53%)]\tLoss: 0.001502\n",
      "Training epoch: 45 [640/900 (70%)]\tLoss: 0.012775\n",
      "Training epoch: 45 [800/900 (88%)]\tLoss: 0.014396\n",
      "=========> Epoch: 45 Average loss: 0.0055\n",
      "Correlation coefficient: 0.6887\n",
      "Training epoch: 46 [0/900 (0%)]\tLoss: 0.005584\n",
      "Training epoch: 46 [160/900 (18%)]\tLoss: 0.006793\n",
      "Training epoch: 46 [320/900 (35%)]\tLoss: 0.010641\n",
      "Training epoch: 46 [480/900 (53%)]\tLoss: 0.001249\n",
      "Training epoch: 46 [640/900 (70%)]\tLoss: 0.009925\n",
      "Training epoch: 46 [800/900 (88%)]\tLoss: 0.008658\n",
      "=========> Epoch: 46 Average loss: 0.0051\n",
      "Correlation coefficient: 0.6923\n",
      "Training epoch: 47 [0/900 (0%)]\tLoss: 0.002248\n",
      "Training epoch: 47 [160/900 (18%)]\tLoss: 0.005610\n",
      "Training epoch: 47 [320/900 (35%)]\tLoss: 0.016126\n",
      "Training epoch: 47 [480/900 (53%)]\tLoss: 0.005698\n",
      "Training epoch: 47 [640/900 (70%)]\tLoss: 0.002946\n",
      "Training epoch: 47 [800/900 (88%)]\tLoss: 0.013336\n",
      "=========> Epoch: 47 Average loss: 0.0067\n",
      "Correlation coefficient: 0.6857\n",
      "Training epoch: 48 [0/900 (0%)]\tLoss: 0.007213\n",
      "Training epoch: 48 [160/900 (18%)]\tLoss: 0.010969\n",
      "Training epoch: 48 [320/900 (35%)]\tLoss: 0.012265\n",
      "Training epoch: 48 [480/900 (53%)]\tLoss: 0.003692\n",
      "Training epoch: 48 [640/900 (70%)]\tLoss: 0.013529\n",
      "Training epoch: 48 [800/900 (88%)]\tLoss: 0.008759\n",
      "=========> Epoch: 48 Average loss: 0.0118\n",
      "Correlation coefficient: 0.6848\n",
      "Training epoch: 49 [0/900 (0%)]\tLoss: 0.013512\n",
      "Training epoch: 49 [160/900 (18%)]\tLoss: 0.011122\n",
      "Training epoch: 49 [320/900 (35%)]\tLoss: 0.017200\n",
      "Training epoch: 49 [480/900 (53%)]\tLoss: 0.008302\n",
      "Training epoch: 49 [640/900 (70%)]\tLoss: 0.020397\n",
      "Training epoch: 49 [800/900 (88%)]\tLoss: 0.012764\n",
      "=========> Epoch: 49 Average loss: 0.0152\n",
      "Correlation coefficient: 0.6794\n",
      "Training epoch: 50 [0/900 (0%)]\tLoss: 0.010351\n",
      "Training epoch: 50 [160/900 (18%)]\tLoss: 0.017525\n",
      "Training epoch: 50 [320/900 (35%)]\tLoss: 0.014874\n",
      "Training epoch: 50 [480/900 (53%)]\tLoss: 0.011431\n",
      "Training epoch: 50 [640/900 (70%)]\tLoss: 0.005325\n",
      "Training epoch: 50 [800/900 (88%)]\tLoss: 0.005884\n",
      "=========> Epoch: 50 Average loss: 0.0124\n",
      "Correlation coefficient: 0.6961\n",
      "Training epoch: 51 [0/900 (0%)]\tLoss: 0.006373\n",
      "Training epoch: 51 [160/900 (18%)]\tLoss: 0.019506\n",
      "Training epoch: 51 [320/900 (35%)]\tLoss: 0.009838\n",
      "Training epoch: 51 [480/900 (53%)]\tLoss: 0.016546\n",
      "Training epoch: 51 [640/900 (70%)]\tLoss: 0.016985\n",
      "Training epoch: 51 [800/900 (88%)]\tLoss: 0.017600\n",
      "=========> Epoch: 51 Average loss: 0.0128\n",
      "Correlation coefficient: 0.6877\n",
      "Training epoch: 52 [0/900 (0%)]\tLoss: 0.007899\n",
      "Training epoch: 52 [160/900 (18%)]\tLoss: 0.008920\n",
      "Training epoch: 52 [320/900 (35%)]\tLoss: 0.012038\n",
      "Training epoch: 52 [480/900 (53%)]\tLoss: 0.012133\n",
      "Training epoch: 52 [640/900 (70%)]\tLoss: 0.017383\n",
      "Training epoch: 52 [800/900 (88%)]\tLoss: 0.018176\n",
      "=========> Epoch: 52 Average loss: 0.0169\n",
      "Correlation coefficient: 0.6790\n",
      "Training epoch: 53 [0/900 (0%)]\tLoss: 0.017433\n",
      "Training epoch: 53 [160/900 (18%)]\tLoss: 0.005531\n",
      "Training epoch: 53 [320/900 (35%)]\tLoss: 0.022025\n",
      "Training epoch: 53 [480/900 (53%)]\tLoss: 0.020747\n",
      "Training epoch: 53 [640/900 (70%)]\tLoss: 0.015386\n",
      "Training epoch: 53 [800/900 (88%)]\tLoss: 0.006508\n",
      "=========> Epoch: 53 Average loss: 0.0171\n",
      "Correlation coefficient: 0.7058\n",
      "Training epoch: 54 [0/900 (0%)]\tLoss: 0.018083\n",
      "Training epoch: 54 [160/900 (18%)]\tLoss: 0.017152\n",
      "Training epoch: 54 [320/900 (35%)]\tLoss: 0.024548\n",
      "Training epoch: 54 [480/900 (53%)]\tLoss: 0.012548\n",
      "Training epoch: 54 [640/900 (70%)]\tLoss: 0.015143\n",
      "Training epoch: 54 [800/900 (88%)]\tLoss: 0.020622\n",
      "=========> Epoch: 54 Average loss: 0.0173\n",
      "Correlation coefficient: 0.6700\n",
      "Training epoch: 55 [0/900 (0%)]\tLoss: 0.058151\n",
      "Training epoch: 55 [160/900 (18%)]\tLoss: 0.009705\n",
      "Training epoch: 55 [320/900 (35%)]\tLoss: 0.014935\n",
      "Training epoch: 55 [480/900 (53%)]\tLoss: 0.016964\n",
      "Training epoch: 55 [640/900 (70%)]\tLoss: 0.017228\n",
      "Training epoch: 55 [800/900 (88%)]\tLoss: 0.008589\n",
      "=========> Epoch: 55 Average loss: 0.0183\n",
      "Correlation coefficient: 0.6986\n",
      "Training epoch: 56 [0/900 (0%)]\tLoss: 0.012487\n",
      "Training epoch: 56 [160/900 (18%)]\tLoss: 0.025263\n",
      "Training epoch: 56 [320/900 (35%)]\tLoss: 0.005475\n",
      "Training epoch: 56 [480/900 (53%)]\tLoss: 0.021037\n",
      "Training epoch: 56 [640/900 (70%)]\tLoss: 0.021162\n",
      "Training epoch: 56 [800/900 (88%)]\tLoss: 0.020835\n",
      "=========> Epoch: 56 Average loss: 0.0165\n",
      "Correlation coefficient: 0.6823\n",
      "Training epoch: 57 [0/900 (0%)]\tLoss: 0.012703\n",
      "Training epoch: 57 [160/900 (18%)]\tLoss: 0.014056\n",
      "Training epoch: 57 [320/900 (35%)]\tLoss: 0.012849\n",
      "Training epoch: 57 [480/900 (53%)]\tLoss: 0.006086\n",
      "Training epoch: 57 [640/900 (70%)]\tLoss: 0.005919\n",
      "Training epoch: 57 [800/900 (88%)]\tLoss: 0.010494\n",
      "=========> Epoch: 57 Average loss: 0.0123\n",
      "Correlation coefficient: 0.7011\n",
      "Training epoch: 58 [0/900 (0%)]\tLoss: 0.011879\n",
      "Training epoch: 58 [160/900 (18%)]\tLoss: 0.011663\n",
      "Training epoch: 58 [320/900 (35%)]\tLoss: 0.009010\n",
      "Training epoch: 58 [480/900 (53%)]\tLoss: 0.004827\n",
      "Training epoch: 58 [640/900 (70%)]\tLoss: 0.016756\n",
      "Training epoch: 58 [800/900 (88%)]\tLoss: 0.007589\n",
      "=========> Epoch: 58 Average loss: 0.0075\n",
      "Correlation coefficient: 0.6866\n",
      "Training epoch: 59 [0/900 (0%)]\tLoss: 0.006946\n",
      "Training epoch: 59 [160/900 (18%)]\tLoss: 0.003582\n",
      "Training epoch: 59 [320/900 (35%)]\tLoss: 0.002488\n",
      "Training epoch: 59 [480/900 (53%)]\tLoss: 0.001423\n",
      "Training epoch: 59 [640/900 (70%)]\tLoss: 0.004351\n",
      "Training epoch: 59 [800/900 (88%)]\tLoss: 0.008558\n",
      "=========> Epoch: 59 Average loss: 0.0046\n",
      "Correlation coefficient: 0.7028\n",
      "Training epoch: 60 [0/900 (0%)]\tLoss: 0.000975\n",
      "Training epoch: 60 [160/900 (18%)]\tLoss: 0.003733\n",
      "Training epoch: 60 [320/900 (35%)]\tLoss: 0.002829\n",
      "Training epoch: 60 [480/900 (53%)]\tLoss: 0.003289\n",
      "Training epoch: 60 [640/900 (70%)]\tLoss: 0.001798\n",
      "Training epoch: 60 [800/900 (88%)]\tLoss: 0.005564\n",
      "=========> Epoch: 60 Average loss: 0.0035\n",
      "Correlation coefficient: 0.6989\n",
      "Training epoch: 61 [0/900 (0%)]\tLoss: 0.002289\n",
      "Training epoch: 61 [160/900 (18%)]\tLoss: 0.001743\n",
      "Training epoch: 61 [320/900 (35%)]\tLoss: 0.001186\n",
      "Training epoch: 61 [480/900 (53%)]\tLoss: 0.014374\n",
      "Training epoch: 61 [640/900 (70%)]\tLoss: 0.002554\n",
      "Training epoch: 61 [800/900 (88%)]\tLoss: 0.001654\n",
      "=========> Epoch: 61 Average loss: 0.0041\n",
      "Correlation coefficient: 0.6984\n",
      "Training epoch: 62 [0/900 (0%)]\tLoss: 0.001225\n",
      "Training epoch: 62 [160/900 (18%)]\tLoss: 0.004411\n",
      "Training epoch: 62 [320/900 (35%)]\tLoss: 0.002550\n",
      "Training epoch: 62 [480/900 (53%)]\tLoss: 0.007894\n",
      "Training epoch: 62 [640/900 (70%)]\tLoss: 0.006159\n",
      "Training epoch: 62 [800/900 (88%)]\tLoss: 0.001358\n",
      "=========> Epoch: 62 Average loss: 0.0037\n",
      "Correlation coefficient: 0.7016\n",
      "Training epoch: 63 [0/900 (0%)]\tLoss: 0.001881\n",
      "Training epoch: 63 [160/900 (18%)]\tLoss: 0.003145\n",
      "Training epoch: 63 [320/900 (35%)]\tLoss: 0.001778\n",
      "Training epoch: 63 [480/900 (53%)]\tLoss: 0.004368\n",
      "Training epoch: 63 [640/900 (70%)]\tLoss: 0.001153\n",
      "Training epoch: 63 [800/900 (88%)]\tLoss: 0.002360\n",
      "=========> Epoch: 63 Average loss: 0.0030\n",
      "Correlation coefficient: 0.6944\n",
      "Training epoch: 64 [0/900 (0%)]\tLoss: 0.001257\n",
      "Training epoch: 64 [160/900 (18%)]\tLoss: 0.002251\n",
      "Training epoch: 64 [320/900 (35%)]\tLoss: 0.001647\n",
      "Training epoch: 64 [480/900 (53%)]\tLoss: 0.003114\n",
      "Training epoch: 64 [640/900 (70%)]\tLoss: 0.003442\n",
      "Training epoch: 64 [800/900 (88%)]\tLoss: 0.003010\n",
      "=========> Epoch: 64 Average loss: 0.0023\n",
      "Correlation coefficient: 0.6963\n",
      "Training epoch: 65 [0/900 (0%)]\tLoss: 0.001160\n",
      "Training epoch: 65 [160/900 (18%)]\tLoss: 0.001335\n",
      "Training epoch: 65 [320/900 (35%)]\tLoss: 0.002013\n",
      "Training epoch: 65 [480/900 (53%)]\tLoss: 0.001869\n",
      "Training epoch: 65 [640/900 (70%)]\tLoss: 0.001033\n",
      "Training epoch: 65 [800/900 (88%)]\tLoss: 0.001952\n",
      "=========> Epoch: 65 Average loss: 0.0022\n",
      "Correlation coefficient: 0.6984\n",
      "⏹️  Epoch 65 early stopping (no improvement for 50 epochs)\n",
      "🏁 Fold 6 best correlation: 0.7069\n",
      "\n",
      "========== Cross-validation Fold 7/10 ==========\n",
      "🔄 Fold 7: Using random initialization\n",
      "Training epoch: 1 [0/900 (0%)]\tLoss: 0.981207\n",
      "Training epoch: 1 [160/900 (18%)]\tLoss: 1.586178\n",
      "Training epoch: 1 [320/900 (35%)]\tLoss: 0.564306\n",
      "Training epoch: 1 [480/900 (53%)]\tLoss: 0.316699\n",
      "Training epoch: 1 [640/900 (70%)]\tLoss: 1.003811\n",
      "Training epoch: 1 [800/900 (88%)]\tLoss: 0.606207\n",
      "=========> Epoch: 1 Average loss: 0.8388\n",
      "Correlation coefficient: 0.6031\n",
      "✅ Epoch 1: New best correlation = 0.6031\n",
      "Training epoch: 2 [0/900 (0%)]\tLoss: 0.702186\n",
      "Training epoch: 2 [160/900 (18%)]\tLoss: 0.513401\n",
      "Training epoch: 2 [320/900 (35%)]\tLoss: 0.327338\n",
      "Training epoch: 2 [480/900 (53%)]\tLoss: 0.648857\n",
      "Training epoch: 2 [640/900 (70%)]\tLoss: 0.246811\n",
      "Training epoch: 2 [800/900 (88%)]\tLoss: 0.218344\n",
      "=========> Epoch: 2 Average loss: 0.4024\n",
      "Correlation coefficient: 0.5785\n",
      "Training epoch: 3 [0/900 (0%)]\tLoss: 0.149721\n",
      "Training epoch: 3 [160/900 (18%)]\tLoss: 0.172054\n",
      "Training epoch: 3 [320/900 (35%)]\tLoss: 0.363132\n",
      "Training epoch: 3 [480/900 (53%)]\tLoss: 0.174194\n",
      "Training epoch: 3 [640/900 (70%)]\tLoss: 0.236984\n",
      "Training epoch: 3 [800/900 (88%)]\tLoss: 0.120150\n",
      "=========> Epoch: 3 Average loss: 0.1512\n",
      "Correlation coefficient: 0.6223\n",
      "✅ Epoch 3: New best correlation = 0.6223\n",
      "Training epoch: 4 [0/900 (0%)]\tLoss: 0.104767\n",
      "Training epoch: 4 [160/900 (18%)]\tLoss: 0.087474\n",
      "Training epoch: 4 [320/900 (35%)]\tLoss: 0.037925\n",
      "Training epoch: 4 [480/900 (53%)]\tLoss: 0.065393\n",
      "Training epoch: 4 [640/900 (70%)]\tLoss: 0.047368\n",
      "Training epoch: 4 [800/900 (88%)]\tLoss: 0.033400\n",
      "=========> Epoch: 4 Average loss: 0.0647\n",
      "Correlation coefficient: 0.6180\n",
      "Training epoch: 5 [0/900 (0%)]\tLoss: 0.044898\n",
      "Training epoch: 5 [160/900 (18%)]\tLoss: 0.078416\n",
      "Training epoch: 5 [320/900 (35%)]\tLoss: 0.018624\n",
      "Training epoch: 5 [480/900 (53%)]\tLoss: 0.008404\n",
      "Training epoch: 5 [640/900 (70%)]\tLoss: 0.044092\n",
      "Training epoch: 5 [800/900 (88%)]\tLoss: 0.022866\n",
      "=========> Epoch: 5 Average loss: 0.0308\n",
      "Correlation coefficient: 0.5985\n",
      "Training epoch: 6 [0/900 (0%)]\tLoss: 0.017417\n",
      "Training epoch: 6 [160/900 (18%)]\tLoss: 0.011214\n",
      "Training epoch: 6 [320/900 (35%)]\tLoss: 0.012404\n",
      "Training epoch: 6 [480/900 (53%)]\tLoss: 0.018771\n",
      "Training epoch: 6 [640/900 (70%)]\tLoss: 0.018962\n",
      "Training epoch: 6 [800/900 (88%)]\tLoss: 0.019756\n",
      "=========> Epoch: 6 Average loss: 0.0189\n",
      "Correlation coefficient: 0.6110\n",
      "Training epoch: 7 [0/900 (0%)]\tLoss: 0.022910\n",
      "Training epoch: 7 [160/900 (18%)]\tLoss: 0.015164\n",
      "Training epoch: 7 [320/900 (35%)]\tLoss: 0.018042\n",
      "Training epoch: 7 [480/900 (53%)]\tLoss: 0.017386\n",
      "Training epoch: 7 [640/900 (70%)]\tLoss: 0.005067\n",
      "Training epoch: 7 [800/900 (88%)]\tLoss: 0.017373\n",
      "=========> Epoch: 7 Average loss: 0.0155\n",
      "Correlation coefficient: 0.6099\n",
      "Training epoch: 8 [0/900 (0%)]\tLoss: 0.005001\n",
      "Training epoch: 8 [160/900 (18%)]\tLoss: 0.009724\n",
      "Training epoch: 8 [320/900 (35%)]\tLoss: 0.012210\n",
      "Training epoch: 8 [480/900 (53%)]\tLoss: 0.005074\n",
      "Training epoch: 8 [640/900 (70%)]\tLoss: 0.004062\n",
      "Training epoch: 8 [800/900 (88%)]\tLoss: 0.005559\n",
      "=========> Epoch: 8 Average loss: 0.0111\n",
      "Correlation coefficient: 0.6209\n",
      "Training epoch: 9 [0/900 (0%)]\tLoss: 0.014667\n",
      "Training epoch: 9 [160/900 (18%)]\tLoss: 0.008788\n",
      "Training epoch: 9 [320/900 (35%)]\tLoss: 0.009830\n",
      "Training epoch: 9 [480/900 (53%)]\tLoss: 0.002913\n",
      "Training epoch: 9 [640/900 (70%)]\tLoss: 0.006375\n",
      "Training epoch: 9 [800/900 (88%)]\tLoss: 0.007504\n",
      "=========> Epoch: 9 Average loss: 0.0100\n",
      "Correlation coefficient: 0.6126\n",
      "Training epoch: 10 [0/900 (0%)]\tLoss: 0.004124\n",
      "Training epoch: 10 [160/900 (18%)]\tLoss: 0.007656\n",
      "Training epoch: 10 [320/900 (35%)]\tLoss: 0.004629\n",
      "Training epoch: 10 [480/900 (53%)]\tLoss: 0.010072\n",
      "Training epoch: 10 [640/900 (70%)]\tLoss: 0.006255\n",
      "Training epoch: 10 [800/900 (88%)]\tLoss: 0.008447\n",
      "=========> Epoch: 10 Average loss: 0.0101\n",
      "Correlation coefficient: 0.6184\n",
      "Training epoch: 11 [0/900 (0%)]\tLoss: 0.002245\n",
      "Training epoch: 11 [160/900 (18%)]\tLoss: 0.009121\n",
      "Training epoch: 11 [320/900 (35%)]\tLoss: 0.003450\n",
      "Training epoch: 11 [480/900 (53%)]\tLoss: 0.007343\n",
      "Training epoch: 11 [640/900 (70%)]\tLoss: 0.005045\n",
      "Training epoch: 11 [800/900 (88%)]\tLoss: 0.026709\n",
      "=========> Epoch: 11 Average loss: 0.0090\n",
      "Correlation coefficient: 0.6175\n",
      "Training epoch: 12 [0/900 (0%)]\tLoss: 0.002505\n",
      "Training epoch: 12 [160/900 (18%)]\tLoss: 0.009129\n",
      "Training epoch: 12 [320/900 (35%)]\tLoss: 0.007131\n",
      "Training epoch: 12 [480/900 (53%)]\tLoss: 0.008528\n",
      "Training epoch: 12 [640/900 (70%)]\tLoss: 0.004323\n",
      "Training epoch: 12 [800/900 (88%)]\tLoss: 0.004731\n",
      "=========> Epoch: 12 Average loss: 0.0072\n",
      "Correlation coefficient: 0.6113\n",
      "Training epoch: 13 [0/900 (0%)]\tLoss: 0.004671\n",
      "Training epoch: 13 [160/900 (18%)]\tLoss: 0.008736\n",
      "Training epoch: 13 [320/900 (35%)]\tLoss: 0.005589\n",
      "Training epoch: 13 [480/900 (53%)]\tLoss: 0.010605\n",
      "Training epoch: 13 [640/900 (70%)]\tLoss: 0.011165\n",
      "Training epoch: 13 [800/900 (88%)]\tLoss: 0.006746\n",
      "=========> Epoch: 13 Average loss: 0.0093\n",
      "Correlation coefficient: 0.6162\n",
      "Training epoch: 14 [0/900 (0%)]\tLoss: 0.011964\n",
      "Training epoch: 14 [160/900 (18%)]\tLoss: 0.006381\n",
      "Training epoch: 14 [320/900 (35%)]\tLoss: 0.005150\n",
      "Training epoch: 14 [480/900 (53%)]\tLoss: 0.003092\n",
      "Training epoch: 14 [640/900 (70%)]\tLoss: 0.010230\n",
      "Training epoch: 14 [800/900 (88%)]\tLoss: 0.013956\n",
      "=========> Epoch: 14 Average loss: 0.0116\n",
      "Correlation coefficient: 0.6116\n",
      "Training epoch: 15 [0/900 (0%)]\tLoss: 0.015589\n",
      "Training epoch: 15 [160/900 (18%)]\tLoss: 0.006956\n",
      "Training epoch: 15 [320/900 (35%)]\tLoss: 0.007822\n",
      "Training epoch: 15 [480/900 (53%)]\tLoss: 0.003792\n",
      "Training epoch: 15 [640/900 (70%)]\tLoss: 0.004824\n",
      "Training epoch: 15 [800/900 (88%)]\tLoss: 0.006271\n",
      "=========> Epoch: 15 Average loss: 0.0118\n",
      "Correlation coefficient: 0.6151\n",
      "Training epoch: 16 [0/900 (0%)]\tLoss: 0.009680\n",
      "Training epoch: 16 [160/900 (18%)]\tLoss: 0.014868\n",
      "Training epoch: 16 [320/900 (35%)]\tLoss: 0.013136\n",
      "Training epoch: 16 [480/900 (53%)]\tLoss: 0.010191\n",
      "Training epoch: 16 [640/900 (70%)]\tLoss: 0.005972\n",
      "Training epoch: 16 [800/900 (88%)]\tLoss: 0.016884\n",
      "=========> Epoch: 16 Average loss: 0.0124\n",
      "Correlation coefficient: 0.6179\n",
      "Training epoch: 17 [0/900 (0%)]\tLoss: 0.021053\n",
      "Training epoch: 17 [160/900 (18%)]\tLoss: 0.011991\n",
      "Training epoch: 17 [320/900 (35%)]\tLoss: 0.009962\n",
      "Training epoch: 17 [480/900 (53%)]\tLoss: 0.006937\n",
      "Training epoch: 17 [640/900 (70%)]\tLoss: 0.015144\n",
      "Training epoch: 17 [800/900 (88%)]\tLoss: 0.021544\n",
      "=========> Epoch: 17 Average loss: 0.0116\n",
      "Correlation coefficient: 0.6240\n",
      "✅ Epoch 17: New best correlation = 0.6240\n",
      "Training epoch: 18 [0/900 (0%)]\tLoss: 0.002238\n",
      "Training epoch: 18 [160/900 (18%)]\tLoss: 0.012078\n",
      "Training epoch: 18 [320/900 (35%)]\tLoss: 0.012583\n",
      "Training epoch: 18 [480/900 (53%)]\tLoss: 0.017629\n",
      "Training epoch: 18 [640/900 (70%)]\tLoss: 0.005693\n",
      "Training epoch: 18 [800/900 (88%)]\tLoss: 0.008507\n",
      "=========> Epoch: 18 Average loss: 0.0124\n",
      "Correlation coefficient: 0.6081\n",
      "Training epoch: 19 [0/900 (0%)]\tLoss: 0.011628\n",
      "Training epoch: 19 [160/900 (18%)]\tLoss: 0.035272\n",
      "Training epoch: 19 [320/900 (35%)]\tLoss: 0.020048\n",
      "Training epoch: 19 [480/900 (53%)]\tLoss: 0.018481\n",
      "Training epoch: 19 [640/900 (70%)]\tLoss: 0.009786\n",
      "Training epoch: 19 [800/900 (88%)]\tLoss: 0.056921\n",
      "=========> Epoch: 19 Average loss: 0.0138\n",
      "Correlation coefficient: 0.6281\n",
      "✅ Epoch 19: New best correlation = 0.6281\n",
      "Training epoch: 20 [0/900 (0%)]\tLoss: 0.003362\n",
      "Training epoch: 20 [160/900 (18%)]\tLoss: 0.006191\n",
      "Training epoch: 20 [320/900 (35%)]\tLoss: 0.019972\n",
      "Training epoch: 20 [480/900 (53%)]\tLoss: 0.012475\n",
      "Training epoch: 20 [640/900 (70%)]\tLoss: 0.009120\n",
      "Training epoch: 20 [800/900 (88%)]\tLoss: 0.004562\n",
      "=========> Epoch: 20 Average loss: 0.0125\n",
      "Correlation coefficient: 0.6112\n",
      "Training epoch: 21 [0/900 (0%)]\tLoss: 0.019250\n",
      "Training epoch: 21 [160/900 (18%)]\tLoss: 0.015211\n",
      "Training epoch: 21 [320/900 (35%)]\tLoss: 0.081199\n",
      "Training epoch: 21 [480/900 (53%)]\tLoss: 0.007163\n",
      "Training epoch: 21 [640/900 (70%)]\tLoss: 0.013507\n",
      "Training epoch: 21 [800/900 (88%)]\tLoss: 0.005193\n",
      "=========> Epoch: 21 Average loss: 0.0137\n",
      "Correlation coefficient: 0.6118\n",
      "Training epoch: 22 [0/900 (0%)]\tLoss: 0.006601\n",
      "Training epoch: 22 [160/900 (18%)]\tLoss: 0.007879\n",
      "Training epoch: 22 [320/900 (35%)]\tLoss: 0.010627\n",
      "Training epoch: 22 [480/900 (53%)]\tLoss: 0.010423\n",
      "Training epoch: 22 [640/900 (70%)]\tLoss: 0.027811\n",
      "Training epoch: 22 [800/900 (88%)]\tLoss: 0.010862\n",
      "=========> Epoch: 22 Average loss: 0.0140\n",
      "Correlation coefficient: 0.6202\n",
      "Training epoch: 23 [0/900 (0%)]\tLoss: 0.008613\n",
      "Training epoch: 23 [160/900 (18%)]\tLoss: 0.005961\n",
      "Training epoch: 23 [320/900 (35%)]\tLoss: 0.008943\n",
      "Training epoch: 23 [480/900 (53%)]\tLoss: 0.003520\n",
      "Training epoch: 23 [640/900 (70%)]\tLoss: 0.019090\n",
      "Training epoch: 23 [800/900 (88%)]\tLoss: 0.008917\n",
      "=========> Epoch: 23 Average loss: 0.0118\n",
      "Correlation coefficient: 0.6295\n",
      "✅ Epoch 23: New best correlation = 0.6295\n",
      "Training epoch: 24 [0/900 (0%)]\tLoss: 0.007175\n",
      "Training epoch: 24 [160/900 (18%)]\tLoss: 0.012538\n",
      "Training epoch: 24 [320/900 (35%)]\tLoss: 0.004121\n",
      "Training epoch: 24 [480/900 (53%)]\tLoss: 0.013717\n",
      "Training epoch: 24 [640/900 (70%)]\tLoss: 0.006177\n",
      "Training epoch: 24 [800/900 (88%)]\tLoss: 0.008766\n",
      "=========> Epoch: 24 Average loss: 0.0124\n",
      "Correlation coefficient: 0.5976\n",
      "Training epoch: 25 [0/900 (0%)]\tLoss: 0.024958\n",
      "Training epoch: 25 [160/900 (18%)]\tLoss: 0.041289\n",
      "Training epoch: 25 [320/900 (35%)]\tLoss: 0.019162\n",
      "Training epoch: 25 [480/900 (53%)]\tLoss: 0.021824\n",
      "Training epoch: 25 [640/900 (70%)]\tLoss: 0.033095\n",
      "Training epoch: 25 [800/900 (88%)]\tLoss: 0.021171\n",
      "=========> Epoch: 25 Average loss: 0.0218\n",
      "Correlation coefficient: 0.6270\n",
      "Training epoch: 26 [0/900 (0%)]\tLoss: 0.013129\n",
      "Training epoch: 26 [160/900 (18%)]\tLoss: 0.021328\n",
      "Training epoch: 26 [320/900 (35%)]\tLoss: 0.063745\n",
      "Training epoch: 26 [480/900 (53%)]\tLoss: 0.013491\n",
      "Training epoch: 26 [640/900 (70%)]\tLoss: 0.079719\n",
      "Training epoch: 26 [800/900 (88%)]\tLoss: 0.024937\n",
      "=========> Epoch: 26 Average loss: 0.0325\n",
      "Correlation coefficient: 0.6140\n",
      "Training epoch: 27 [0/900 (0%)]\tLoss: 0.010990\n",
      "Training epoch: 27 [160/900 (18%)]\tLoss: 0.027663\n",
      "Training epoch: 27 [320/900 (35%)]\tLoss: 0.019732\n",
      "Training epoch: 27 [480/900 (53%)]\tLoss: 0.017838\n",
      "Training epoch: 27 [640/900 (70%)]\tLoss: 0.020364\n",
      "Training epoch: 27 [800/900 (88%)]\tLoss: 0.017699\n",
      "=========> Epoch: 27 Average loss: 0.0227\n",
      "Correlation coefficient: 0.6315\n",
      "✅ Epoch 27: New best correlation = 0.6315\n",
      "Training epoch: 28 [0/900 (0%)]\tLoss: 0.018328\n",
      "Training epoch: 28 [160/900 (18%)]\tLoss: 0.015288\n",
      "Training epoch: 28 [320/900 (35%)]\tLoss: 0.028874\n",
      "Training epoch: 28 [480/900 (53%)]\tLoss: 0.028435\n",
      "Training epoch: 28 [640/900 (70%)]\tLoss: 0.012526\n",
      "Training epoch: 28 [800/900 (88%)]\tLoss: 0.006015\n",
      "=========> Epoch: 28 Average loss: 0.0190\n",
      "Correlation coefficient: 0.6221\n",
      "Training epoch: 29 [0/900 (0%)]\tLoss: 0.008623\n",
      "Training epoch: 29 [160/900 (18%)]\tLoss: 0.009565\n",
      "Training epoch: 29 [320/900 (35%)]\tLoss: 0.012078\n",
      "Training epoch: 29 [480/900 (53%)]\tLoss: 0.014118\n",
      "Training epoch: 29 [640/900 (70%)]\tLoss: 0.008302\n",
      "Training epoch: 29 [800/900 (88%)]\tLoss: 0.022429\n",
      "=========> Epoch: 29 Average loss: 0.0144\n",
      "Correlation coefficient: 0.6265\n",
      "Training epoch: 30 [0/900 (0%)]\tLoss: 0.006338\n",
      "Training epoch: 30 [160/900 (18%)]\tLoss: 0.007653\n",
      "Training epoch: 30 [320/900 (35%)]\tLoss: 0.004519\n",
      "Training epoch: 30 [480/900 (53%)]\tLoss: 0.009174\n",
      "Training epoch: 30 [640/900 (70%)]\tLoss: 0.015006\n",
      "Training epoch: 30 [800/900 (88%)]\tLoss: 0.002885\n",
      "=========> Epoch: 30 Average loss: 0.0093\n",
      "Correlation coefficient: 0.6276\n",
      "Training epoch: 31 [0/900 (0%)]\tLoss: 0.003669\n",
      "Training epoch: 31 [160/900 (18%)]\tLoss: 0.018708\n",
      "Training epoch: 31 [320/900 (35%)]\tLoss: 0.004905\n",
      "Training epoch: 31 [480/900 (53%)]\tLoss: 0.004556\n",
      "Training epoch: 31 [640/900 (70%)]\tLoss: 0.007454\n",
      "Training epoch: 31 [800/900 (88%)]\tLoss: 0.001765\n",
      "=========> Epoch: 31 Average loss: 0.0063\n",
      "Correlation coefficient: 0.6182\n",
      "Training epoch: 32 [0/900 (0%)]\tLoss: 0.004335\n",
      "Training epoch: 32 [160/900 (18%)]\tLoss: 0.005256\n",
      "Training epoch: 32 [320/900 (35%)]\tLoss: 0.002239\n",
      "Training epoch: 32 [480/900 (53%)]\tLoss: 0.008294\n",
      "Training epoch: 32 [640/900 (70%)]\tLoss: 0.007632\n",
      "Training epoch: 32 [800/900 (88%)]\tLoss: 0.004894\n",
      "=========> Epoch: 32 Average loss: 0.0062\n",
      "Correlation coefficient: 0.6316\n",
      "✅ Epoch 32: New best correlation = 0.6316\n",
      "Training epoch: 33 [0/900 (0%)]\tLoss: 0.001163\n",
      "Training epoch: 33 [160/900 (18%)]\tLoss: 0.003067\n",
      "Training epoch: 33 [320/900 (35%)]\tLoss: 0.021144\n",
      "Training epoch: 33 [480/900 (53%)]\tLoss: 0.004405\n",
      "Training epoch: 33 [640/900 (70%)]\tLoss: 0.006277\n",
      "Training epoch: 33 [800/900 (88%)]\tLoss: 0.002930\n",
      "=========> Epoch: 33 Average loss: 0.0049\n",
      "Correlation coefficient: 0.6227\n",
      "Training epoch: 34 [0/900 (0%)]\tLoss: 0.004145\n",
      "Training epoch: 34 [160/900 (18%)]\tLoss: 0.007938\n",
      "Training epoch: 34 [320/900 (35%)]\tLoss: 0.005078\n",
      "Training epoch: 34 [480/900 (53%)]\tLoss: 0.005823\n",
      "Training epoch: 34 [640/900 (70%)]\tLoss: 0.000807\n",
      "Training epoch: 34 [800/900 (88%)]\tLoss: 0.005688\n",
      "=========> Epoch: 34 Average loss: 0.0039\n",
      "Correlation coefficient: 0.6350\n",
      "✅ Epoch 34: New best correlation = 0.6350\n",
      "Training epoch: 35 [0/900 (0%)]\tLoss: 0.001611\n",
      "Training epoch: 35 [160/900 (18%)]\tLoss: 0.002061\n",
      "Training epoch: 35 [320/900 (35%)]\tLoss: 0.003951\n",
      "Training epoch: 35 [480/900 (53%)]\tLoss: 0.004265\n",
      "Training epoch: 35 [640/900 (70%)]\tLoss: 0.002112\n",
      "Training epoch: 35 [800/900 (88%)]\tLoss: 0.001561\n",
      "=========> Epoch: 35 Average loss: 0.0034\n",
      "Correlation coefficient: 0.6223\n",
      "Training epoch: 36 [0/900 (0%)]\tLoss: 0.005301\n",
      "Training epoch: 36 [160/900 (18%)]\tLoss: 0.003551\n",
      "Training epoch: 36 [320/900 (35%)]\tLoss: 0.001970\n",
      "Training epoch: 36 [480/900 (53%)]\tLoss: 0.001987\n",
      "Training epoch: 36 [640/900 (70%)]\tLoss: 0.001310\n",
      "Training epoch: 36 [800/900 (88%)]\tLoss: 0.004406\n",
      "=========> Epoch: 36 Average loss: 0.0031\n",
      "Correlation coefficient: 0.6267\n",
      "Training epoch: 37 [0/900 (0%)]\tLoss: 0.002058\n",
      "Training epoch: 37 [160/900 (18%)]\tLoss: 0.004152\n",
      "Training epoch: 37 [320/900 (35%)]\tLoss: 0.001779\n",
      "Training epoch: 37 [480/900 (53%)]\tLoss: 0.003103\n",
      "Training epoch: 37 [640/900 (70%)]\tLoss: 0.002499\n",
      "Training epoch: 37 [800/900 (88%)]\tLoss: 0.005340\n",
      "=========> Epoch: 37 Average loss: 0.0033\n",
      "Correlation coefficient: 0.6258\n",
      "Training epoch: 38 [0/900 (0%)]\tLoss: 0.002609\n",
      "Training epoch: 38 [160/900 (18%)]\tLoss: 0.002793\n",
      "Training epoch: 38 [320/900 (35%)]\tLoss: 0.002693\n",
      "Training epoch: 38 [480/900 (53%)]\tLoss: 0.005630\n",
      "Training epoch: 38 [640/900 (70%)]\tLoss: 0.004369\n",
      "Training epoch: 38 [800/900 (88%)]\tLoss: 0.003848\n",
      "=========> Epoch: 38 Average loss: 0.0048\n",
      "Correlation coefficient: 0.6223\n",
      "Training epoch: 39 [0/900 (0%)]\tLoss: 0.006799\n",
      "Training epoch: 39 [160/900 (18%)]\tLoss: 0.004886\n",
      "Training epoch: 39 [320/900 (35%)]\tLoss: 0.011575\n",
      "Training epoch: 39 [480/900 (53%)]\tLoss: 0.006607\n",
      "Training epoch: 39 [640/900 (70%)]\tLoss: 0.004091\n",
      "Training epoch: 39 [800/900 (88%)]\tLoss: 0.006860\n",
      "=========> Epoch: 39 Average loss: 0.0061\n",
      "Correlation coefficient: 0.6292\n",
      "Training epoch: 40 [0/900 (0%)]\tLoss: 0.002779\n",
      "Training epoch: 40 [160/900 (18%)]\tLoss: 0.008717\n",
      "Training epoch: 40 [320/900 (35%)]\tLoss: 0.013023\n",
      "Training epoch: 40 [480/900 (53%)]\tLoss: 0.004935\n",
      "Training epoch: 40 [640/900 (70%)]\tLoss: 0.021193\n",
      "Training epoch: 40 [800/900 (88%)]\tLoss: 0.002167\n",
      "=========> Epoch: 40 Average loss: 0.0120\n",
      "Correlation coefficient: 0.6216\n",
      "Training epoch: 41 [0/900 (0%)]\tLoss: 0.003985\n",
      "Training epoch: 41 [160/900 (18%)]\tLoss: 0.034195\n",
      "Training epoch: 41 [320/900 (35%)]\tLoss: 0.047722\n",
      "Training epoch: 41 [480/900 (53%)]\tLoss: 0.019501\n",
      "Training epoch: 41 [640/900 (70%)]\tLoss: 0.008889\n",
      "Training epoch: 41 [800/900 (88%)]\tLoss: 0.013698\n",
      "=========> Epoch: 41 Average loss: 0.0184\n",
      "Correlation coefficient: 0.6344\n",
      "Training epoch: 42 [0/900 (0%)]\tLoss: 0.013518\n",
      "Training epoch: 42 [160/900 (18%)]\tLoss: 0.035521\n",
      "Training epoch: 42 [320/900 (35%)]\tLoss: 0.052020\n",
      "Training epoch: 42 [480/900 (53%)]\tLoss: 0.012148\n",
      "Training epoch: 42 [640/900 (70%)]\tLoss: 0.014945\n",
      "Training epoch: 42 [800/900 (88%)]\tLoss: 0.021415\n",
      "=========> Epoch: 42 Average loss: 0.0200\n",
      "Correlation coefficient: 0.6166\n",
      "Training epoch: 43 [0/900 (0%)]\tLoss: 0.017142\n",
      "Training epoch: 43 [160/900 (18%)]\tLoss: 0.019474\n",
      "Training epoch: 43 [320/900 (35%)]\tLoss: 0.019812\n",
      "Training epoch: 43 [480/900 (53%)]\tLoss: 0.032809\n",
      "Training epoch: 43 [640/900 (70%)]\tLoss: 0.010807\n",
      "Training epoch: 43 [800/900 (88%)]\tLoss: 0.013086\n",
      "=========> Epoch: 43 Average loss: 0.0213\n",
      "Correlation coefficient: 0.6240\n",
      "Training epoch: 44 [0/900 (0%)]\tLoss: 0.034875\n",
      "Training epoch: 44 [160/900 (18%)]\tLoss: 0.015162\n",
      "Training epoch: 44 [320/900 (35%)]\tLoss: 0.017544\n",
      "Training epoch: 44 [480/900 (53%)]\tLoss: 0.024270\n",
      "Training epoch: 44 [640/900 (70%)]\tLoss: 0.039022\n",
      "Training epoch: 44 [800/900 (88%)]\tLoss: 0.019407\n",
      "=========> Epoch: 44 Average loss: 0.0197\n",
      "Correlation coefficient: 0.6192\n",
      "Training epoch: 45 [0/900 (0%)]\tLoss: 0.006097\n",
      "Training epoch: 45 [160/900 (18%)]\tLoss: 0.010875\n",
      "Training epoch: 45 [320/900 (35%)]\tLoss: 0.014812\n",
      "Training epoch: 45 [480/900 (53%)]\tLoss: 0.011983\n",
      "Training epoch: 45 [640/900 (70%)]\tLoss: 0.010741\n",
      "Training epoch: 45 [800/900 (88%)]\tLoss: 0.020254\n",
      "=========> Epoch: 45 Average loss: 0.0207\n",
      "Correlation coefficient: 0.6262\n",
      "Training epoch: 46 [0/900 (0%)]\tLoss: 0.033149\n",
      "Training epoch: 46 [160/900 (18%)]\tLoss: 0.015624\n",
      "Training epoch: 46 [320/900 (35%)]\tLoss: 0.016802\n",
      "Training epoch: 46 [480/900 (53%)]\tLoss: 0.032073\n",
      "Training epoch: 46 [640/900 (70%)]\tLoss: 0.007230\n",
      "Training epoch: 46 [800/900 (88%)]\tLoss: 0.007646\n",
      "=========> Epoch: 46 Average loss: 0.0213\n",
      "Correlation coefficient: 0.6101\n",
      "Training epoch: 47 [0/900 (0%)]\tLoss: 0.018012\n",
      "Training epoch: 47 [160/900 (18%)]\tLoss: 0.013534\n",
      "Training epoch: 47 [320/900 (35%)]\tLoss: 0.010304\n",
      "Training epoch: 47 [480/900 (53%)]\tLoss: 0.015221\n",
      "Training epoch: 47 [640/900 (70%)]\tLoss: 0.033641\n",
      "Training epoch: 47 [800/900 (88%)]\tLoss: 0.017606\n",
      "=========> Epoch: 47 Average loss: 0.0149\n",
      "Correlation coefficient: 0.6156\n",
      "Training epoch: 48 [0/900 (0%)]\tLoss: 0.011404\n",
      "Training epoch: 48 [160/900 (18%)]\tLoss: 0.005414\n",
      "Training epoch: 48 [320/900 (35%)]\tLoss: 0.016027\n",
      "Training epoch: 48 [480/900 (53%)]\tLoss: 0.007201\n",
      "Training epoch: 48 [640/900 (70%)]\tLoss: 0.010024\n",
      "Training epoch: 48 [800/900 (88%)]\tLoss: 0.009124\n",
      "=========> Epoch: 48 Average loss: 0.0109\n",
      "Correlation coefficient: 0.6296\n",
      "Training epoch: 49 [0/900 (0%)]\tLoss: 0.004267\n",
      "Training epoch: 49 [160/900 (18%)]\tLoss: 0.005026\n",
      "Training epoch: 49 [320/900 (35%)]\tLoss: 0.005810\n",
      "Training epoch: 49 [480/900 (53%)]\tLoss: 0.004069\n",
      "Training epoch: 49 [640/900 (70%)]\tLoss: 0.008773\n",
      "Training epoch: 49 [800/900 (88%)]\tLoss: 0.005754\n",
      "=========> Epoch: 49 Average loss: 0.0071\n",
      "Correlation coefficient: 0.6189\n",
      "Training epoch: 50 [0/900 (0%)]\tLoss: 0.005603\n",
      "Training epoch: 50 [160/900 (18%)]\tLoss: 0.000979\n",
      "Training epoch: 50 [320/900 (35%)]\tLoss: 0.002546\n",
      "Training epoch: 50 [480/900 (53%)]\tLoss: 0.001203\n",
      "Training epoch: 50 [640/900 (70%)]\tLoss: 0.008596\n",
      "Training epoch: 50 [800/900 (88%)]\tLoss: 0.003979\n",
      "=========> Epoch: 50 Average loss: 0.0082\n",
      "Correlation coefficient: 0.6273\n",
      "Training epoch: 51 [0/900 (0%)]\tLoss: 0.002387\n",
      "Training epoch: 51 [160/900 (18%)]\tLoss: 0.003340\n",
      "Training epoch: 51 [320/900 (35%)]\tLoss: 0.005064\n",
      "Training epoch: 51 [480/900 (53%)]\tLoss: 0.007192\n",
      "Training epoch: 51 [640/900 (70%)]\tLoss: 0.005997\n",
      "Training epoch: 51 [800/900 (88%)]\tLoss: 0.005150\n",
      "=========> Epoch: 51 Average loss: 0.0069\n",
      "Correlation coefficient: 0.6319\n",
      "Training epoch: 52 [0/900 (0%)]\tLoss: 0.004512\n",
      "Training epoch: 52 [160/900 (18%)]\tLoss: 0.006637\n",
      "Training epoch: 52 [320/900 (35%)]\tLoss: 0.006413\n",
      "Training epoch: 52 [480/900 (53%)]\tLoss: 0.010185\n",
      "Training epoch: 52 [640/900 (70%)]\tLoss: 0.007203\n",
      "Training epoch: 52 [800/900 (88%)]\tLoss: 0.016441\n",
      "=========> Epoch: 52 Average loss: 0.0082\n",
      "Correlation coefficient: 0.6251\n",
      "Training epoch: 53 [0/900 (0%)]\tLoss: 0.009317\n",
      "Training epoch: 53 [160/900 (18%)]\tLoss: 0.007361\n",
      "Training epoch: 53 [320/900 (35%)]\tLoss: 0.002374\n",
      "Training epoch: 53 [480/900 (53%)]\tLoss: 0.003005\n",
      "Training epoch: 53 [640/900 (70%)]\tLoss: 0.004892\n",
      "Training epoch: 53 [800/900 (88%)]\tLoss: 0.007039\n",
      "=========> Epoch: 53 Average loss: 0.0087\n",
      "Correlation coefficient: 0.6295\n",
      "Training epoch: 54 [0/900 (0%)]\tLoss: 0.010061\n",
      "Training epoch: 54 [160/900 (18%)]\tLoss: 0.002093\n",
      "Training epoch: 54 [320/900 (35%)]\tLoss: 0.006204\n",
      "Training epoch: 54 [480/900 (53%)]\tLoss: 0.013238\n",
      "Training epoch: 54 [640/900 (70%)]\tLoss: 0.011663\n",
      "Training epoch: 54 [800/900 (88%)]\tLoss: 0.003697\n",
      "=========> Epoch: 54 Average loss: 0.0068\n",
      "Correlation coefficient: 0.6308\n",
      "Training epoch: 55 [0/900 (0%)]\tLoss: 0.005042\n",
      "Training epoch: 55 [160/900 (18%)]\tLoss: 0.001960\n",
      "Training epoch: 55 [320/900 (35%)]\tLoss: 0.010289\n",
      "Training epoch: 55 [480/900 (53%)]\tLoss: 0.012379\n",
      "Training epoch: 55 [640/900 (70%)]\tLoss: 0.005493\n",
      "Training epoch: 55 [800/900 (88%)]\tLoss: 0.007571\n",
      "=========> Epoch: 55 Average loss: 0.0059\n",
      "Correlation coefficient: 0.6268\n",
      "Training epoch: 56 [0/900 (0%)]\tLoss: 0.002029\n",
      "Training epoch: 56 [160/900 (18%)]\tLoss: 0.003159\n",
      "Training epoch: 56 [320/900 (35%)]\tLoss: 0.003548\n",
      "Training epoch: 56 [480/900 (53%)]\tLoss: 0.003451\n",
      "Training epoch: 56 [640/900 (70%)]\tLoss: 0.002758\n",
      "Training epoch: 56 [800/900 (88%)]\tLoss: 0.009411\n",
      "=========> Epoch: 56 Average loss: 0.0055\n",
      "Correlation coefficient: 0.6294\n",
      "Training epoch: 57 [0/900 (0%)]\tLoss: 0.003542\n",
      "Training epoch: 57 [160/900 (18%)]\tLoss: 0.003719\n",
      "Training epoch: 57 [320/900 (35%)]\tLoss: 0.007645\n",
      "Training epoch: 57 [480/900 (53%)]\tLoss: 0.004175\n",
      "Training epoch: 57 [640/900 (70%)]\tLoss: 0.003451\n",
      "Training epoch: 57 [800/900 (88%)]\tLoss: 0.006590\n",
      "=========> Epoch: 57 Average loss: 0.0054\n",
      "Correlation coefficient: 0.6324\n",
      "Training epoch: 58 [0/900 (0%)]\tLoss: 0.004042\n",
      "Training epoch: 58 [160/900 (18%)]\tLoss: 0.002062\n",
      "Training epoch: 58 [320/900 (35%)]\tLoss: 0.005536\n",
      "Training epoch: 58 [480/900 (53%)]\tLoss: 0.007812\n",
      "Training epoch: 58 [640/900 (70%)]\tLoss: 0.004741\n",
      "Training epoch: 58 [800/900 (88%)]\tLoss: 0.005846\n",
      "=========> Epoch: 58 Average loss: 0.0059\n",
      "Correlation coefficient: 0.6288\n",
      "Training epoch: 59 [0/900 (0%)]\tLoss: 0.005116\n",
      "Training epoch: 59 [160/900 (18%)]\tLoss: 0.007884\n",
      "Training epoch: 59 [320/900 (35%)]\tLoss: 0.009874\n",
      "Training epoch: 59 [480/900 (53%)]\tLoss: 0.008299\n",
      "Training epoch: 59 [640/900 (70%)]\tLoss: 0.009076\n",
      "Training epoch: 59 [800/900 (88%)]\tLoss: 0.002639\n",
      "=========> Epoch: 59 Average loss: 0.0069\n",
      "Correlation coefficient: 0.6276\n",
      "Training epoch: 60 [0/900 (0%)]\tLoss: 0.003392\n",
      "Training epoch: 60 [160/900 (18%)]\tLoss: 0.004923\n",
      "Training epoch: 60 [320/900 (35%)]\tLoss: 0.006252\n",
      "Training epoch: 60 [480/900 (53%)]\tLoss: 0.005792\n",
      "Training epoch: 60 [640/900 (70%)]\tLoss: 0.004082\n",
      "Training epoch: 60 [800/900 (88%)]\tLoss: 0.003406\n",
      "=========> Epoch: 60 Average loss: 0.0084\n",
      "Correlation coefficient: 0.6295\n",
      "Training epoch: 61 [0/900 (0%)]\tLoss: 0.009950\n",
      "Training epoch: 61 [160/900 (18%)]\tLoss: 0.003995\n",
      "Training epoch: 61 [320/900 (35%)]\tLoss: 0.006029\n",
      "Training epoch: 61 [480/900 (53%)]\tLoss: 0.014702\n",
      "Training epoch: 61 [640/900 (70%)]\tLoss: 0.009082\n",
      "Training epoch: 61 [800/900 (88%)]\tLoss: 0.008309\n",
      "=========> Epoch: 61 Average loss: 0.0079\n",
      "Correlation coefficient: 0.6312\n",
      "Training epoch: 62 [0/900 (0%)]\tLoss: 0.012250\n",
      "Training epoch: 62 [160/900 (18%)]\tLoss: 0.003533\n",
      "Training epoch: 62 [320/900 (35%)]\tLoss: 0.006081\n",
      "Training epoch: 62 [480/900 (53%)]\tLoss: 0.008585\n",
      "Training epoch: 62 [640/900 (70%)]\tLoss: 0.018897\n",
      "Training epoch: 62 [800/900 (88%)]\tLoss: 0.017099\n",
      "=========> Epoch: 62 Average loss: 0.0081\n",
      "Correlation coefficient: 0.6237\n",
      "Training epoch: 63 [0/900 (0%)]\tLoss: 0.011607\n",
      "Training epoch: 63 [160/900 (18%)]\tLoss: 0.007375\n",
      "Training epoch: 63 [320/900 (35%)]\tLoss: 0.012526\n",
      "Training epoch: 63 [480/900 (53%)]\tLoss: 0.026127\n",
      "Training epoch: 63 [640/900 (70%)]\tLoss: 0.009879\n",
      "Training epoch: 63 [800/900 (88%)]\tLoss: 0.019290\n",
      "=========> Epoch: 63 Average loss: 0.0111\n",
      "Correlation coefficient: 0.6268\n",
      "Training epoch: 64 [0/900 (0%)]\tLoss: 0.011613\n",
      "Training epoch: 64 [160/900 (18%)]\tLoss: 0.011960\n",
      "Training epoch: 64 [320/900 (35%)]\tLoss: 0.007625\n",
      "Training epoch: 64 [480/900 (53%)]\tLoss: 0.003246\n",
      "Training epoch: 64 [640/900 (70%)]\tLoss: 0.005878\n",
      "Training epoch: 64 [800/900 (88%)]\tLoss: 0.013981\n",
      "=========> Epoch: 64 Average loss: 0.0122\n",
      "Correlation coefficient: 0.6412\n",
      "✅ Epoch 64: New best correlation = 0.6412\n",
      "Training epoch: 65 [0/900 (0%)]\tLoss: 0.024319\n",
      "Training epoch: 65 [160/900 (18%)]\tLoss: 0.014809\n",
      "Training epoch: 65 [320/900 (35%)]\tLoss: 0.006097\n",
      "Training epoch: 65 [480/900 (53%)]\tLoss: 0.019558\n",
      "Training epoch: 65 [640/900 (70%)]\tLoss: 0.019802\n",
      "Training epoch: 65 [800/900 (88%)]\tLoss: 0.010621\n",
      "=========> Epoch: 65 Average loss: 0.0109\n",
      "Correlation coefficient: 0.6307\n",
      "Training epoch: 66 [0/900 (0%)]\tLoss: 0.002321\n",
      "Training epoch: 66 [160/900 (18%)]\tLoss: 0.005590\n",
      "Training epoch: 66 [320/900 (35%)]\tLoss: 0.015363\n",
      "Training epoch: 66 [480/900 (53%)]\tLoss: 0.011740\n",
      "Training epoch: 66 [640/900 (70%)]\tLoss: 0.012807\n",
      "Training epoch: 66 [800/900 (88%)]\tLoss: 0.013499\n",
      "=========> Epoch: 66 Average loss: 0.0125\n",
      "Correlation coefficient: 0.6302\n",
      "Training epoch: 67 [0/900 (0%)]\tLoss: 0.011520\n",
      "Training epoch: 67 [160/900 (18%)]\tLoss: 0.010960\n",
      "Training epoch: 67 [320/900 (35%)]\tLoss: 0.018036\n",
      "Training epoch: 67 [480/900 (53%)]\tLoss: 0.022060\n",
      "Training epoch: 67 [640/900 (70%)]\tLoss: 0.011260\n",
      "Training epoch: 67 [800/900 (88%)]\tLoss: 0.014420\n",
      "=========> Epoch: 67 Average loss: 0.0176\n",
      "Correlation coefficient: 0.6180\n",
      "Training epoch: 68 [0/900 (0%)]\tLoss: 0.012486\n",
      "Training epoch: 68 [160/900 (18%)]\tLoss: 0.025809\n",
      "Training epoch: 68 [320/900 (35%)]\tLoss: 0.004211\n",
      "Training epoch: 68 [480/900 (53%)]\tLoss: 0.012887\n",
      "Training epoch: 68 [640/900 (70%)]\tLoss: 0.026709\n",
      "Training epoch: 68 [800/900 (88%)]\tLoss: 0.008795\n",
      "=========> Epoch: 68 Average loss: 0.0162\n",
      "Correlation coefficient: 0.6344\n",
      "Training epoch: 69 [0/900 (0%)]\tLoss: 0.017067\n",
      "Training epoch: 69 [160/900 (18%)]\tLoss: 0.025534\n",
      "Training epoch: 69 [320/900 (35%)]\tLoss: 0.012646\n",
      "Training epoch: 69 [480/900 (53%)]\tLoss: 0.013061\n",
      "Training epoch: 69 [640/900 (70%)]\tLoss: 0.019019\n",
      "Training epoch: 69 [800/900 (88%)]\tLoss: 0.017503\n",
      "=========> Epoch: 69 Average loss: 0.0157\n",
      "Correlation coefficient: 0.6240\n",
      "Training epoch: 70 [0/900 (0%)]\tLoss: 0.026310\n",
      "Training epoch: 70 [160/900 (18%)]\tLoss: 0.008165\n",
      "Training epoch: 70 [320/900 (35%)]\tLoss: 0.021196\n",
      "Training epoch: 70 [480/900 (53%)]\tLoss: 0.015842\n",
      "Training epoch: 70 [640/900 (70%)]\tLoss: 0.010665\n",
      "Training epoch: 70 [800/900 (88%)]\tLoss: 0.011014\n",
      "=========> Epoch: 70 Average loss: 0.0141\n",
      "Correlation coefficient: 0.6365\n",
      "Training epoch: 71 [0/900 (0%)]\tLoss: 0.008281\n",
      "Training epoch: 71 [160/900 (18%)]\tLoss: 0.012439\n",
      "Training epoch: 71 [320/900 (35%)]\tLoss: 0.024445\n",
      "Training epoch: 71 [480/900 (53%)]\tLoss: 0.017920\n",
      "Training epoch: 71 [640/900 (70%)]\tLoss: 0.008698\n",
      "Training epoch: 71 [800/900 (88%)]\tLoss: 0.008265\n",
      "=========> Epoch: 71 Average loss: 0.0107\n",
      "Correlation coefficient: 0.6343\n",
      "Training epoch: 72 [0/900 (0%)]\tLoss: 0.004801\n",
      "Training epoch: 72 [160/900 (18%)]\tLoss: 0.008562\n",
      "Training epoch: 72 [320/900 (35%)]\tLoss: 0.005624\n",
      "Training epoch: 72 [480/900 (53%)]\tLoss: 0.005641\n",
      "Training epoch: 72 [640/900 (70%)]\tLoss: 0.012740\n",
      "Training epoch: 72 [800/900 (88%)]\tLoss: 0.008801\n",
      "=========> Epoch: 72 Average loss: 0.0078\n",
      "Correlation coefficient: 0.6343\n",
      "Training epoch: 73 [0/900 (0%)]\tLoss: 0.009273\n",
      "Training epoch: 73 [160/900 (18%)]\tLoss: 0.015156\n",
      "Training epoch: 73 [320/900 (35%)]\tLoss: 0.006758\n",
      "Training epoch: 73 [480/900 (53%)]\tLoss: 0.007659\n",
      "Training epoch: 73 [640/900 (70%)]\tLoss: 0.003934\n",
      "Training epoch: 73 [800/900 (88%)]\tLoss: 0.005386\n",
      "=========> Epoch: 73 Average loss: 0.0098\n",
      "Correlation coefficient: 0.6462\n",
      "✅ Epoch 73: New best correlation = 0.6462\n",
      "Training epoch: 74 [0/900 (0%)]\tLoss: 0.003486\n",
      "Training epoch: 74 [160/900 (18%)]\tLoss: 0.006886\n",
      "Training epoch: 74 [320/900 (35%)]\tLoss: 0.004113\n",
      "Training epoch: 74 [480/900 (53%)]\tLoss: 0.003669\n",
      "Training epoch: 74 [640/900 (70%)]\tLoss: 0.003919\n",
      "Training epoch: 74 [800/900 (88%)]\tLoss: 0.006092\n",
      "=========> Epoch: 74 Average loss: 0.0084\n",
      "Correlation coefficient: 0.6324\n",
      "Training epoch: 75 [0/900 (0%)]\tLoss: 0.002421\n",
      "Training epoch: 75 [160/900 (18%)]\tLoss: 0.008416\n",
      "Training epoch: 75 [320/900 (35%)]\tLoss: 0.006163\n",
      "Training epoch: 75 [480/900 (53%)]\tLoss: 0.007892\n",
      "Training epoch: 75 [640/900 (70%)]\tLoss: 0.011395\n",
      "Training epoch: 75 [800/900 (88%)]\tLoss: 0.006167\n",
      "=========> Epoch: 75 Average loss: 0.0070\n",
      "Correlation coefficient: 0.6397\n",
      "Training epoch: 76 [0/900 (0%)]\tLoss: 0.006271\n",
      "Training epoch: 76 [160/900 (18%)]\tLoss: 0.004335\n",
      "Training epoch: 76 [320/900 (35%)]\tLoss: 0.009223\n",
      "Training epoch: 76 [480/900 (53%)]\tLoss: 0.006981\n",
      "Training epoch: 76 [640/900 (70%)]\tLoss: 0.005488\n",
      "Training epoch: 76 [800/900 (88%)]\tLoss: 0.003628\n",
      "=========> Epoch: 76 Average loss: 0.0057\n",
      "Correlation coefficient: 0.6365\n",
      "Training epoch: 77 [0/900 (0%)]\tLoss: 0.003214\n",
      "Training epoch: 77 [160/900 (18%)]\tLoss: 0.004297\n",
      "Training epoch: 77 [320/900 (35%)]\tLoss: 0.002088\n",
      "Training epoch: 77 [480/900 (53%)]\tLoss: 0.008748\n",
      "Training epoch: 77 [640/900 (70%)]\tLoss: 0.004093\n",
      "Training epoch: 77 [800/900 (88%)]\tLoss: 0.005020\n",
      "=========> Epoch: 77 Average loss: 0.0076\n",
      "Correlation coefficient: 0.6406\n",
      "Training epoch: 78 [0/900 (0%)]\tLoss: 0.002625\n",
      "Training epoch: 78 [160/900 (18%)]\tLoss: 0.020542\n",
      "Training epoch: 78 [320/900 (35%)]\tLoss: 0.007437\n",
      "Training epoch: 78 [480/900 (53%)]\tLoss: 0.002674\n",
      "Training epoch: 78 [640/900 (70%)]\tLoss: 0.014735\n",
      "Training epoch: 78 [800/900 (88%)]\tLoss: 0.006533\n",
      "=========> Epoch: 78 Average loss: 0.0086\n",
      "Correlation coefficient: 0.6309\n",
      "Training epoch: 79 [0/900 (0%)]\tLoss: 0.005889\n",
      "Training epoch: 79 [160/900 (18%)]\tLoss: 0.008010\n",
      "Training epoch: 79 [320/900 (35%)]\tLoss: 0.003838\n",
      "Training epoch: 79 [480/900 (53%)]\tLoss: 0.012202\n",
      "Training epoch: 79 [640/900 (70%)]\tLoss: 0.006561\n",
      "Training epoch: 79 [800/900 (88%)]\tLoss: 0.004164\n",
      "=========> Epoch: 79 Average loss: 0.0075\n",
      "Correlation coefficient: 0.6411\n",
      "Training epoch: 80 [0/900 (0%)]\tLoss: 0.006659\n",
      "Training epoch: 80 [160/900 (18%)]\tLoss: 0.005279\n",
      "Training epoch: 80 [320/900 (35%)]\tLoss: 0.002737\n",
      "Training epoch: 80 [480/900 (53%)]\tLoss: 0.003069\n",
      "Training epoch: 80 [640/900 (70%)]\tLoss: 0.004505\n",
      "Training epoch: 80 [800/900 (88%)]\tLoss: 0.002377\n",
      "=========> Epoch: 80 Average loss: 0.0060\n",
      "Correlation coefficient: 0.6394\n",
      "Training epoch: 81 [0/900 (0%)]\tLoss: 0.004427\n",
      "Training epoch: 81 [160/900 (18%)]\tLoss: 0.002607\n",
      "Training epoch: 81 [320/900 (35%)]\tLoss: 0.003882\n",
      "Training epoch: 81 [480/900 (53%)]\tLoss: 0.003014\n",
      "Training epoch: 81 [640/900 (70%)]\tLoss: 0.004433\n",
      "Training epoch: 81 [800/900 (88%)]\tLoss: 0.003078\n",
      "=========> Epoch: 81 Average loss: 0.0047\n",
      "Correlation coefficient: 0.6397\n",
      "Training epoch: 82 [0/900 (0%)]\tLoss: 0.005892\n",
      "Training epoch: 82 [160/900 (18%)]\tLoss: 0.001759\n",
      "Training epoch: 82 [320/900 (35%)]\tLoss: 0.011319\n",
      "Training epoch: 82 [480/900 (53%)]\tLoss: 0.005450\n",
      "Training epoch: 82 [640/900 (70%)]\tLoss: 0.003059\n",
      "Training epoch: 82 [800/900 (88%)]\tLoss: 0.001243\n",
      "=========> Epoch: 82 Average loss: 0.0047\n",
      "Correlation coefficient: 0.6397\n",
      "Training epoch: 83 [0/900 (0%)]\tLoss: 0.002734\n",
      "Training epoch: 83 [160/900 (18%)]\tLoss: 0.002811\n",
      "Training epoch: 83 [320/900 (35%)]\tLoss: 0.004259\n",
      "Training epoch: 83 [480/900 (53%)]\tLoss: 0.010578\n",
      "Training epoch: 83 [640/900 (70%)]\tLoss: 0.004285\n",
      "Training epoch: 83 [800/900 (88%)]\tLoss: 0.019778\n",
      "=========> Epoch: 83 Average loss: 0.0063\n",
      "Correlation coefficient: 0.6442\n",
      "Training epoch: 84 [0/900 (0%)]\tLoss: 0.001163\n",
      "Training epoch: 84 [160/900 (18%)]\tLoss: 0.009949\n",
      "Training epoch: 84 [320/900 (35%)]\tLoss: 0.010209\n",
      "Training epoch: 84 [480/900 (53%)]\tLoss: 0.008772\n",
      "Training epoch: 84 [640/900 (70%)]\tLoss: 0.016429\n",
      "Training epoch: 84 [800/900 (88%)]\tLoss: 0.009384\n",
      "=========> Epoch: 84 Average loss: 0.0086\n",
      "Correlation coefficient: 0.6438\n",
      "Training epoch: 85 [0/900 (0%)]\tLoss: 0.006691\n",
      "Training epoch: 85 [160/900 (18%)]\tLoss: 0.006662\n",
      "Training epoch: 85 [320/900 (35%)]\tLoss: 0.013533\n",
      "Training epoch: 85 [480/900 (53%)]\tLoss: 0.009045\n",
      "Training epoch: 85 [640/900 (70%)]\tLoss: 0.004399\n",
      "Training epoch: 85 [800/900 (88%)]\tLoss: 0.004003\n",
      "=========> Epoch: 85 Average loss: 0.0087\n",
      "Correlation coefficient: 0.6428\n",
      "Training epoch: 86 [0/900 (0%)]\tLoss: 0.004817\n",
      "Training epoch: 86 [160/900 (18%)]\tLoss: 0.004264\n",
      "Training epoch: 86 [320/900 (35%)]\tLoss: 0.005306\n",
      "Training epoch: 86 [480/900 (53%)]\tLoss: 0.005624\n",
      "Training epoch: 86 [640/900 (70%)]\tLoss: 0.007908\n",
      "Training epoch: 86 [800/900 (88%)]\tLoss: 0.004779\n",
      "=========> Epoch: 86 Average loss: 0.0091\n",
      "Correlation coefficient: 0.6371\n",
      "Training epoch: 87 [0/900 (0%)]\tLoss: 0.011106\n",
      "Training epoch: 87 [160/900 (18%)]\tLoss: 0.008563\n",
      "Training epoch: 87 [320/900 (35%)]\tLoss: 0.002453\n",
      "Training epoch: 87 [480/900 (53%)]\tLoss: 0.004415\n",
      "Training epoch: 87 [640/900 (70%)]\tLoss: 0.013891\n",
      "Training epoch: 87 [800/900 (88%)]\tLoss: 0.010031\n",
      "=========> Epoch: 87 Average loss: 0.0106\n",
      "Correlation coefficient: 0.6485\n",
      "✅ Epoch 87: New best correlation = 0.6485\n",
      "Training epoch: 88 [0/900 (0%)]\tLoss: 0.013902\n",
      "Training epoch: 88 [160/900 (18%)]\tLoss: 0.010362\n",
      "Training epoch: 88 [320/900 (35%)]\tLoss: 0.007715\n",
      "Training epoch: 88 [480/900 (53%)]\tLoss: 0.009280\n",
      "Training epoch: 88 [640/900 (70%)]\tLoss: 0.004477\n",
      "Training epoch: 88 [800/900 (88%)]\tLoss: 0.007415\n",
      "=========> Epoch: 88 Average loss: 0.0151\n",
      "Correlation coefficient: 0.6192\n",
      "Training epoch: 89 [0/900 (0%)]\tLoss: 0.034432\n",
      "Training epoch: 89 [160/900 (18%)]\tLoss: 0.039237\n",
      "Training epoch: 89 [320/900 (35%)]\tLoss: 0.016214\n",
      "Training epoch: 89 [480/900 (53%)]\tLoss: 0.041457\n",
      "Training epoch: 89 [640/900 (70%)]\tLoss: 0.020078\n",
      "Training epoch: 89 [800/900 (88%)]\tLoss: 0.018857\n",
      "=========> Epoch: 89 Average loss: 0.0252\n",
      "Correlation coefficient: 0.6276\n",
      "Training epoch: 90 [0/900 (0%)]\tLoss: 0.010880\n",
      "Training epoch: 90 [160/900 (18%)]\tLoss: 0.045440\n",
      "Training epoch: 90 [320/900 (35%)]\tLoss: 0.009375\n",
      "Training epoch: 90 [480/900 (53%)]\tLoss: 0.009227\n",
      "Training epoch: 90 [640/900 (70%)]\tLoss: 0.010975\n",
      "Training epoch: 90 [800/900 (88%)]\tLoss: 0.009019\n",
      "=========> Epoch: 90 Average loss: 0.0220\n",
      "Correlation coefficient: 0.6237\n",
      "Training epoch: 91 [0/900 (0%)]\tLoss: 0.019396\n",
      "Training epoch: 91 [160/900 (18%)]\tLoss: 0.071458\n",
      "Training epoch: 91 [320/900 (35%)]\tLoss: 0.010037\n",
      "Training epoch: 91 [480/900 (53%)]\tLoss: 0.005360\n",
      "Training epoch: 91 [640/900 (70%)]\tLoss: 0.009671\n",
      "Training epoch: 91 [800/900 (88%)]\tLoss: 0.003776\n",
      "=========> Epoch: 91 Average loss: 0.0150\n",
      "Correlation coefficient: 0.6301\n",
      "Training epoch: 92 [0/900 (0%)]\tLoss: 0.007209\n",
      "Training epoch: 92 [160/900 (18%)]\tLoss: 0.006227\n",
      "Training epoch: 92 [320/900 (35%)]\tLoss: 0.008925\n",
      "Training epoch: 92 [480/900 (53%)]\tLoss: 0.005858\n",
      "Training epoch: 92 [640/900 (70%)]\tLoss: 0.010482\n",
      "Training epoch: 92 [800/900 (88%)]\tLoss: 0.009169\n",
      "=========> Epoch: 92 Average loss: 0.0144\n",
      "Correlation coefficient: 0.6330\n",
      "Training epoch: 93 [0/900 (0%)]\tLoss: 0.006911\n",
      "Training epoch: 93 [160/900 (18%)]\tLoss: 0.023836\n",
      "Training epoch: 93 [320/900 (35%)]\tLoss: 0.009462\n",
      "Training epoch: 93 [480/900 (53%)]\tLoss: 0.011027\n",
      "Training epoch: 93 [640/900 (70%)]\tLoss: 0.005314\n",
      "Training epoch: 93 [800/900 (88%)]\tLoss: 0.027662\n",
      "=========> Epoch: 93 Average loss: 0.0147\n",
      "Correlation coefficient: 0.6449\n",
      "Training epoch: 94 [0/900 (0%)]\tLoss: 0.032200\n",
      "Training epoch: 94 [160/900 (18%)]\tLoss: 0.018823\n",
      "Training epoch: 94 [320/900 (35%)]\tLoss: 0.034263\n",
      "Training epoch: 94 [480/900 (53%)]\tLoss: 0.004097\n",
      "Training epoch: 94 [640/900 (70%)]\tLoss: 0.011357\n",
      "Training epoch: 94 [800/900 (88%)]\tLoss: 0.008997\n",
      "=========> Epoch: 94 Average loss: 0.0121\n",
      "Correlation coefficient: 0.6381\n",
      "Training epoch: 95 [0/900 (0%)]\tLoss: 0.008219\n",
      "Training epoch: 95 [160/900 (18%)]\tLoss: 0.020767\n",
      "Training epoch: 95 [320/900 (35%)]\tLoss: 0.019284\n",
      "Training epoch: 95 [480/900 (53%)]\tLoss: 0.012054\n",
      "Training epoch: 95 [640/900 (70%)]\tLoss: 0.025844\n",
      "Training epoch: 95 [800/900 (88%)]\tLoss: 0.012237\n",
      "=========> Epoch: 95 Average loss: 0.0123\n",
      "Correlation coefficient: 0.6259\n",
      "Training epoch: 96 [0/900 (0%)]\tLoss: 0.006075\n",
      "Training epoch: 96 [160/900 (18%)]\tLoss: 0.020044\n",
      "Training epoch: 96 [320/900 (35%)]\tLoss: 0.013192\n",
      "Training epoch: 96 [480/900 (53%)]\tLoss: 0.011445\n",
      "Training epoch: 96 [640/900 (70%)]\tLoss: 0.004219\n",
      "Training epoch: 96 [800/900 (88%)]\tLoss: 0.005816\n",
      "=========> Epoch: 96 Average loss: 0.0102\n",
      "Correlation coefficient: 0.6364\n",
      "Training epoch: 97 [0/900 (0%)]\tLoss: 0.003099\n",
      "Training epoch: 97 [160/900 (18%)]\tLoss: 0.021501\n",
      "Training epoch: 97 [320/900 (35%)]\tLoss: 0.004796\n",
      "Training epoch: 97 [480/900 (53%)]\tLoss: 0.008690\n",
      "Training epoch: 97 [640/900 (70%)]\tLoss: 0.005334\n",
      "Training epoch: 97 [800/900 (88%)]\tLoss: 0.012867\n",
      "=========> Epoch: 97 Average loss: 0.0083\n",
      "Correlation coefficient: 0.6355\n",
      "Training epoch: 98 [0/900 (0%)]\tLoss: 0.011817\n",
      "Training epoch: 98 [160/900 (18%)]\tLoss: 0.005837\n",
      "Training epoch: 98 [320/900 (35%)]\tLoss: 0.007579\n",
      "Training epoch: 98 [480/900 (53%)]\tLoss: 0.002640\n",
      "Training epoch: 98 [640/900 (70%)]\tLoss: 0.008981\n",
      "Training epoch: 98 [800/900 (88%)]\tLoss: 0.003516\n",
      "=========> Epoch: 98 Average loss: 0.0073\n",
      "Correlation coefficient: 0.6377\n",
      "Training epoch: 99 [0/900 (0%)]\tLoss: 0.003817\n",
      "Training epoch: 99 [160/900 (18%)]\tLoss: 0.004240\n",
      "Training epoch: 99 [320/900 (35%)]\tLoss: 0.006155\n",
      "Training epoch: 99 [480/900 (53%)]\tLoss: 0.004019\n",
      "Training epoch: 99 [640/900 (70%)]\tLoss: 0.004956\n",
      "Training epoch: 99 [800/900 (88%)]\tLoss: 0.004839\n",
      "=========> Epoch: 99 Average loss: 0.0048\n",
      "Correlation coefficient: 0.6412\n",
      "Training epoch: 100 [0/900 (0%)]\tLoss: 0.009454\n",
      "Training epoch: 100 [160/900 (18%)]\tLoss: 0.005962\n",
      "Training epoch: 100 [320/900 (35%)]\tLoss: 0.003111\n",
      "Training epoch: 100 [480/900 (53%)]\tLoss: 0.002169\n",
      "Training epoch: 100 [640/900 (70%)]\tLoss: 0.003790\n",
      "Training epoch: 100 [800/900 (88%)]\tLoss: 0.001112\n",
      "=========> Epoch: 100 Average loss: 0.0035\n",
      "Correlation coefficient: 0.6383\n",
      "Training epoch: 101 [0/900 (0%)]\tLoss: 0.004244\n",
      "Training epoch: 101 [160/900 (18%)]\tLoss: 0.002386\n",
      "Training epoch: 101 [320/900 (35%)]\tLoss: 0.002850\n",
      "Training epoch: 101 [480/900 (53%)]\tLoss: 0.001325\n",
      "Training epoch: 101 [640/900 (70%)]\tLoss: 0.004681\n",
      "Training epoch: 101 [800/900 (88%)]\tLoss: 0.002039\n",
      "=========> Epoch: 101 Average loss: 0.0027\n",
      "Correlation coefficient: 0.6404\n",
      "Training epoch: 102 [0/900 (0%)]\tLoss: 0.000589\n",
      "Training epoch: 102 [160/900 (18%)]\tLoss: 0.001737\n",
      "Training epoch: 102 [320/900 (35%)]\tLoss: 0.004986\n",
      "Training epoch: 102 [480/900 (53%)]\tLoss: 0.001153\n",
      "Training epoch: 102 [640/900 (70%)]\tLoss: 0.000570\n",
      "Training epoch: 102 [800/900 (88%)]\tLoss: 0.004493\n",
      "=========> Epoch: 102 Average loss: 0.0019\n",
      "Correlation coefficient: 0.6370\n",
      "Training epoch: 103 [0/900 (0%)]\tLoss: 0.001786\n",
      "Training epoch: 103 [160/900 (18%)]\tLoss: 0.000814\n",
      "Training epoch: 103 [320/900 (35%)]\tLoss: 0.000673\n",
      "Training epoch: 103 [480/900 (53%)]\tLoss: 0.001612\n",
      "Training epoch: 103 [640/900 (70%)]\tLoss: 0.001958\n",
      "Training epoch: 103 [800/900 (88%)]\tLoss: 0.001035\n",
      "=========> Epoch: 103 Average loss: 0.0012\n",
      "Correlation coefficient: 0.6353\n",
      "Training epoch: 104 [0/900 (0%)]\tLoss: 0.000432\n",
      "Training epoch: 104 [160/900 (18%)]\tLoss: 0.000867\n",
      "Training epoch: 104 [320/900 (35%)]\tLoss: 0.000742\n",
      "Training epoch: 104 [480/900 (53%)]\tLoss: 0.000262\n",
      "Training epoch: 104 [640/900 (70%)]\tLoss: 0.002064\n",
      "Training epoch: 104 [800/900 (88%)]\tLoss: 0.000439\n",
      "=========> Epoch: 104 Average loss: 0.0010\n",
      "Correlation coefficient: 0.6399\n",
      "Training epoch: 105 [0/900 (0%)]\tLoss: 0.000622\n",
      "Training epoch: 105 [160/900 (18%)]\tLoss: 0.000302\n",
      "Training epoch: 105 [320/900 (35%)]\tLoss: 0.001015\n",
      "Training epoch: 105 [480/900 (53%)]\tLoss: 0.000809\n",
      "Training epoch: 105 [640/900 (70%)]\tLoss: 0.000306\n",
      "Training epoch: 105 [800/900 (88%)]\tLoss: 0.000755\n",
      "=========> Epoch: 105 Average loss: 0.0010\n",
      "Correlation coefficient: 0.6386\n",
      "Training epoch: 106 [0/900 (0%)]\tLoss: 0.001079\n",
      "Training epoch: 106 [160/900 (18%)]\tLoss: 0.000157\n",
      "Training epoch: 106 [320/900 (35%)]\tLoss: 0.000383\n",
      "Training epoch: 106 [480/900 (53%)]\tLoss: 0.000404\n",
      "Training epoch: 106 [640/900 (70%)]\tLoss: 0.000724\n",
      "Training epoch: 106 [800/900 (88%)]\tLoss: 0.000861\n",
      "=========> Epoch: 106 Average loss: 0.0013\n",
      "Correlation coefficient: 0.6339\n",
      "Training epoch: 107 [0/900 (0%)]\tLoss: 0.000659\n",
      "Training epoch: 107 [160/900 (18%)]\tLoss: 0.001436\n",
      "Training epoch: 107 [320/900 (35%)]\tLoss: 0.001857\n",
      "Training epoch: 107 [480/900 (53%)]\tLoss: 0.001344\n",
      "Training epoch: 107 [640/900 (70%)]\tLoss: 0.001109\n",
      "Training epoch: 107 [800/900 (88%)]\tLoss: 0.000650\n",
      "=========> Epoch: 107 Average loss: 0.0014\n",
      "Correlation coefficient: 0.6406\n",
      "Training epoch: 108 [0/900 (0%)]\tLoss: 0.000728\n",
      "Training epoch: 108 [160/900 (18%)]\tLoss: 0.000567\n",
      "Training epoch: 108 [320/900 (35%)]\tLoss: 0.000731\n",
      "Training epoch: 108 [480/900 (53%)]\tLoss: 0.000187\n",
      "Training epoch: 108 [640/900 (70%)]\tLoss: 0.001020\n",
      "Training epoch: 108 [800/900 (88%)]\tLoss: 0.001380\n",
      "=========> Epoch: 108 Average loss: 0.0017\n",
      "Correlation coefficient: 0.6388\n",
      "Training epoch: 109 [0/900 (0%)]\tLoss: 0.002232\n",
      "Training epoch: 109 [160/900 (18%)]\tLoss: 0.001675\n",
      "Training epoch: 109 [320/900 (35%)]\tLoss: 0.002619\n",
      "Training epoch: 109 [480/900 (53%)]\tLoss: 0.001690\n",
      "Training epoch: 109 [640/900 (70%)]\tLoss: 0.000786\n",
      "Training epoch: 109 [800/900 (88%)]\tLoss: 0.007331\n",
      "=========> Epoch: 109 Average loss: 0.0029\n",
      "Correlation coefficient: 0.6389\n",
      "Training epoch: 110 [0/900 (0%)]\tLoss: 0.002856\n",
      "Training epoch: 110 [160/900 (18%)]\tLoss: 0.006678\n",
      "Training epoch: 110 [320/900 (35%)]\tLoss: 0.001422\n",
      "Training epoch: 110 [480/900 (53%)]\tLoss: 0.002534\n",
      "Training epoch: 110 [640/900 (70%)]\tLoss: 0.003931\n",
      "Training epoch: 110 [800/900 (88%)]\tLoss: 0.002988\n",
      "=========> Epoch: 110 Average loss: 0.0041\n",
      "Correlation coefficient: 0.6245\n",
      "Training epoch: 111 [0/900 (0%)]\tLoss: 0.001423\n",
      "Training epoch: 111 [160/900 (18%)]\tLoss: 0.002701\n",
      "Training epoch: 111 [320/900 (35%)]\tLoss: 0.006155\n",
      "Training epoch: 111 [480/900 (53%)]\tLoss: 0.004797\n",
      "Training epoch: 111 [640/900 (70%)]\tLoss: 0.006214\n",
      "Training epoch: 111 [800/900 (88%)]\tLoss: 0.010019\n",
      "=========> Epoch: 111 Average loss: 0.0063\n",
      "Correlation coefficient: 0.6448\n",
      "Training epoch: 112 [0/900 (0%)]\tLoss: 0.004532\n",
      "Training epoch: 112 [160/900 (18%)]\tLoss: 0.015158\n",
      "Training epoch: 112 [320/900 (35%)]\tLoss: 0.010304\n",
      "Training epoch: 112 [480/900 (53%)]\tLoss: 0.008340\n",
      "Training epoch: 112 [640/900 (70%)]\tLoss: 0.007303\n",
      "Training epoch: 112 [800/900 (88%)]\tLoss: 0.024606\n",
      "=========> Epoch: 112 Average loss: 0.0095\n",
      "Correlation coefficient: 0.6305\n",
      "Training epoch: 113 [0/900 (0%)]\tLoss: 0.001319\n",
      "Training epoch: 113 [160/900 (18%)]\tLoss: 0.015395\n",
      "Training epoch: 113 [320/900 (35%)]\tLoss: 0.020972\n",
      "Training epoch: 113 [480/900 (53%)]\tLoss: 0.019827\n",
      "Training epoch: 113 [640/900 (70%)]\tLoss: 0.011972\n",
      "Training epoch: 113 [800/900 (88%)]\tLoss: 0.009884\n",
      "=========> Epoch: 113 Average loss: 0.0212\n",
      "Correlation coefficient: 0.6449\n",
      "Training epoch: 114 [0/900 (0%)]\tLoss: 0.028443\n",
      "Training epoch: 114 [160/900 (18%)]\tLoss: 0.011892\n",
      "Training epoch: 114 [320/900 (35%)]\tLoss: 0.008257\n",
      "Training epoch: 114 [480/900 (53%)]\tLoss: 0.013845\n",
      "Training epoch: 114 [640/900 (70%)]\tLoss: 0.040761\n",
      "Training epoch: 114 [800/900 (88%)]\tLoss: 0.024082\n",
      "=========> Epoch: 114 Average loss: 0.0427\n",
      "Correlation coefficient: 0.6271\n",
      "Training epoch: 115 [0/900 (0%)]\tLoss: 0.046504\n",
      "Training epoch: 115 [160/900 (18%)]\tLoss: 0.073090\n",
      "Training epoch: 115 [320/900 (35%)]\tLoss: 0.027179\n",
      "Training epoch: 115 [480/900 (53%)]\tLoss: 0.029206\n",
      "Training epoch: 115 [640/900 (70%)]\tLoss: 0.051521\n",
      "Training epoch: 115 [800/900 (88%)]\tLoss: 0.022625\n",
      "=========> Epoch: 115 Average loss: 0.0458\n",
      "Correlation coefficient: 0.6275\n",
      "Training epoch: 116 [0/900 (0%)]\tLoss: 0.069746\n",
      "Training epoch: 116 [160/900 (18%)]\tLoss: 0.029151\n",
      "Training epoch: 116 [320/900 (35%)]\tLoss: 0.027387\n",
      "Training epoch: 116 [480/900 (53%)]\tLoss: 0.030593\n",
      "Training epoch: 116 [640/900 (70%)]\tLoss: 0.026329\n",
      "Training epoch: 116 [800/900 (88%)]\tLoss: 0.024621\n",
      "=========> Epoch: 116 Average loss: 0.0430\n",
      "Correlation coefficient: 0.6189\n",
      "Training epoch: 117 [0/900 (0%)]\tLoss: 0.030661\n",
      "Training epoch: 117 [160/900 (18%)]\tLoss: 0.022787\n",
      "Training epoch: 117 [320/900 (35%)]\tLoss: 0.029194\n",
      "Training epoch: 117 [480/900 (53%)]\tLoss: 0.031847\n",
      "Training epoch: 117 [640/900 (70%)]\tLoss: 0.017148\n",
      "Training epoch: 117 [800/900 (88%)]\tLoss: 0.050351\n",
      "=========> Epoch: 117 Average loss: 0.0291\n",
      "Correlation coefficient: 0.5992\n",
      "Training epoch: 118 [0/900 (0%)]\tLoss: 0.033079\n",
      "Training epoch: 118 [160/900 (18%)]\tLoss: 0.006857\n",
      "Training epoch: 118 [320/900 (35%)]\tLoss: 0.027153\n",
      "Training epoch: 118 [480/900 (53%)]\tLoss: 0.012769\n",
      "Training epoch: 118 [640/900 (70%)]\tLoss: 0.011998\n",
      "Training epoch: 118 [800/900 (88%)]\tLoss: 0.017811\n",
      "=========> Epoch: 118 Average loss: 0.0149\n",
      "Correlation coefficient: 0.6405\n",
      "Training epoch: 119 [0/900 (0%)]\tLoss: 0.003725\n",
      "Training epoch: 119 [160/900 (18%)]\tLoss: 0.014552\n",
      "Training epoch: 119 [320/900 (35%)]\tLoss: 0.008969\n",
      "Training epoch: 119 [480/900 (53%)]\tLoss: 0.004745\n",
      "Training epoch: 119 [640/900 (70%)]\tLoss: 0.014005\n",
      "Training epoch: 119 [800/900 (88%)]\tLoss: 0.003885\n",
      "=========> Epoch: 119 Average loss: 0.0079\n",
      "Correlation coefficient: 0.6219\n",
      "Training epoch: 120 [0/900 (0%)]\tLoss: 0.005857\n",
      "Training epoch: 120 [160/900 (18%)]\tLoss: 0.003169\n",
      "Training epoch: 120 [320/900 (35%)]\tLoss: 0.003578\n",
      "Training epoch: 120 [480/900 (53%)]\tLoss: 0.007258\n",
      "Training epoch: 120 [640/900 (70%)]\tLoss: 0.003907\n",
      "Training epoch: 120 [800/900 (88%)]\tLoss: 0.003586\n",
      "=========> Epoch: 120 Average loss: 0.0054\n",
      "Correlation coefficient: 0.6277\n",
      "Training epoch: 121 [0/900 (0%)]\tLoss: 0.002995\n",
      "Training epoch: 121 [160/900 (18%)]\tLoss: 0.003306\n",
      "Training epoch: 121 [320/900 (35%)]\tLoss: 0.001797\n",
      "Training epoch: 121 [480/900 (53%)]\tLoss: 0.000914\n",
      "Training epoch: 121 [640/900 (70%)]\tLoss: 0.001480\n",
      "Training epoch: 121 [800/900 (88%)]\tLoss: 0.004451\n",
      "=========> Epoch: 121 Average loss: 0.0038\n",
      "Correlation coefficient: 0.6340\n",
      "Training epoch: 122 [0/900 (0%)]\tLoss: 0.002127\n",
      "Training epoch: 122 [160/900 (18%)]\tLoss: 0.005557\n",
      "Training epoch: 122 [320/900 (35%)]\tLoss: 0.002946\n",
      "Training epoch: 122 [480/900 (53%)]\tLoss: 0.003883\n",
      "Training epoch: 122 [640/900 (70%)]\tLoss: 0.001330\n",
      "Training epoch: 122 [800/900 (88%)]\tLoss: 0.001150\n",
      "=========> Epoch: 122 Average loss: 0.0022\n",
      "Correlation coefficient: 0.6295\n",
      "Training epoch: 123 [0/900 (0%)]\tLoss: 0.000706\n",
      "Training epoch: 123 [160/900 (18%)]\tLoss: 0.000539\n",
      "Training epoch: 123 [320/900 (35%)]\tLoss: 0.000414\n",
      "Training epoch: 123 [480/900 (53%)]\tLoss: 0.004750\n",
      "Training epoch: 123 [640/900 (70%)]\tLoss: 0.001548\n",
      "Training epoch: 123 [800/900 (88%)]\tLoss: 0.000757\n",
      "=========> Epoch: 123 Average loss: 0.0012\n",
      "Correlation coefficient: 0.6343\n",
      "Training epoch: 124 [0/900 (0%)]\tLoss: 0.000955\n",
      "Training epoch: 124 [160/900 (18%)]\tLoss: 0.000933\n",
      "Training epoch: 124 [320/900 (35%)]\tLoss: 0.000604\n",
      "Training epoch: 124 [480/900 (53%)]\tLoss: 0.000896\n",
      "Training epoch: 124 [640/900 (70%)]\tLoss: 0.000382\n",
      "Training epoch: 124 [800/900 (88%)]\tLoss: 0.000851\n",
      "=========> Epoch: 124 Average loss: 0.0009\n",
      "Correlation coefficient: 0.6344\n",
      "Training epoch: 125 [0/900 (0%)]\tLoss: 0.001110\n",
      "Training epoch: 125 [160/900 (18%)]\tLoss: 0.001859\n",
      "Training epoch: 125 [320/900 (35%)]\tLoss: 0.000248\n",
      "Training epoch: 125 [480/900 (53%)]\tLoss: 0.000530\n",
      "Training epoch: 125 [640/900 (70%)]\tLoss: 0.000137\n",
      "Training epoch: 125 [800/900 (88%)]\tLoss: 0.000144\n",
      "=========> Epoch: 125 Average loss: 0.0006\n",
      "Correlation coefficient: 0.6319\n",
      "Training epoch: 126 [0/900 (0%)]\tLoss: 0.000885\n",
      "Training epoch: 126 [160/900 (18%)]\tLoss: 0.000288\n",
      "Training epoch: 126 [320/900 (35%)]\tLoss: 0.000350\n",
      "Training epoch: 126 [480/900 (53%)]\tLoss: 0.003158\n",
      "Training epoch: 126 [640/900 (70%)]\tLoss: 0.000383\n",
      "Training epoch: 126 [800/900 (88%)]\tLoss: 0.000449\n",
      "=========> Epoch: 126 Average loss: 0.0006\n",
      "Correlation coefficient: 0.6338\n",
      "Training epoch: 127 [0/900 (0%)]\tLoss: 0.000038\n",
      "Training epoch: 127 [160/900 (18%)]\tLoss: 0.000400\n",
      "Training epoch: 127 [320/900 (35%)]\tLoss: 0.000837\n",
      "Training epoch: 127 [480/900 (53%)]\tLoss: 0.001006\n",
      "Training epoch: 127 [640/900 (70%)]\tLoss: 0.000113\n",
      "Training epoch: 127 [800/900 (88%)]\tLoss: 0.000502\n",
      "=========> Epoch: 127 Average loss: 0.0005\n",
      "Correlation coefficient: 0.6330\n",
      "Training epoch: 128 [0/900 (0%)]\tLoss: 0.000501\n",
      "Training epoch: 128 [160/900 (18%)]\tLoss: 0.000477\n",
      "Training epoch: 128 [320/900 (35%)]\tLoss: 0.000451\n",
      "Training epoch: 128 [480/900 (53%)]\tLoss: 0.000138\n",
      "Training epoch: 128 [640/900 (70%)]\tLoss: 0.000389\n",
      "Training epoch: 128 [800/900 (88%)]\tLoss: 0.000784\n",
      "=========> Epoch: 128 Average loss: 0.0005\n",
      "Correlation coefficient: 0.6332\n",
      "Training epoch: 129 [0/900 (0%)]\tLoss: 0.000696\n",
      "Training epoch: 129 [160/900 (18%)]\tLoss: 0.000389\n",
      "Training epoch: 129 [320/900 (35%)]\tLoss: 0.000480\n",
      "Training epoch: 129 [480/900 (53%)]\tLoss: 0.000169\n",
      "Training epoch: 129 [640/900 (70%)]\tLoss: 0.000792\n",
      "Training epoch: 129 [800/900 (88%)]\tLoss: 0.000473\n",
      "=========> Epoch: 129 Average loss: 0.0004\n",
      "Correlation coefficient: 0.6324\n",
      "Training epoch: 130 [0/900 (0%)]\tLoss: 0.000557\n",
      "Training epoch: 130 [160/900 (18%)]\tLoss: 0.000129\n",
      "Training epoch: 130 [320/900 (35%)]\tLoss: 0.000298\n",
      "Training epoch: 130 [480/900 (53%)]\tLoss: 0.000456\n",
      "Training epoch: 130 [640/900 (70%)]\tLoss: 0.000279\n",
      "Training epoch: 130 [800/900 (88%)]\tLoss: 0.000213\n",
      "=========> Epoch: 130 Average loss: 0.0005\n",
      "Correlation coefficient: 0.6353\n",
      "Training epoch: 131 [0/900 (0%)]\tLoss: 0.000348\n",
      "Training epoch: 131 [160/900 (18%)]\tLoss: 0.000303\n",
      "Training epoch: 131 [320/900 (35%)]\tLoss: 0.000211\n",
      "Training epoch: 131 [480/900 (53%)]\tLoss: 0.000099\n",
      "Training epoch: 131 [640/900 (70%)]\tLoss: 0.000387\n",
      "Training epoch: 131 [800/900 (88%)]\tLoss: 0.000679\n",
      "=========> Epoch: 131 Average loss: 0.0004\n",
      "Correlation coefficient: 0.6336\n",
      "Training epoch: 132 [0/900 (0%)]\tLoss: 0.000180\n",
      "Training epoch: 132 [160/900 (18%)]\tLoss: 0.000243\n",
      "Training epoch: 132 [320/900 (35%)]\tLoss: 0.000516\n",
      "Training epoch: 132 [480/900 (53%)]\tLoss: 0.000177\n",
      "Training epoch: 132 [640/900 (70%)]\tLoss: 0.000256\n",
      "Training epoch: 132 [800/900 (88%)]\tLoss: 0.000323\n",
      "=========> Epoch: 132 Average loss: 0.0004\n",
      "Correlation coefficient: 0.6336\n",
      "Training epoch: 133 [0/900 (0%)]\tLoss: 0.000360\n",
      "Training epoch: 133 [160/900 (18%)]\tLoss: 0.000175\n",
      "Training epoch: 133 [320/900 (35%)]\tLoss: 0.000168\n",
      "Training epoch: 133 [480/900 (53%)]\tLoss: 0.000206\n",
      "Training epoch: 133 [640/900 (70%)]\tLoss: 0.000417\n",
      "Training epoch: 133 [800/900 (88%)]\tLoss: 0.000323\n",
      "=========> Epoch: 133 Average loss: 0.0004\n",
      "Correlation coefficient: 0.6325\n",
      "Training epoch: 134 [0/900 (0%)]\tLoss: 0.000331\n",
      "Training epoch: 134 [160/900 (18%)]\tLoss: 0.000575\n",
      "Training epoch: 134 [320/900 (35%)]\tLoss: 0.000309\n",
      "Training epoch: 134 [480/900 (53%)]\tLoss: 0.000405\n",
      "Training epoch: 134 [640/900 (70%)]\tLoss: 0.000416\n",
      "Training epoch: 134 [800/900 (88%)]\tLoss: 0.000172\n",
      "=========> Epoch: 134 Average loss: 0.0006\n",
      "Correlation coefficient: 0.6338\n",
      "Training epoch: 135 [0/900 (0%)]\tLoss: 0.000216\n",
      "Training epoch: 135 [160/900 (18%)]\tLoss: 0.001071\n",
      "Training epoch: 135 [320/900 (35%)]\tLoss: 0.000362\n",
      "Training epoch: 135 [480/900 (53%)]\tLoss: 0.000204\n",
      "Training epoch: 135 [640/900 (70%)]\tLoss: 0.000515\n",
      "Training epoch: 135 [800/900 (88%)]\tLoss: 0.000840\n",
      "=========> Epoch: 135 Average loss: 0.0006\n",
      "Correlation coefficient: 0.6345\n",
      "Training epoch: 136 [0/900 (0%)]\tLoss: 0.000974\n",
      "Training epoch: 136 [160/900 (18%)]\tLoss: 0.000685\n",
      "Training epoch: 136 [320/900 (35%)]\tLoss: 0.000636\n",
      "Training epoch: 136 [480/900 (53%)]\tLoss: 0.000420\n",
      "Training epoch: 136 [640/900 (70%)]\tLoss: 0.000395\n",
      "Training epoch: 136 [800/900 (88%)]\tLoss: 0.000782\n",
      "=========> Epoch: 136 Average loss: 0.0011\n",
      "Correlation coefficient: 0.6353\n",
      "Training epoch: 137 [0/900 (0%)]\tLoss: 0.000139\n",
      "Training epoch: 137 [160/900 (18%)]\tLoss: 0.001064\n",
      "Training epoch: 137 [320/900 (35%)]\tLoss: 0.001368\n",
      "Training epoch: 137 [480/900 (53%)]\tLoss: 0.001442\n",
      "Training epoch: 137 [640/900 (70%)]\tLoss: 0.001847\n",
      "Training epoch: 137 [800/900 (88%)]\tLoss: 0.001025\n",
      "=========> Epoch: 137 Average loss: 0.0013\n",
      "Correlation coefficient: 0.6356\n",
      "⏹️  Epoch 137 early stopping (no improvement for 50 epochs)\n",
      "🏁 Fold 7 best correlation: 0.6485\n",
      "\n",
      "========== Cross-validation Fold 8/10 ==========\n",
      "🔄 Fold 8: Using random initialization\n",
      "Training epoch: 1 [0/900 (0%)]\tLoss: 0.792847\n",
      "Training epoch: 1 [160/900 (18%)]\tLoss: 1.108772\n",
      "Training epoch: 1 [320/900 (35%)]\tLoss: 0.774771\n",
      "Training epoch: 1 [480/900 (53%)]\tLoss: 0.834692\n",
      "Training epoch: 1 [640/900 (70%)]\tLoss: 0.593431\n",
      "Training epoch: 1 [800/900 (88%)]\tLoss: 0.712940\n",
      "=========> Epoch: 1 Average loss: 0.9029\n",
      "Correlation coefficient: 0.6564\n",
      "✅ Epoch 1: New best correlation = 0.6564\n",
      "Training epoch: 2 [0/900 (0%)]\tLoss: 0.191481\n",
      "Training epoch: 2 [160/900 (18%)]\tLoss: 0.286206\n",
      "Training epoch: 2 [320/900 (35%)]\tLoss: 0.248666\n",
      "Training epoch: 2 [480/900 (53%)]\tLoss: 0.218241\n",
      "Training epoch: 2 [640/900 (70%)]\tLoss: 0.310234\n",
      "Training epoch: 2 [800/900 (88%)]\tLoss: 0.349490\n",
      "=========> Epoch: 2 Average loss: 0.3285\n",
      "Correlation coefficient: 0.6010\n",
      "Training epoch: 3 [0/900 (0%)]\tLoss: 0.072980\n",
      "Training epoch: 3 [160/900 (18%)]\tLoss: 0.160283\n",
      "Training epoch: 3 [320/900 (35%)]\tLoss: 0.095934\n",
      "Training epoch: 3 [480/900 (53%)]\tLoss: 0.233508\n",
      "Training epoch: 3 [640/900 (70%)]\tLoss: 0.075497\n",
      "Training epoch: 3 [800/900 (88%)]\tLoss: 0.098133\n",
      "=========> Epoch: 3 Average loss: 0.1265\n",
      "Correlation coefficient: 0.5891\n",
      "Training epoch: 4 [0/900 (0%)]\tLoss: 0.064954\n",
      "Training epoch: 4 [160/900 (18%)]\tLoss: 0.113068\n",
      "Training epoch: 4 [320/900 (35%)]\tLoss: 0.026424\n",
      "Training epoch: 4 [480/900 (53%)]\tLoss: 0.043682\n",
      "Training epoch: 4 [640/900 (70%)]\tLoss: 0.030553\n",
      "Training epoch: 4 [800/900 (88%)]\tLoss: 0.054254\n",
      "=========> Epoch: 4 Average loss: 0.0480\n",
      "Correlation coefficient: 0.6046\n",
      "Training epoch: 5 [0/900 (0%)]\tLoss: 0.014343\n",
      "Training epoch: 5 [160/900 (18%)]\tLoss: 0.026462\n",
      "Training epoch: 5 [320/900 (35%)]\tLoss: 0.069407\n",
      "Training epoch: 5 [480/900 (53%)]\tLoss: 0.062691\n",
      "Training epoch: 5 [640/900 (70%)]\tLoss: 0.029298\n",
      "Training epoch: 5 [800/900 (88%)]\tLoss: 0.017892\n",
      "=========> Epoch: 5 Average loss: 0.0366\n",
      "Correlation coefficient: 0.6230\n",
      "Training epoch: 6 [0/900 (0%)]\tLoss: 0.022049\n",
      "Training epoch: 6 [160/900 (18%)]\tLoss: 0.076020\n",
      "Training epoch: 6 [320/900 (35%)]\tLoss: 0.026658\n",
      "Training epoch: 6 [480/900 (53%)]\tLoss: 0.089396\n",
      "Training epoch: 6 [640/900 (70%)]\tLoss: 0.022166\n",
      "Training epoch: 6 [800/900 (88%)]\tLoss: 0.012423\n",
      "=========> Epoch: 6 Average loss: 0.0246\n",
      "Correlation coefficient: 0.6155\n",
      "Training epoch: 7 [0/900 (0%)]\tLoss: 0.022347\n",
      "Training epoch: 7 [160/900 (18%)]\tLoss: 0.019221\n",
      "Training epoch: 7 [320/900 (35%)]\tLoss: 0.015724\n",
      "Training epoch: 7 [480/900 (53%)]\tLoss: 0.014178\n",
      "Training epoch: 7 [640/900 (70%)]\tLoss: 0.016736\n",
      "Training epoch: 7 [800/900 (88%)]\tLoss: 0.018284\n",
      "=========> Epoch: 7 Average loss: 0.0185\n",
      "Correlation coefficient: 0.6201\n",
      "Training epoch: 8 [0/900 (0%)]\tLoss: 0.005267\n",
      "Training epoch: 8 [160/900 (18%)]\tLoss: 0.009110\n",
      "Training epoch: 8 [320/900 (35%)]\tLoss: 0.026759\n",
      "Training epoch: 8 [480/900 (53%)]\tLoss: 0.021431\n",
      "Training epoch: 8 [640/900 (70%)]\tLoss: 0.022505\n",
      "Training epoch: 8 [800/900 (88%)]\tLoss: 0.016942\n",
      "=========> Epoch: 8 Average loss: 0.0144\n",
      "Correlation coefficient: 0.6123\n",
      "Training epoch: 9 [0/900 (0%)]\tLoss: 0.012757\n",
      "Training epoch: 9 [160/900 (18%)]\tLoss: 0.005498\n",
      "Training epoch: 9 [320/900 (35%)]\tLoss: 0.006108\n",
      "Training epoch: 9 [480/900 (53%)]\tLoss: 0.003368\n",
      "Training epoch: 9 [640/900 (70%)]\tLoss: 0.016390\n",
      "Training epoch: 9 [800/900 (88%)]\tLoss: 0.013528\n",
      "=========> Epoch: 9 Average loss: 0.0104\n",
      "Correlation coefficient: 0.6146\n",
      "Training epoch: 10 [0/900 (0%)]\tLoss: 0.003073\n",
      "Training epoch: 10 [160/900 (18%)]\tLoss: 0.007839\n",
      "Training epoch: 10 [320/900 (35%)]\tLoss: 0.007773\n",
      "Training epoch: 10 [480/900 (53%)]\tLoss: 0.008479\n",
      "Training epoch: 10 [640/900 (70%)]\tLoss: 0.007501\n",
      "Training epoch: 10 [800/900 (88%)]\tLoss: 0.014782\n",
      "=========> Epoch: 10 Average loss: 0.0075\n",
      "Correlation coefficient: 0.6107\n",
      "Training epoch: 11 [0/900 (0%)]\tLoss: 0.004885\n",
      "Training epoch: 11 [160/900 (18%)]\tLoss: 0.019967\n",
      "Training epoch: 11 [320/900 (35%)]\tLoss: 0.003833\n",
      "Training epoch: 11 [480/900 (53%)]\tLoss: 0.003927\n",
      "Training epoch: 11 [640/900 (70%)]\tLoss: 0.003661\n",
      "Training epoch: 11 [800/900 (88%)]\tLoss: 0.005513\n",
      "=========> Epoch: 11 Average loss: 0.0074\n",
      "Correlation coefficient: 0.6249\n",
      "Training epoch: 12 [0/900 (0%)]\tLoss: 0.006234\n",
      "Training epoch: 12 [160/900 (18%)]\tLoss: 0.003920\n",
      "Training epoch: 12 [320/900 (35%)]\tLoss: 0.008121\n",
      "Training epoch: 12 [480/900 (53%)]\tLoss: 0.005939\n",
      "Training epoch: 12 [640/900 (70%)]\tLoss: 0.006969\n",
      "Training epoch: 12 [800/900 (88%)]\tLoss: 0.011964\n",
      "=========> Epoch: 12 Average loss: 0.0066\n",
      "Correlation coefficient: 0.6133\n",
      "Training epoch: 13 [0/900 (0%)]\tLoss: 0.026146\n",
      "Training epoch: 13 [160/900 (18%)]\tLoss: 0.009686\n",
      "Training epoch: 13 [320/900 (35%)]\tLoss: 0.009119\n",
      "Training epoch: 13 [480/900 (53%)]\tLoss: 0.003686\n",
      "Training epoch: 13 [640/900 (70%)]\tLoss: 0.004510\n",
      "Training epoch: 13 [800/900 (88%)]\tLoss: 0.006820\n",
      "=========> Epoch: 13 Average loss: 0.0072\n",
      "Correlation coefficient: 0.6164\n",
      "Training epoch: 14 [0/900 (0%)]\tLoss: 0.012778\n",
      "Training epoch: 14 [160/900 (18%)]\tLoss: 0.002134\n",
      "Training epoch: 14 [320/900 (35%)]\tLoss: 0.012704\n",
      "Training epoch: 14 [480/900 (53%)]\tLoss: 0.009369\n",
      "Training epoch: 14 [640/900 (70%)]\tLoss: 0.005060\n",
      "Training epoch: 14 [800/900 (88%)]\tLoss: 0.004063\n",
      "=========> Epoch: 14 Average loss: 0.0085\n",
      "Correlation coefficient: 0.6167\n",
      "Training epoch: 15 [0/900 (0%)]\tLoss: 0.002177\n",
      "Training epoch: 15 [160/900 (18%)]\tLoss: 0.009407\n",
      "Training epoch: 15 [320/900 (35%)]\tLoss: 0.006003\n",
      "Training epoch: 15 [480/900 (53%)]\tLoss: 0.007169\n",
      "Training epoch: 15 [640/900 (70%)]\tLoss: 0.005820\n",
      "Training epoch: 15 [800/900 (88%)]\tLoss: 0.015107\n",
      "=========> Epoch: 15 Average loss: 0.0079\n",
      "Correlation coefficient: 0.6237\n",
      "Training epoch: 16 [0/900 (0%)]\tLoss: 0.008131\n",
      "Training epoch: 16 [160/900 (18%)]\tLoss: 0.019764\n",
      "Training epoch: 16 [320/900 (35%)]\tLoss: 0.012393\n",
      "Training epoch: 16 [480/900 (53%)]\tLoss: 0.004135\n",
      "Training epoch: 16 [640/900 (70%)]\tLoss: 0.006298\n",
      "Training epoch: 16 [800/900 (88%)]\tLoss: 0.004199\n",
      "=========> Epoch: 16 Average loss: 0.0077\n",
      "Correlation coefficient: 0.6186\n",
      "Training epoch: 17 [0/900 (0%)]\tLoss: 0.003289\n",
      "Training epoch: 17 [160/900 (18%)]\tLoss: 0.001650\n",
      "Training epoch: 17 [320/900 (35%)]\tLoss: 0.007165\n",
      "Training epoch: 17 [480/900 (53%)]\tLoss: 0.021929\n",
      "Training epoch: 17 [640/900 (70%)]\tLoss: 0.003765\n",
      "Training epoch: 17 [800/900 (88%)]\tLoss: 0.004266\n",
      "=========> Epoch: 17 Average loss: 0.0086\n",
      "Correlation coefficient: 0.6204\n",
      "Training epoch: 18 [0/900 (0%)]\tLoss: 0.014230\n",
      "Training epoch: 18 [160/900 (18%)]\tLoss: 0.018522\n",
      "Training epoch: 18 [320/900 (35%)]\tLoss: 0.010081\n",
      "Training epoch: 18 [480/900 (53%)]\tLoss: 0.004902\n",
      "Training epoch: 18 [640/900 (70%)]\tLoss: 0.006082\n",
      "Training epoch: 18 [800/900 (88%)]\tLoss: 0.006399\n",
      "=========> Epoch: 18 Average loss: 0.0119\n",
      "Correlation coefficient: 0.6072\n",
      "Training epoch: 19 [0/900 (0%)]\tLoss: 0.008712\n",
      "Training epoch: 19 [160/900 (18%)]\tLoss: 0.013663\n",
      "Training epoch: 19 [320/900 (35%)]\tLoss: 0.015480\n",
      "Training epoch: 19 [480/900 (53%)]\tLoss: 0.008287\n",
      "Training epoch: 19 [640/900 (70%)]\tLoss: 0.010771\n",
      "Training epoch: 19 [800/900 (88%)]\tLoss: 0.011661\n",
      "=========> Epoch: 19 Average loss: 0.0156\n",
      "Correlation coefficient: 0.6083\n",
      "Training epoch: 20 [0/900 (0%)]\tLoss: 0.009389\n",
      "Training epoch: 20 [160/900 (18%)]\tLoss: 0.007867\n",
      "Training epoch: 20 [320/900 (35%)]\tLoss: 0.020025\n",
      "Training epoch: 20 [480/900 (53%)]\tLoss: 0.014987\n",
      "Training epoch: 20 [640/900 (70%)]\tLoss: 0.009859\n",
      "Training epoch: 20 [800/900 (88%)]\tLoss: 0.015476\n",
      "=========> Epoch: 20 Average loss: 0.0152\n",
      "Correlation coefficient: 0.6282\n",
      "Training epoch: 21 [0/900 (0%)]\tLoss: 0.012575\n",
      "Training epoch: 21 [160/900 (18%)]\tLoss: 0.013551\n",
      "Training epoch: 21 [320/900 (35%)]\tLoss: 0.018395\n",
      "Training epoch: 21 [480/900 (53%)]\tLoss: 0.021550\n",
      "Training epoch: 21 [640/900 (70%)]\tLoss: 0.025050\n",
      "Training epoch: 21 [800/900 (88%)]\tLoss: 0.015884\n",
      "=========> Epoch: 21 Average loss: 0.0168\n",
      "Correlation coefficient: 0.6143\n",
      "Training epoch: 22 [0/900 (0%)]\tLoss: 0.009527\n",
      "Training epoch: 22 [160/900 (18%)]\tLoss: 0.017793\n",
      "Training epoch: 22 [320/900 (35%)]\tLoss: 0.012422\n",
      "Training epoch: 22 [480/900 (53%)]\tLoss: 0.019319\n",
      "Training epoch: 22 [640/900 (70%)]\tLoss: 0.007010\n",
      "Training epoch: 22 [800/900 (88%)]\tLoss: 0.007522\n",
      "=========> Epoch: 22 Average loss: 0.0152\n",
      "Correlation coefficient: 0.6144\n",
      "Training epoch: 23 [0/900 (0%)]\tLoss: 0.019147\n",
      "Training epoch: 23 [160/900 (18%)]\tLoss: 0.012988\n",
      "Training epoch: 23 [320/900 (35%)]\tLoss: 0.010668\n",
      "Training epoch: 23 [480/900 (53%)]\tLoss: 0.024176\n",
      "Training epoch: 23 [640/900 (70%)]\tLoss: 0.007318\n",
      "Training epoch: 23 [800/900 (88%)]\tLoss: 0.017541\n",
      "=========> Epoch: 23 Average loss: 0.0142\n",
      "Correlation coefficient: 0.6135\n",
      "Training epoch: 24 [0/900 (0%)]\tLoss: 0.002787\n",
      "Training epoch: 24 [160/900 (18%)]\tLoss: 0.009013\n",
      "Training epoch: 24 [320/900 (35%)]\tLoss: 0.011666\n",
      "Training epoch: 24 [480/900 (53%)]\tLoss: 0.003985\n",
      "Training epoch: 24 [640/900 (70%)]\tLoss: 0.012512\n",
      "Training epoch: 24 [800/900 (88%)]\tLoss: 0.012069\n",
      "=========> Epoch: 24 Average loss: 0.0116\n",
      "Correlation coefficient: 0.6107\n",
      "Training epoch: 25 [0/900 (0%)]\tLoss: 0.007181\n",
      "Training epoch: 25 [160/900 (18%)]\tLoss: 0.008377\n",
      "Training epoch: 25 [320/900 (35%)]\tLoss: 0.007279\n",
      "Training epoch: 25 [480/900 (53%)]\tLoss: 0.016103\n",
      "Training epoch: 25 [640/900 (70%)]\tLoss: 0.009911\n",
      "Training epoch: 25 [800/900 (88%)]\tLoss: 0.006306\n",
      "=========> Epoch: 25 Average loss: 0.0128\n",
      "Correlation coefficient: 0.6096\n",
      "Training epoch: 26 [0/900 (0%)]\tLoss: 0.012706\n",
      "Training epoch: 26 [160/900 (18%)]\tLoss: 0.016809\n",
      "Training epoch: 26 [320/900 (35%)]\tLoss: 0.017179\n",
      "Training epoch: 26 [480/900 (53%)]\tLoss: 0.013866\n",
      "Training epoch: 26 [640/900 (70%)]\tLoss: 0.005608\n",
      "Training epoch: 26 [800/900 (88%)]\tLoss: 0.012221\n",
      "=========> Epoch: 26 Average loss: 0.0122\n",
      "Correlation coefficient: 0.6215\n",
      "Training epoch: 27 [0/900 (0%)]\tLoss: 0.017743\n",
      "Training epoch: 27 [160/900 (18%)]\tLoss: 0.003160\n",
      "Training epoch: 27 [320/900 (35%)]\tLoss: 0.008030\n",
      "Training epoch: 27 [480/900 (53%)]\tLoss: 0.011540\n",
      "Training epoch: 27 [640/900 (70%)]\tLoss: 0.023891\n",
      "Training epoch: 27 [800/900 (88%)]\tLoss: 0.005873\n",
      "=========> Epoch: 27 Average loss: 0.0107\n",
      "Correlation coefficient: 0.6168\n",
      "Training epoch: 28 [0/900 (0%)]\tLoss: 0.006483\n",
      "Training epoch: 28 [160/900 (18%)]\tLoss: 0.004937\n",
      "Training epoch: 28 [320/900 (35%)]\tLoss: 0.003146\n",
      "Training epoch: 28 [480/900 (53%)]\tLoss: 0.005080\n",
      "Training epoch: 28 [640/900 (70%)]\tLoss: 0.012715\n",
      "Training epoch: 28 [800/900 (88%)]\tLoss: 0.039860\n",
      "=========> Epoch: 28 Average loss: 0.0098\n",
      "Correlation coefficient: 0.6141\n",
      "Training epoch: 29 [0/900 (0%)]\tLoss: 0.004380\n",
      "Training epoch: 29 [160/900 (18%)]\tLoss: 0.009866\n",
      "Training epoch: 29 [320/900 (35%)]\tLoss: 0.007493\n",
      "Training epoch: 29 [480/900 (53%)]\tLoss: 0.003770\n",
      "Training epoch: 29 [640/900 (70%)]\tLoss: 0.003861\n",
      "Training epoch: 29 [800/900 (88%)]\tLoss: 0.010736\n",
      "=========> Epoch: 29 Average loss: 0.0073\n",
      "Correlation coefficient: 0.6125\n",
      "Training epoch: 30 [0/900 (0%)]\tLoss: 0.008811\n",
      "Training epoch: 30 [160/900 (18%)]\tLoss: 0.005259\n",
      "Training epoch: 30 [320/900 (35%)]\tLoss: 0.002887\n",
      "Training epoch: 30 [480/900 (53%)]\tLoss: 0.007628\n",
      "Training epoch: 30 [640/900 (70%)]\tLoss: 0.008263\n",
      "Training epoch: 30 [800/900 (88%)]\tLoss: 0.011581\n",
      "=========> Epoch: 30 Average loss: 0.0078\n",
      "Correlation coefficient: 0.6170\n",
      "Training epoch: 31 [0/900 (0%)]\tLoss: 0.007188\n",
      "Training epoch: 31 [160/900 (18%)]\tLoss: 0.017493\n",
      "Training epoch: 31 [320/900 (35%)]\tLoss: 0.005268\n",
      "Training epoch: 31 [480/900 (53%)]\tLoss: 0.028229\n",
      "Training epoch: 31 [640/900 (70%)]\tLoss: 0.002471\n",
      "Training epoch: 31 [800/900 (88%)]\tLoss: 0.008743\n",
      "=========> Epoch: 31 Average loss: 0.0111\n",
      "Correlation coefficient: 0.6184\n",
      "Training epoch: 32 [0/900 (0%)]\tLoss: 0.014405\n",
      "Training epoch: 32 [160/900 (18%)]\tLoss: 0.012473\n",
      "Training epoch: 32 [320/900 (35%)]\tLoss: 0.011804\n",
      "Training epoch: 32 [480/900 (53%)]\tLoss: 0.009260\n",
      "Training epoch: 32 [640/900 (70%)]\tLoss: 0.012549\n",
      "Training epoch: 32 [800/900 (88%)]\tLoss: 0.017588\n",
      "=========> Epoch: 32 Average loss: 0.0141\n",
      "Correlation coefficient: 0.6093\n",
      "Training epoch: 33 [0/900 (0%)]\tLoss: 0.002965\n",
      "Training epoch: 33 [160/900 (18%)]\tLoss: 0.006812\n",
      "Training epoch: 33 [320/900 (35%)]\tLoss: 0.003850\n",
      "Training epoch: 33 [480/900 (53%)]\tLoss: 0.020033\n",
      "Training epoch: 33 [640/900 (70%)]\tLoss: 0.007230\n",
      "Training epoch: 33 [800/900 (88%)]\tLoss: 0.010779\n",
      "=========> Epoch: 33 Average loss: 0.0147\n",
      "Correlation coefficient: 0.6159\n",
      "Training epoch: 34 [0/900 (0%)]\tLoss: 0.012313\n",
      "Training epoch: 34 [160/900 (18%)]\tLoss: 0.016254\n",
      "Training epoch: 34 [320/900 (35%)]\tLoss: 0.020888\n",
      "Training epoch: 34 [480/900 (53%)]\tLoss: 0.014309\n",
      "Training epoch: 34 [640/900 (70%)]\tLoss: 0.010690\n",
      "Training epoch: 34 [800/900 (88%)]\tLoss: 0.016745\n",
      "=========> Epoch: 34 Average loss: 0.0237\n",
      "Correlation coefficient: 0.6258\n",
      "Training epoch: 35 [0/900 (0%)]\tLoss: 0.038338\n",
      "Training epoch: 35 [160/900 (18%)]\tLoss: 0.043634\n",
      "Training epoch: 35 [320/900 (35%)]\tLoss: 0.018100\n",
      "Training epoch: 35 [480/900 (53%)]\tLoss: 0.039083\n",
      "Training epoch: 35 [640/900 (70%)]\tLoss: 0.015586\n",
      "Training epoch: 35 [800/900 (88%)]\tLoss: 0.041069\n",
      "=========> Epoch: 35 Average loss: 0.0307\n",
      "Correlation coefficient: 0.6005\n",
      "Training epoch: 36 [0/900 (0%)]\tLoss: 0.083384\n",
      "Training epoch: 36 [160/900 (18%)]\tLoss: 0.012830\n",
      "Training epoch: 36 [320/900 (35%)]\tLoss: 0.026251\n",
      "Training epoch: 36 [480/900 (53%)]\tLoss: 0.048694\n",
      "Training epoch: 36 [640/900 (70%)]\tLoss: 0.035551\n",
      "Training epoch: 36 [800/900 (88%)]\tLoss: 0.003293\n",
      "=========> Epoch: 36 Average loss: 0.0234\n",
      "Correlation coefficient: 0.6171\n",
      "Training epoch: 37 [0/900 (0%)]\tLoss: 0.012655\n",
      "Training epoch: 37 [160/900 (18%)]\tLoss: 0.013824\n",
      "Training epoch: 37 [320/900 (35%)]\tLoss: 0.043560\n",
      "Training epoch: 37 [480/900 (53%)]\tLoss: 0.009067\n",
      "Training epoch: 37 [640/900 (70%)]\tLoss: 0.021600\n",
      "Training epoch: 37 [800/900 (88%)]\tLoss: 0.018520\n",
      "=========> Epoch: 37 Average loss: 0.0182\n",
      "Correlation coefficient: 0.6068\n",
      "Training epoch: 38 [0/900 (0%)]\tLoss: 0.005149\n",
      "Training epoch: 38 [160/900 (18%)]\tLoss: 0.005316\n",
      "Training epoch: 38 [320/900 (35%)]\tLoss: 0.013276\n",
      "Training epoch: 38 [480/900 (53%)]\tLoss: 0.006592\n",
      "Training epoch: 38 [640/900 (70%)]\tLoss: 0.018416\n",
      "Training epoch: 38 [800/900 (88%)]\tLoss: 0.008525\n",
      "=========> Epoch: 38 Average loss: 0.0133\n",
      "Correlation coefficient: 0.6179\n",
      "Training epoch: 39 [0/900 (0%)]\tLoss: 0.004315\n",
      "Training epoch: 39 [160/900 (18%)]\tLoss: 0.006281\n",
      "Training epoch: 39 [320/900 (35%)]\tLoss: 0.008443\n",
      "Training epoch: 39 [480/900 (53%)]\tLoss: 0.008354\n",
      "Training epoch: 39 [640/900 (70%)]\tLoss: 0.013395\n",
      "Training epoch: 39 [800/900 (88%)]\tLoss: 0.006841\n",
      "=========> Epoch: 39 Average loss: 0.0119\n",
      "Correlation coefficient: 0.6217\n",
      "Training epoch: 40 [0/900 (0%)]\tLoss: 0.007595\n",
      "Training epoch: 40 [160/900 (18%)]\tLoss: 0.002026\n",
      "Training epoch: 40 [320/900 (35%)]\tLoss: 0.004542\n",
      "Training epoch: 40 [480/900 (53%)]\tLoss: 0.003776\n",
      "Training epoch: 40 [640/900 (70%)]\tLoss: 0.007262\n",
      "Training epoch: 40 [800/900 (88%)]\tLoss: 0.006291\n",
      "=========> Epoch: 40 Average loss: 0.0076\n",
      "Correlation coefficient: 0.6106\n",
      "Training epoch: 41 [0/900 (0%)]\tLoss: 0.002562\n",
      "Training epoch: 41 [160/900 (18%)]\tLoss: 0.008005\n",
      "Training epoch: 41 [320/900 (35%)]\tLoss: 0.004132\n",
      "Training epoch: 41 [480/900 (53%)]\tLoss: 0.005330\n",
      "Training epoch: 41 [640/900 (70%)]\tLoss: 0.006618\n",
      "Training epoch: 41 [800/900 (88%)]\tLoss: 0.007100\n",
      "=========> Epoch: 41 Average loss: 0.0071\n",
      "Correlation coefficient: 0.6251\n",
      "Training epoch: 42 [0/900 (0%)]\tLoss: 0.003143\n",
      "Training epoch: 42 [160/900 (18%)]\tLoss: 0.013738\n",
      "Training epoch: 42 [320/900 (35%)]\tLoss: 0.003538\n",
      "Training epoch: 42 [480/900 (53%)]\tLoss: 0.033936\n",
      "Training epoch: 42 [640/900 (70%)]\tLoss: 0.003051\n",
      "Training epoch: 42 [800/900 (88%)]\tLoss: 0.018363\n",
      "=========> Epoch: 42 Average loss: 0.0083\n",
      "Correlation coefficient: 0.6206\n",
      "Training epoch: 43 [0/900 (0%)]\tLoss: 0.003394\n",
      "Training epoch: 43 [160/900 (18%)]\tLoss: 0.008448\n",
      "Training epoch: 43 [320/900 (35%)]\tLoss: 0.006002\n",
      "Training epoch: 43 [480/900 (53%)]\tLoss: 0.003076\n",
      "Training epoch: 43 [640/900 (70%)]\tLoss: 0.005573\n",
      "Training epoch: 43 [800/900 (88%)]\tLoss: 0.009157\n",
      "=========> Epoch: 43 Average loss: 0.0083\n",
      "Correlation coefficient: 0.6230\n",
      "Training epoch: 44 [0/900 (0%)]\tLoss: 0.002651\n",
      "Training epoch: 44 [160/900 (18%)]\tLoss: 0.004253\n",
      "Training epoch: 44 [320/900 (35%)]\tLoss: 0.004020\n",
      "Training epoch: 44 [480/900 (53%)]\tLoss: 0.003365\n",
      "Training epoch: 44 [640/900 (70%)]\tLoss: 0.004722\n",
      "Training epoch: 44 [800/900 (88%)]\tLoss: 0.002203\n",
      "=========> Epoch: 44 Average loss: 0.0063\n",
      "Correlation coefficient: 0.6216\n",
      "Training epoch: 45 [0/900 (0%)]\tLoss: 0.006463\n",
      "Training epoch: 45 [160/900 (18%)]\tLoss: 0.007881\n",
      "Training epoch: 45 [320/900 (35%)]\tLoss: 0.013030\n",
      "Training epoch: 45 [480/900 (53%)]\tLoss: 0.005360\n",
      "Training epoch: 45 [640/900 (70%)]\tLoss: 0.002633\n",
      "Training epoch: 45 [800/900 (88%)]\tLoss: 0.005602\n",
      "=========> Epoch: 45 Average loss: 0.0065\n",
      "Correlation coefficient: 0.6151\n",
      "Training epoch: 46 [0/900 (0%)]\tLoss: 0.002627\n",
      "Training epoch: 46 [160/900 (18%)]\tLoss: 0.008179\n",
      "Training epoch: 46 [320/900 (35%)]\tLoss: 0.006469\n",
      "Training epoch: 46 [480/900 (53%)]\tLoss: 0.012213\n",
      "Training epoch: 46 [640/900 (70%)]\tLoss: 0.012976\n",
      "Training epoch: 46 [800/900 (88%)]\tLoss: 0.006188\n",
      "=========> Epoch: 46 Average loss: 0.0078\n",
      "Correlation coefficient: 0.6187\n",
      "Training epoch: 47 [0/900 (0%)]\tLoss: 0.005425\n",
      "Training epoch: 47 [160/900 (18%)]\tLoss: 0.009553\n",
      "Training epoch: 47 [320/900 (35%)]\tLoss: 0.005849\n",
      "Training epoch: 47 [480/900 (53%)]\tLoss: 0.002835\n",
      "Training epoch: 47 [640/900 (70%)]\tLoss: 0.004971\n",
      "Training epoch: 47 [800/900 (88%)]\tLoss: 0.003437\n",
      "=========> Epoch: 47 Average loss: 0.0073\n",
      "Correlation coefficient: 0.6201\n",
      "Training epoch: 48 [0/900 (0%)]\tLoss: 0.003114\n",
      "Training epoch: 48 [160/900 (18%)]\tLoss: 0.012060\n",
      "Training epoch: 48 [320/900 (35%)]\tLoss: 0.010665\n",
      "Training epoch: 48 [480/900 (53%)]\tLoss: 0.022026\n",
      "Training epoch: 48 [640/900 (70%)]\tLoss: 0.009781\n",
      "Training epoch: 48 [800/900 (88%)]\tLoss: 0.003306\n",
      "=========> Epoch: 48 Average loss: 0.0066\n",
      "Correlation coefficient: 0.6185\n",
      "Training epoch: 49 [0/900 (0%)]\tLoss: 0.001753\n",
      "Training epoch: 49 [160/900 (18%)]\tLoss: 0.003318\n",
      "Training epoch: 49 [320/900 (35%)]\tLoss: 0.003343\n",
      "Training epoch: 49 [480/900 (53%)]\tLoss: 0.001513\n",
      "Training epoch: 49 [640/900 (70%)]\tLoss: 0.003879\n",
      "Training epoch: 49 [800/900 (88%)]\tLoss: 0.001996\n",
      "=========> Epoch: 49 Average loss: 0.0056\n",
      "Correlation coefficient: 0.6113\n",
      "Training epoch: 50 [0/900 (0%)]\tLoss: 0.005270\n",
      "Training epoch: 50 [160/900 (18%)]\tLoss: 0.001979\n",
      "Training epoch: 50 [320/900 (35%)]\tLoss: 0.003459\n",
      "Training epoch: 50 [480/900 (53%)]\tLoss: 0.007300\n",
      "Training epoch: 50 [640/900 (70%)]\tLoss: 0.002117\n",
      "Training epoch: 50 [800/900 (88%)]\tLoss: 0.014776\n",
      "=========> Epoch: 50 Average loss: 0.0046\n",
      "Correlation coefficient: 0.6153\n",
      "Training epoch: 51 [0/900 (0%)]\tLoss: 0.006971\n",
      "Training epoch: 51 [160/900 (18%)]\tLoss: 0.002587\n",
      "Training epoch: 51 [320/900 (35%)]\tLoss: 0.005766\n",
      "Training epoch: 51 [480/900 (53%)]\tLoss: 0.004581\n",
      "Training epoch: 51 [640/900 (70%)]\tLoss: 0.003342\n",
      "Training epoch: 51 [800/900 (88%)]\tLoss: 0.001592\n",
      "=========> Epoch: 51 Average loss: 0.0042\n",
      "Correlation coefficient: 0.6170\n",
      "⏹️  Epoch 51 early stopping (no improvement for 50 epochs)\n",
      "🏁 Fold 8 best correlation: 0.6564\n",
      "\n",
      "========== Cross-validation Fold 9/10 ==========\n",
      "🔄 Fold 9: Using random initialization\n",
      "Training epoch: 1 [0/900 (0%)]\tLoss: 0.852224\n",
      "Training epoch: 1 [160/900 (18%)]\tLoss: 0.277997\n",
      "Training epoch: 1 [320/900 (35%)]\tLoss: 0.560715\n",
      "Training epoch: 1 [480/900 (53%)]\tLoss: 0.535253\n",
      "Training epoch: 1 [640/900 (70%)]\tLoss: 0.597972\n",
      "Training epoch: 1 [800/900 (88%)]\tLoss: 0.516010\n",
      "=========> Epoch: 1 Average loss: 0.7487\n",
      "Correlation coefficient: 0.6482\n",
      "✅ Epoch 1: New best correlation = 0.6482\n",
      "Training epoch: 2 [0/900 (0%)]\tLoss: 0.443186\n",
      "Training epoch: 2 [160/900 (18%)]\tLoss: 0.238096\n",
      "Training epoch: 2 [320/900 (35%)]\tLoss: 0.547761\n",
      "Training epoch: 2 [480/900 (53%)]\tLoss: 0.244476\n",
      "Training epoch: 2 [640/900 (70%)]\tLoss: 0.329533\n",
      "Training epoch: 2 [800/900 (88%)]\tLoss: 0.283087\n",
      "=========> Epoch: 2 Average loss: 0.3454\n",
      "Correlation coefficient: 0.5992\n",
      "Training epoch: 3 [0/900 (0%)]\tLoss: 0.057533\n",
      "Training epoch: 3 [160/900 (18%)]\tLoss: 0.205631\n",
      "Training epoch: 3 [320/900 (35%)]\tLoss: 0.175965\n",
      "Training epoch: 3 [480/900 (53%)]\tLoss: 0.062910\n",
      "Training epoch: 3 [640/900 (70%)]\tLoss: 0.044345\n",
      "Training epoch: 3 [800/900 (88%)]\tLoss: 0.143131\n",
      "=========> Epoch: 3 Average loss: 0.1178\n",
      "Correlation coefficient: 0.5988\n",
      "Training epoch: 4 [0/900 (0%)]\tLoss: 0.149978\n",
      "Training epoch: 4 [160/900 (18%)]\tLoss: 0.038391\n",
      "Training epoch: 4 [320/900 (35%)]\tLoss: 0.056916\n",
      "Training epoch: 4 [480/900 (53%)]\tLoss: 0.038413\n",
      "Training epoch: 4 [640/900 (70%)]\tLoss: 0.065294\n",
      "Training epoch: 4 [800/900 (88%)]\tLoss: 0.057013\n",
      "=========> Epoch: 4 Average loss: 0.0597\n",
      "Correlation coefficient: 0.6132\n",
      "Training epoch: 5 [0/900 (0%)]\tLoss: 0.027617\n",
      "Training epoch: 5 [160/900 (18%)]\tLoss: 0.053273\n",
      "Training epoch: 5 [320/900 (35%)]\tLoss: 0.024576\n",
      "Training epoch: 5 [480/900 (53%)]\tLoss: 0.042351\n",
      "Training epoch: 5 [640/900 (70%)]\tLoss: 0.024994\n",
      "Training epoch: 5 [800/900 (88%)]\tLoss: 0.047791\n",
      "=========> Epoch: 5 Average loss: 0.0303\n",
      "Correlation coefficient: 0.6147\n",
      "Training epoch: 6 [0/900 (0%)]\tLoss: 0.015753\n",
      "Training epoch: 6 [160/900 (18%)]\tLoss: 0.004947\n",
      "Training epoch: 6 [320/900 (35%)]\tLoss: 0.011168\n",
      "Training epoch: 6 [480/900 (53%)]\tLoss: 0.015663\n",
      "Training epoch: 6 [640/900 (70%)]\tLoss: 0.005361\n",
      "Training epoch: 6 [800/900 (88%)]\tLoss: 0.018799\n",
      "=========> Epoch: 6 Average loss: 0.0157\n",
      "Correlation coefficient: 0.6273\n",
      "Training epoch: 7 [0/900 (0%)]\tLoss: 0.029763\n",
      "Training epoch: 7 [160/900 (18%)]\tLoss: 0.008614\n",
      "Training epoch: 7 [320/900 (35%)]\tLoss: 0.009127\n",
      "Training epoch: 7 [480/900 (53%)]\tLoss: 0.012306\n",
      "Training epoch: 7 [640/900 (70%)]\tLoss: 0.005297\n",
      "Training epoch: 7 [800/900 (88%)]\tLoss: 0.022210\n",
      "=========> Epoch: 7 Average loss: 0.0122\n",
      "Correlation coefficient: 0.6313\n",
      "Training epoch: 8 [0/900 (0%)]\tLoss: 0.010356\n",
      "Training epoch: 8 [160/900 (18%)]\tLoss: 0.006122\n",
      "Training epoch: 8 [320/900 (35%)]\tLoss: 0.008801\n",
      "Training epoch: 8 [480/900 (53%)]\tLoss: 0.010322\n",
      "Training epoch: 8 [640/900 (70%)]\tLoss: 0.013940\n",
      "Training epoch: 8 [800/900 (88%)]\tLoss: 0.005357\n",
      "=========> Epoch: 8 Average loss: 0.0134\n",
      "Correlation coefficient: 0.6229\n",
      "Training epoch: 9 [0/900 (0%)]\tLoss: 0.009678\n",
      "Training epoch: 9 [160/900 (18%)]\tLoss: 0.023185\n",
      "Training epoch: 9 [320/900 (35%)]\tLoss: 0.008684\n",
      "Training epoch: 9 [480/900 (53%)]\tLoss: 0.012922\n",
      "Training epoch: 9 [640/900 (70%)]\tLoss: 0.020679\n",
      "Training epoch: 9 [800/900 (88%)]\tLoss: 0.010975\n",
      "=========> Epoch: 9 Average loss: 0.0105\n",
      "Correlation coefficient: 0.6303\n",
      "Training epoch: 10 [0/900 (0%)]\tLoss: 0.003945\n",
      "Training epoch: 10 [160/900 (18%)]\tLoss: 0.003108\n",
      "Training epoch: 10 [320/900 (35%)]\tLoss: 0.010363\n",
      "Training epoch: 10 [480/900 (53%)]\tLoss: 0.003432\n",
      "Training epoch: 10 [640/900 (70%)]\tLoss: 0.029053\n",
      "Training epoch: 10 [800/900 (88%)]\tLoss: 0.013441\n",
      "=========> Epoch: 10 Average loss: 0.0077\n",
      "Correlation coefficient: 0.6250\n",
      "Training epoch: 11 [0/900 (0%)]\tLoss: 0.009369\n",
      "Training epoch: 11 [160/900 (18%)]\tLoss: 0.005306\n",
      "Training epoch: 11 [320/900 (35%)]\tLoss: 0.012164\n",
      "Training epoch: 11 [480/900 (53%)]\tLoss: 0.007984\n",
      "Training epoch: 11 [640/900 (70%)]\tLoss: 0.003882\n",
      "Training epoch: 11 [800/900 (88%)]\tLoss: 0.013707\n",
      "=========> Epoch: 11 Average loss: 0.0079\n",
      "Correlation coefficient: 0.6291\n",
      "Training epoch: 12 [0/900 (0%)]\tLoss: 0.011156\n",
      "Training epoch: 12 [160/900 (18%)]\tLoss: 0.007757\n",
      "Training epoch: 12 [320/900 (35%)]\tLoss: 0.003650\n",
      "Training epoch: 12 [480/900 (53%)]\tLoss: 0.011512\n",
      "Training epoch: 12 [640/900 (70%)]\tLoss: 0.006453\n",
      "Training epoch: 12 [800/900 (88%)]\tLoss: 0.010870\n",
      "=========> Epoch: 12 Average loss: 0.0091\n",
      "Correlation coefficient: 0.6329\n",
      "Training epoch: 13 [0/900 (0%)]\tLoss: 0.006827\n",
      "Training epoch: 13 [160/900 (18%)]\tLoss: 0.012388\n",
      "Training epoch: 13 [320/900 (35%)]\tLoss: 0.002118\n",
      "Training epoch: 13 [480/900 (53%)]\tLoss: 0.012141\n",
      "Training epoch: 13 [640/900 (70%)]\tLoss: 0.006571\n",
      "Training epoch: 13 [800/900 (88%)]\tLoss: 0.009988\n",
      "=========> Epoch: 13 Average loss: 0.0089\n",
      "Correlation coefficient: 0.6275\n",
      "Training epoch: 14 [0/900 (0%)]\tLoss: 0.008332\n",
      "Training epoch: 14 [160/900 (18%)]\tLoss: 0.006493\n",
      "Training epoch: 14 [320/900 (35%)]\tLoss: 0.010505\n",
      "Training epoch: 14 [480/900 (53%)]\tLoss: 0.011174\n",
      "Training epoch: 14 [640/900 (70%)]\tLoss: 0.010742\n",
      "Training epoch: 14 [800/900 (88%)]\tLoss: 0.004579\n",
      "=========> Epoch: 14 Average loss: 0.0091\n",
      "Correlation coefficient: 0.6291\n",
      "Training epoch: 15 [0/900 (0%)]\tLoss: 0.007379\n",
      "Training epoch: 15 [160/900 (18%)]\tLoss: 0.010983\n",
      "Training epoch: 15 [320/900 (35%)]\tLoss: 0.005188\n",
      "Training epoch: 15 [480/900 (53%)]\tLoss: 0.004797\n",
      "Training epoch: 15 [640/900 (70%)]\tLoss: 0.014071\n",
      "Training epoch: 15 [800/900 (88%)]\tLoss: 0.004352\n",
      "=========> Epoch: 15 Average loss: 0.0100\n",
      "Correlation coefficient: 0.6278\n",
      "Training epoch: 16 [0/900 (0%)]\tLoss: 0.005802\n",
      "Training epoch: 16 [160/900 (18%)]\tLoss: 0.005827\n",
      "Training epoch: 16 [320/900 (35%)]\tLoss: 0.004018\n",
      "Training epoch: 16 [480/900 (53%)]\tLoss: 0.022958\n",
      "Training epoch: 16 [640/900 (70%)]\tLoss: 0.005947\n",
      "Training epoch: 16 [800/900 (88%)]\tLoss: 0.006158\n",
      "=========> Epoch: 16 Average loss: 0.0077\n",
      "Correlation coefficient: 0.6384\n",
      "Training epoch: 17 [0/900 (0%)]\tLoss: 0.002998\n",
      "Training epoch: 17 [160/900 (18%)]\tLoss: 0.005269\n",
      "Training epoch: 17 [320/900 (35%)]\tLoss: 0.008302\n",
      "Training epoch: 17 [480/900 (53%)]\tLoss: 0.008144\n",
      "Training epoch: 17 [640/900 (70%)]\tLoss: 0.009554\n",
      "Training epoch: 17 [800/900 (88%)]\tLoss: 0.007205\n",
      "=========> Epoch: 17 Average loss: 0.0070\n",
      "Correlation coefficient: 0.6288\n",
      "Training epoch: 18 [0/900 (0%)]\tLoss: 0.004129\n",
      "Training epoch: 18 [160/900 (18%)]\tLoss: 0.005019\n",
      "Training epoch: 18 [320/900 (35%)]\tLoss: 0.006030\n",
      "Training epoch: 18 [480/900 (53%)]\tLoss: 0.005889\n",
      "Training epoch: 18 [640/900 (70%)]\tLoss: 0.005753\n",
      "Training epoch: 18 [800/900 (88%)]\tLoss: 0.001771\n",
      "=========> Epoch: 18 Average loss: 0.0072\n",
      "Correlation coefficient: 0.6442\n",
      "Training epoch: 19 [0/900 (0%)]\tLoss: 0.005167\n",
      "Training epoch: 19 [160/900 (18%)]\tLoss: 0.024253\n",
      "Training epoch: 19 [320/900 (35%)]\tLoss: 0.023030\n",
      "Training epoch: 19 [480/900 (53%)]\tLoss: 0.004414\n",
      "Training epoch: 19 [640/900 (70%)]\tLoss: 0.005436\n",
      "Training epoch: 19 [800/900 (88%)]\tLoss: 0.003646\n",
      "=========> Epoch: 19 Average loss: 0.0088\n",
      "Correlation coefficient: 0.6364\n",
      "Training epoch: 20 [0/900 (0%)]\tLoss: 0.040404\n",
      "Training epoch: 20 [160/900 (18%)]\tLoss: 0.012034\n",
      "Training epoch: 20 [320/900 (35%)]\tLoss: 0.018287\n",
      "Training epoch: 20 [480/900 (53%)]\tLoss: 0.014710\n",
      "Training epoch: 20 [640/900 (70%)]\tLoss: 0.007350\n",
      "Training epoch: 20 [800/900 (88%)]\tLoss: 0.013772\n",
      "=========> Epoch: 20 Average loss: 0.0109\n",
      "Correlation coefficient: 0.6412\n",
      "Training epoch: 21 [0/900 (0%)]\tLoss: 0.009898\n",
      "Training epoch: 21 [160/900 (18%)]\tLoss: 0.010964\n",
      "Training epoch: 21 [320/900 (35%)]\tLoss: 0.007782\n",
      "Training epoch: 21 [480/900 (53%)]\tLoss: 0.008717\n",
      "Training epoch: 21 [640/900 (70%)]\tLoss: 0.011137\n",
      "Training epoch: 21 [800/900 (88%)]\tLoss: 0.005108\n",
      "=========> Epoch: 21 Average loss: 0.0121\n",
      "Correlation coefficient: 0.6236\n",
      "Training epoch: 22 [0/900 (0%)]\tLoss: 0.005957\n",
      "Training epoch: 22 [160/900 (18%)]\tLoss: 0.024183\n",
      "Training epoch: 22 [320/900 (35%)]\tLoss: 0.007176\n",
      "Training epoch: 22 [480/900 (53%)]\tLoss: 0.010128\n",
      "Training epoch: 22 [640/900 (70%)]\tLoss: 0.014818\n",
      "Training epoch: 22 [800/900 (88%)]\tLoss: 0.010870\n",
      "=========> Epoch: 22 Average loss: 0.0131\n",
      "Correlation coefficient: 0.6354\n",
      "Training epoch: 23 [0/900 (0%)]\tLoss: 0.014534\n",
      "Training epoch: 23 [160/900 (18%)]\tLoss: 0.015391\n",
      "Training epoch: 23 [320/900 (35%)]\tLoss: 0.004489\n",
      "Training epoch: 23 [480/900 (53%)]\tLoss: 0.005885\n",
      "Training epoch: 23 [640/900 (70%)]\tLoss: 0.009141\n",
      "Training epoch: 23 [800/900 (88%)]\tLoss: 0.011576\n",
      "=========> Epoch: 23 Average loss: 0.0129\n",
      "Correlation coefficient: 0.6378\n",
      "Training epoch: 24 [0/900 (0%)]\tLoss: 0.005779\n",
      "Training epoch: 24 [160/900 (18%)]\tLoss: 0.018744\n",
      "Training epoch: 24 [320/900 (35%)]\tLoss: 0.007128\n",
      "Training epoch: 24 [480/900 (53%)]\tLoss: 0.011309\n",
      "Training epoch: 24 [640/900 (70%)]\tLoss: 0.022274\n",
      "Training epoch: 24 [800/900 (88%)]\tLoss: 0.005711\n",
      "=========> Epoch: 24 Average loss: 0.0135\n",
      "Correlation coefficient: 0.6412\n",
      "Training epoch: 25 [0/900 (0%)]\tLoss: 0.003403\n",
      "Training epoch: 25 [160/900 (18%)]\tLoss: 0.027107\n",
      "Training epoch: 25 [320/900 (35%)]\tLoss: 0.003690\n",
      "Training epoch: 25 [480/900 (53%)]\tLoss: 0.008809\n",
      "Training epoch: 25 [640/900 (70%)]\tLoss: 0.015671\n",
      "Training epoch: 25 [800/900 (88%)]\tLoss: 0.008188\n",
      "=========> Epoch: 25 Average loss: 0.0124\n",
      "Correlation coefficient: 0.6203\n",
      "Training epoch: 26 [0/900 (0%)]\tLoss: 0.016879\n",
      "Training epoch: 26 [160/900 (18%)]\tLoss: 0.015804\n",
      "Training epoch: 26 [320/900 (35%)]\tLoss: 0.020710\n",
      "Training epoch: 26 [480/900 (53%)]\tLoss: 0.014239\n",
      "Training epoch: 26 [640/900 (70%)]\tLoss: 0.070696\n",
      "Training epoch: 26 [800/900 (88%)]\tLoss: 0.024531\n",
      "=========> Epoch: 26 Average loss: 0.0160\n",
      "Correlation coefficient: 0.6297\n",
      "Training epoch: 27 [0/900 (0%)]\tLoss: 0.030764\n",
      "Training epoch: 27 [160/900 (18%)]\tLoss: 0.007504\n",
      "Training epoch: 27 [320/900 (35%)]\tLoss: 0.007512\n",
      "Training epoch: 27 [480/900 (53%)]\tLoss: 0.017143\n",
      "Training epoch: 27 [640/900 (70%)]\tLoss: 0.008595\n",
      "Training epoch: 27 [800/900 (88%)]\tLoss: 0.021100\n",
      "=========> Epoch: 27 Average loss: 0.0186\n",
      "Correlation coefficient: 0.6325\n",
      "Training epoch: 28 [0/900 (0%)]\tLoss: 0.024520\n",
      "Training epoch: 28 [160/900 (18%)]\tLoss: 0.016307\n",
      "Training epoch: 28 [320/900 (35%)]\tLoss: 0.015937\n",
      "Training epoch: 28 [480/900 (53%)]\tLoss: 0.012911\n",
      "Training epoch: 28 [640/900 (70%)]\tLoss: 0.015069\n",
      "Training epoch: 28 [800/900 (88%)]\tLoss: 0.009609\n",
      "=========> Epoch: 28 Average loss: 0.0163\n",
      "Correlation coefficient: 0.6414\n",
      "Training epoch: 29 [0/900 (0%)]\tLoss: 0.022487\n",
      "Training epoch: 29 [160/900 (18%)]\tLoss: 0.014659\n",
      "Training epoch: 29 [320/900 (35%)]\tLoss: 0.015360\n",
      "Training epoch: 29 [480/900 (53%)]\tLoss: 0.009099\n",
      "Training epoch: 29 [640/900 (70%)]\tLoss: 0.010641\n",
      "Training epoch: 29 [800/900 (88%)]\tLoss: 0.024355\n",
      "=========> Epoch: 29 Average loss: 0.0139\n",
      "Correlation coefficient: 0.6447\n",
      "Training epoch: 30 [0/900 (0%)]\tLoss: 0.012425\n",
      "Training epoch: 30 [160/900 (18%)]\tLoss: 0.026300\n",
      "Training epoch: 30 [320/900 (35%)]\tLoss: 0.006115\n",
      "Training epoch: 30 [480/900 (53%)]\tLoss: 0.014672\n",
      "Training epoch: 30 [640/900 (70%)]\tLoss: 0.004592\n",
      "Training epoch: 30 [800/900 (88%)]\tLoss: 0.056163\n",
      "=========> Epoch: 30 Average loss: 0.0129\n",
      "Correlation coefficient: 0.6414\n",
      "Training epoch: 31 [0/900 (0%)]\tLoss: 0.032673\n",
      "Training epoch: 31 [160/900 (18%)]\tLoss: 0.006388\n",
      "Training epoch: 31 [320/900 (35%)]\tLoss: 0.015453\n",
      "Training epoch: 31 [480/900 (53%)]\tLoss: 0.007162\n",
      "Training epoch: 31 [640/900 (70%)]\tLoss: 0.020425\n",
      "Training epoch: 31 [800/900 (88%)]\tLoss: 0.020479\n",
      "=========> Epoch: 31 Average loss: 0.0154\n",
      "Correlation coefficient: 0.6399\n",
      "Training epoch: 32 [0/900 (0%)]\tLoss: 0.017416\n",
      "Training epoch: 32 [160/900 (18%)]\tLoss: 0.014659\n",
      "Training epoch: 32 [320/900 (35%)]\tLoss: 0.014023\n",
      "Training epoch: 32 [480/900 (53%)]\tLoss: 0.010316\n",
      "Training epoch: 32 [640/900 (70%)]\tLoss: 0.023582\n",
      "Training epoch: 32 [800/900 (88%)]\tLoss: 0.010447\n",
      "=========> Epoch: 32 Average loss: 0.0180\n",
      "Correlation coefficient: 0.6464\n",
      "Training epoch: 33 [0/900 (0%)]\tLoss: 0.009748\n",
      "Training epoch: 33 [160/900 (18%)]\tLoss: 0.006292\n",
      "Training epoch: 33 [320/900 (35%)]\tLoss: 0.031535\n",
      "Training epoch: 33 [480/900 (53%)]\tLoss: 0.007410\n",
      "Training epoch: 33 [640/900 (70%)]\tLoss: 0.003350\n",
      "Training epoch: 33 [800/900 (88%)]\tLoss: 0.010758\n",
      "=========> Epoch: 33 Average loss: 0.0127\n",
      "Correlation coefficient: 0.6425\n",
      "Training epoch: 34 [0/900 (0%)]\tLoss: 0.007368\n",
      "Training epoch: 34 [160/900 (18%)]\tLoss: 0.017371\n",
      "Training epoch: 34 [320/900 (35%)]\tLoss: 0.016980\n",
      "Training epoch: 34 [480/900 (53%)]\tLoss: 0.004187\n",
      "Training epoch: 34 [640/900 (70%)]\tLoss: 0.006740\n",
      "Training epoch: 34 [800/900 (88%)]\tLoss: 0.002227\n",
      "=========> Epoch: 34 Average loss: 0.0112\n",
      "Correlation coefficient: 0.6422\n",
      "Training epoch: 35 [0/900 (0%)]\tLoss: 0.012385\n",
      "Training epoch: 35 [160/900 (18%)]\tLoss: 0.011586\n",
      "Training epoch: 35 [320/900 (35%)]\tLoss: 0.005419\n",
      "Training epoch: 35 [480/900 (53%)]\tLoss: 0.008092\n",
      "Training epoch: 35 [640/900 (70%)]\tLoss: 0.004884\n",
      "Training epoch: 35 [800/900 (88%)]\tLoss: 0.008791\n",
      "=========> Epoch: 35 Average loss: 0.0103\n",
      "Correlation coefficient: 0.6519\n",
      "✅ Epoch 35: New best correlation = 0.6519\n",
      "Training epoch: 36 [0/900 (0%)]\tLoss: 0.005155\n",
      "Training epoch: 36 [160/900 (18%)]\tLoss: 0.004229\n",
      "Training epoch: 36 [320/900 (35%)]\tLoss: 0.007209\n",
      "Training epoch: 36 [480/900 (53%)]\tLoss: 0.014466\n",
      "Training epoch: 36 [640/900 (70%)]\tLoss: 0.011814\n",
      "Training epoch: 36 [800/900 (88%)]\tLoss: 0.008826\n",
      "=========> Epoch: 36 Average loss: 0.0073\n",
      "Correlation coefficient: 0.6472\n",
      "Training epoch: 37 [0/900 (0%)]\tLoss: 0.001907\n",
      "Training epoch: 37 [160/900 (18%)]\tLoss: 0.002483\n",
      "Training epoch: 37 [320/900 (35%)]\tLoss: 0.005468\n",
      "Training epoch: 37 [480/900 (53%)]\tLoss: 0.008496\n",
      "Training epoch: 37 [640/900 (70%)]\tLoss: 0.004634\n",
      "Training epoch: 37 [800/900 (88%)]\tLoss: 0.010115\n",
      "=========> Epoch: 37 Average loss: 0.0051\n",
      "Correlation coefficient: 0.6483\n",
      "Training epoch: 38 [0/900 (0%)]\tLoss: 0.005434\n",
      "Training epoch: 38 [160/900 (18%)]\tLoss: 0.003912\n",
      "Training epoch: 38 [320/900 (35%)]\tLoss: 0.001698\n",
      "Training epoch: 38 [480/900 (53%)]\tLoss: 0.006173\n",
      "Training epoch: 38 [640/900 (70%)]\tLoss: 0.004512\n",
      "Training epoch: 38 [800/900 (88%)]\tLoss: 0.004165\n",
      "=========> Epoch: 38 Average loss: 0.0046\n",
      "Correlation coefficient: 0.6446\n",
      "Training epoch: 39 [0/900 (0%)]\tLoss: 0.008761\n",
      "Training epoch: 39 [160/900 (18%)]\tLoss: 0.001084\n",
      "Training epoch: 39 [320/900 (35%)]\tLoss: 0.001804\n",
      "Training epoch: 39 [480/900 (53%)]\tLoss: 0.003109\n",
      "Training epoch: 39 [640/900 (70%)]\tLoss: 0.001720\n",
      "Training epoch: 39 [800/900 (88%)]\tLoss: 0.005321\n",
      "=========> Epoch: 39 Average loss: 0.0035\n",
      "Correlation coefficient: 0.6491\n",
      "Training epoch: 40 [0/900 (0%)]\tLoss: 0.006979\n",
      "Training epoch: 40 [160/900 (18%)]\tLoss: 0.004406\n",
      "Training epoch: 40 [320/900 (35%)]\tLoss: 0.003173\n",
      "Training epoch: 40 [480/900 (53%)]\tLoss: 0.000919\n",
      "Training epoch: 40 [640/900 (70%)]\tLoss: 0.001261\n",
      "Training epoch: 40 [800/900 (88%)]\tLoss: 0.005772\n",
      "=========> Epoch: 40 Average loss: 0.0038\n",
      "Correlation coefficient: 0.6424\n",
      "Training epoch: 41 [0/900 (0%)]\tLoss: 0.003803\n",
      "Training epoch: 41 [160/900 (18%)]\tLoss: 0.003157\n",
      "Training epoch: 41 [320/900 (35%)]\tLoss: 0.006352\n",
      "Training epoch: 41 [480/900 (53%)]\tLoss: 0.004526\n",
      "Training epoch: 41 [640/900 (70%)]\tLoss: 0.003343\n",
      "Training epoch: 41 [800/900 (88%)]\tLoss: 0.001634\n",
      "=========> Epoch: 41 Average loss: 0.0049\n",
      "Correlation coefficient: 0.6508\n",
      "Training epoch: 42 [0/900 (0%)]\tLoss: 0.009347\n",
      "Training epoch: 42 [160/900 (18%)]\tLoss: 0.005228\n",
      "Training epoch: 42 [320/900 (35%)]\tLoss: 0.001722\n",
      "Training epoch: 42 [480/900 (53%)]\tLoss: 0.005281\n",
      "Training epoch: 42 [640/900 (70%)]\tLoss: 0.003478\n",
      "Training epoch: 42 [800/900 (88%)]\tLoss: 0.011403\n",
      "=========> Epoch: 42 Average loss: 0.0059\n",
      "Correlation coefficient: 0.6465\n",
      "Training epoch: 43 [0/900 (0%)]\tLoss: 0.003668\n",
      "Training epoch: 43 [160/900 (18%)]\tLoss: 0.021229\n",
      "Training epoch: 43 [320/900 (35%)]\tLoss: 0.016625\n",
      "Training epoch: 43 [480/900 (53%)]\tLoss: 0.004438\n",
      "Training epoch: 43 [640/900 (70%)]\tLoss: 0.011154\n",
      "Training epoch: 43 [800/900 (88%)]\tLoss: 0.012735\n",
      "=========> Epoch: 43 Average loss: 0.0088\n",
      "Correlation coefficient: 0.6402\n",
      "Training epoch: 44 [0/900 (0%)]\tLoss: 0.006987\n",
      "Training epoch: 44 [160/900 (18%)]\tLoss: 0.012912\n",
      "Training epoch: 44 [320/900 (35%)]\tLoss: 0.008636\n",
      "Training epoch: 44 [480/900 (53%)]\tLoss: 0.011643\n",
      "Training epoch: 44 [640/900 (70%)]\tLoss: 0.006725\n",
      "Training epoch: 44 [800/900 (88%)]\tLoss: 0.007411\n",
      "=========> Epoch: 44 Average loss: 0.0128\n",
      "Correlation coefficient: 0.6496\n",
      "Training epoch: 45 [0/900 (0%)]\tLoss: 0.010228\n",
      "Training epoch: 45 [160/900 (18%)]\tLoss: 0.007188\n",
      "Training epoch: 45 [320/900 (35%)]\tLoss: 0.004350\n",
      "Training epoch: 45 [480/900 (53%)]\tLoss: 0.014128\n",
      "Training epoch: 45 [640/900 (70%)]\tLoss: 0.004349\n",
      "Training epoch: 45 [800/900 (88%)]\tLoss: 0.007844\n",
      "=========> Epoch: 45 Average loss: 0.0108\n",
      "Correlation coefficient: 0.6415\n",
      "Training epoch: 46 [0/900 (0%)]\tLoss: 0.013576\n",
      "Training epoch: 46 [160/900 (18%)]\tLoss: 0.011371\n",
      "Training epoch: 46 [320/900 (35%)]\tLoss: 0.013140\n",
      "Training epoch: 46 [480/900 (53%)]\tLoss: 0.011322\n",
      "Training epoch: 46 [640/900 (70%)]\tLoss: 0.009351\n",
      "Training epoch: 46 [800/900 (88%)]\tLoss: 0.005601\n",
      "=========> Epoch: 46 Average loss: 0.0106\n",
      "Correlation coefficient: 0.6387\n",
      "Training epoch: 47 [0/900 (0%)]\tLoss: 0.009010\n",
      "Training epoch: 47 [160/900 (18%)]\tLoss: 0.014963\n",
      "Training epoch: 47 [320/900 (35%)]\tLoss: 0.006116\n",
      "Training epoch: 47 [480/900 (53%)]\tLoss: 0.012672\n",
      "Training epoch: 47 [640/900 (70%)]\tLoss: 0.011575\n",
      "Training epoch: 47 [800/900 (88%)]\tLoss: 0.017344\n",
      "=========> Epoch: 47 Average loss: 0.0121\n",
      "Correlation coefficient: 0.6319\n",
      "Training epoch: 48 [0/900 (0%)]\tLoss: 0.005675\n",
      "Training epoch: 48 [160/900 (18%)]\tLoss: 0.001926\n",
      "Training epoch: 48 [320/900 (35%)]\tLoss: 0.006539\n",
      "Training epoch: 48 [480/900 (53%)]\tLoss: 0.005666\n",
      "Training epoch: 48 [640/900 (70%)]\tLoss: 0.011433\n",
      "Training epoch: 48 [800/900 (88%)]\tLoss: 0.020695\n",
      "=========> Epoch: 48 Average loss: 0.0131\n",
      "Correlation coefficient: 0.6506\n",
      "Training epoch: 49 [0/900 (0%)]\tLoss: 0.008579\n",
      "Training epoch: 49 [160/900 (18%)]\tLoss: 0.013458\n",
      "Training epoch: 49 [320/900 (35%)]\tLoss: 0.012505\n",
      "Training epoch: 49 [480/900 (53%)]\tLoss: 0.012515\n",
      "Training epoch: 49 [640/900 (70%)]\tLoss: 0.006285\n",
      "Training epoch: 49 [800/900 (88%)]\tLoss: 0.005708\n",
      "=========> Epoch: 49 Average loss: 0.0162\n",
      "Correlation coefficient: 0.6407\n",
      "Training epoch: 50 [0/900 (0%)]\tLoss: 0.004390\n",
      "Training epoch: 50 [160/900 (18%)]\tLoss: 0.005478\n",
      "Training epoch: 50 [320/900 (35%)]\tLoss: 0.006883\n",
      "Training epoch: 50 [480/900 (53%)]\tLoss: 0.023341\n",
      "Training epoch: 50 [640/900 (70%)]\tLoss: 0.010005\n",
      "Training epoch: 50 [800/900 (88%)]\tLoss: 0.007583\n",
      "=========> Epoch: 50 Average loss: 0.0120\n",
      "Correlation coefficient: 0.6499\n",
      "Training epoch: 51 [0/900 (0%)]\tLoss: 0.027849\n",
      "Training epoch: 51 [160/900 (18%)]\tLoss: 0.007375\n",
      "Training epoch: 51 [320/900 (35%)]\tLoss: 0.009278\n",
      "Training epoch: 51 [480/900 (53%)]\tLoss: 0.008134\n",
      "Training epoch: 51 [640/900 (70%)]\tLoss: 0.017096\n",
      "Training epoch: 51 [800/900 (88%)]\tLoss: 0.004884\n",
      "=========> Epoch: 51 Average loss: 0.0119\n",
      "Correlation coefficient: 0.6506\n",
      "Training epoch: 52 [0/900 (0%)]\tLoss: 0.014692\n",
      "Training epoch: 52 [160/900 (18%)]\tLoss: 0.006238\n",
      "Training epoch: 52 [320/900 (35%)]\tLoss: 0.014133\n",
      "Training epoch: 52 [480/900 (53%)]\tLoss: 0.005600\n",
      "Training epoch: 52 [640/900 (70%)]\tLoss: 0.006962\n",
      "Training epoch: 52 [800/900 (88%)]\tLoss: 0.019928\n",
      "=========> Epoch: 52 Average loss: 0.0135\n",
      "Correlation coefficient: 0.6451\n",
      "Training epoch: 53 [0/900 (0%)]\tLoss: 0.004305\n",
      "Training epoch: 53 [160/900 (18%)]\tLoss: 0.020290\n",
      "Training epoch: 53 [320/900 (35%)]\tLoss: 0.008839\n",
      "Training epoch: 53 [480/900 (53%)]\tLoss: 0.014322\n",
      "Training epoch: 53 [640/900 (70%)]\tLoss: 0.007509\n",
      "Training epoch: 53 [800/900 (88%)]\tLoss: 0.012496\n",
      "=========> Epoch: 53 Average loss: 0.0121\n",
      "Correlation coefficient: 0.6477\n",
      "Training epoch: 54 [0/900 (0%)]\tLoss: 0.007225\n",
      "Training epoch: 54 [160/900 (18%)]\tLoss: 0.008046\n",
      "Training epoch: 54 [320/900 (35%)]\tLoss: 0.005107\n",
      "Training epoch: 54 [480/900 (53%)]\tLoss: 0.008529\n",
      "Training epoch: 54 [640/900 (70%)]\tLoss: 0.008381\n",
      "Training epoch: 54 [800/900 (88%)]\tLoss: 0.009670\n",
      "=========> Epoch: 54 Average loss: 0.0114\n",
      "Correlation coefficient: 0.6583\n",
      "✅ Epoch 54: New best correlation = 0.6583\n",
      "Training epoch: 55 [0/900 (0%)]\tLoss: 0.007368\n",
      "Training epoch: 55 [160/900 (18%)]\tLoss: 0.008637\n",
      "Training epoch: 55 [320/900 (35%)]\tLoss: 0.012841\n",
      "Training epoch: 55 [480/900 (53%)]\tLoss: 0.007112\n",
      "Training epoch: 55 [640/900 (70%)]\tLoss: 0.002803\n",
      "Training epoch: 55 [800/900 (88%)]\tLoss: 0.006860\n",
      "=========> Epoch: 55 Average loss: 0.0079\n",
      "Correlation coefficient: 0.6386\n",
      "Training epoch: 56 [0/900 (0%)]\tLoss: 0.004454\n",
      "Training epoch: 56 [160/900 (18%)]\tLoss: 0.012073\n",
      "Training epoch: 56 [320/900 (35%)]\tLoss: 0.007838\n",
      "Training epoch: 56 [480/900 (53%)]\tLoss: 0.011676\n",
      "Training epoch: 56 [640/900 (70%)]\tLoss: 0.008014\n",
      "Training epoch: 56 [800/900 (88%)]\tLoss: 0.007422\n",
      "=========> Epoch: 56 Average loss: 0.0082\n",
      "Correlation coefficient: 0.6512\n",
      "Training epoch: 57 [0/900 (0%)]\tLoss: 0.007867\n",
      "Training epoch: 57 [160/900 (18%)]\tLoss: 0.036454\n",
      "Training epoch: 57 [320/900 (35%)]\tLoss: 0.017540\n",
      "Training epoch: 57 [480/900 (53%)]\tLoss: 0.011903\n",
      "Training epoch: 57 [640/900 (70%)]\tLoss: 0.016866\n",
      "Training epoch: 57 [800/900 (88%)]\tLoss: 0.004273\n",
      "=========> Epoch: 57 Average loss: 0.0203\n",
      "Correlation coefficient: 0.6463\n",
      "Training epoch: 58 [0/900 (0%)]\tLoss: 0.011551\n",
      "Training epoch: 58 [160/900 (18%)]\tLoss: 0.017276\n",
      "Training epoch: 58 [320/900 (35%)]\tLoss: 0.029644\n",
      "Training epoch: 58 [480/900 (53%)]\tLoss: 0.013700\n",
      "Training epoch: 58 [640/900 (70%)]\tLoss: 0.018622\n",
      "Training epoch: 58 [800/900 (88%)]\tLoss: 0.027938\n",
      "=========> Epoch: 58 Average loss: 0.0225\n",
      "Correlation coefficient: 0.6503\n",
      "Training epoch: 59 [0/900 (0%)]\tLoss: 0.016160\n",
      "Training epoch: 59 [160/900 (18%)]\tLoss: 0.018788\n",
      "Training epoch: 59 [320/900 (35%)]\tLoss: 0.015421\n",
      "Training epoch: 59 [480/900 (53%)]\tLoss: 0.022299\n",
      "Training epoch: 59 [640/900 (70%)]\tLoss: 0.049807\n",
      "Training epoch: 59 [800/900 (88%)]\tLoss: 0.007025\n",
      "=========> Epoch: 59 Average loss: 0.0226\n",
      "Correlation coefficient: 0.6434\n",
      "Training epoch: 60 [0/900 (0%)]\tLoss: 0.021127\n",
      "Training epoch: 60 [160/900 (18%)]\tLoss: 0.008560\n",
      "Training epoch: 60 [320/900 (35%)]\tLoss: 0.029798\n",
      "Training epoch: 60 [480/900 (53%)]\tLoss: 0.052118\n",
      "Training epoch: 60 [640/900 (70%)]\tLoss: 0.015277\n",
      "Training epoch: 60 [800/900 (88%)]\tLoss: 0.067042\n",
      "=========> Epoch: 60 Average loss: 0.0219\n",
      "Correlation coefficient: 0.6579\n",
      "Training epoch: 61 [0/900 (0%)]\tLoss: 0.016086\n",
      "Training epoch: 61 [160/900 (18%)]\tLoss: 0.012652\n",
      "Training epoch: 61 [320/900 (35%)]\tLoss: 0.007357\n",
      "Training epoch: 61 [480/900 (53%)]\tLoss: 0.012094\n",
      "Training epoch: 61 [640/900 (70%)]\tLoss: 0.015238\n",
      "Training epoch: 61 [800/900 (88%)]\tLoss: 0.016907\n",
      "=========> Epoch: 61 Average loss: 0.0141\n",
      "Correlation coefficient: 0.6360\n",
      "Training epoch: 62 [0/900 (0%)]\tLoss: 0.011886\n",
      "Training epoch: 62 [160/900 (18%)]\tLoss: 0.005311\n",
      "Training epoch: 62 [320/900 (35%)]\tLoss: 0.008240\n",
      "Training epoch: 62 [480/900 (53%)]\tLoss: 0.011633\n",
      "Training epoch: 62 [640/900 (70%)]\tLoss: 0.005026\n",
      "Training epoch: 62 [800/900 (88%)]\tLoss: 0.013401\n",
      "=========> Epoch: 62 Average loss: 0.0078\n",
      "Correlation coefficient: 0.6599\n",
      "✅ Epoch 62: New best correlation = 0.6599\n",
      "Training epoch: 63 [0/900 (0%)]\tLoss: 0.006395\n",
      "Training epoch: 63 [160/900 (18%)]\tLoss: 0.007137\n",
      "Training epoch: 63 [320/900 (35%)]\tLoss: 0.004059\n",
      "Training epoch: 63 [480/900 (53%)]\tLoss: 0.005485\n",
      "Training epoch: 63 [640/900 (70%)]\tLoss: 0.004360\n",
      "Training epoch: 63 [800/900 (88%)]\tLoss: 0.001480\n",
      "=========> Epoch: 63 Average loss: 0.0050\n",
      "Correlation coefficient: 0.6472\n",
      "Training epoch: 64 [0/900 (0%)]\tLoss: 0.008242\n",
      "Training epoch: 64 [160/900 (18%)]\tLoss: 0.002319\n",
      "Training epoch: 64 [320/900 (35%)]\tLoss: 0.004127\n",
      "Training epoch: 64 [480/900 (53%)]\tLoss: 0.010337\n",
      "Training epoch: 64 [640/900 (70%)]\tLoss: 0.011637\n",
      "Training epoch: 64 [800/900 (88%)]\tLoss: 0.001884\n",
      "=========> Epoch: 64 Average loss: 0.0038\n",
      "Correlation coefficient: 0.6404\n",
      "Training epoch: 65 [0/900 (0%)]\tLoss: 0.000361\n",
      "Training epoch: 65 [160/900 (18%)]\tLoss: 0.001689\n",
      "Training epoch: 65 [320/900 (35%)]\tLoss: 0.002396\n",
      "Training epoch: 65 [480/900 (53%)]\tLoss: 0.002142\n",
      "Training epoch: 65 [640/900 (70%)]\tLoss: 0.002202\n",
      "Training epoch: 65 [800/900 (88%)]\tLoss: 0.003518\n",
      "=========> Epoch: 65 Average loss: 0.0024\n",
      "Correlation coefficient: 0.6475\n",
      "Training epoch: 66 [0/900 (0%)]\tLoss: 0.004931\n",
      "Training epoch: 66 [160/900 (18%)]\tLoss: 0.000712\n",
      "Training epoch: 66 [320/900 (35%)]\tLoss: 0.002682\n",
      "Training epoch: 66 [480/900 (53%)]\tLoss: 0.001322\n",
      "Training epoch: 66 [640/900 (70%)]\tLoss: 0.001066\n",
      "Training epoch: 66 [800/900 (88%)]\tLoss: 0.001190\n",
      "=========> Epoch: 66 Average loss: 0.0017\n",
      "Correlation coefficient: 0.6449\n",
      "Training epoch: 67 [0/900 (0%)]\tLoss: 0.001445\n",
      "Training epoch: 67 [160/900 (18%)]\tLoss: 0.000414\n",
      "Training epoch: 67 [320/900 (35%)]\tLoss: 0.000380\n",
      "Training epoch: 67 [480/900 (53%)]\tLoss: 0.000417\n",
      "Training epoch: 67 [640/900 (70%)]\tLoss: 0.001616\n",
      "Training epoch: 67 [800/900 (88%)]\tLoss: 0.002080\n",
      "=========> Epoch: 67 Average loss: 0.0010\n",
      "Correlation coefficient: 0.6499\n",
      "Training epoch: 68 [0/900 (0%)]\tLoss: 0.000365\n",
      "Training epoch: 68 [160/900 (18%)]\tLoss: 0.000762\n",
      "Training epoch: 68 [320/900 (35%)]\tLoss: 0.000609\n",
      "Training epoch: 68 [480/900 (53%)]\tLoss: 0.000509\n",
      "Training epoch: 68 [640/900 (70%)]\tLoss: 0.001272\n",
      "Training epoch: 68 [800/900 (88%)]\tLoss: 0.000842\n",
      "=========> Epoch: 68 Average loss: 0.0006\n",
      "Correlation coefficient: 0.6464\n",
      "Training epoch: 69 [0/900 (0%)]\tLoss: 0.000114\n",
      "Training epoch: 69 [160/900 (18%)]\tLoss: 0.000347\n",
      "Training epoch: 69 [320/900 (35%)]\tLoss: 0.000596\n",
      "Training epoch: 69 [480/900 (53%)]\tLoss: 0.002075\n",
      "Training epoch: 69 [640/900 (70%)]\tLoss: 0.000693\n",
      "Training epoch: 69 [800/900 (88%)]\tLoss: 0.000350\n",
      "=========> Epoch: 69 Average loss: 0.0008\n",
      "Correlation coefficient: 0.6498\n",
      "Training epoch: 70 [0/900 (0%)]\tLoss: 0.001309\n",
      "Training epoch: 70 [160/900 (18%)]\tLoss: 0.002003\n",
      "Training epoch: 70 [320/900 (35%)]\tLoss: 0.000276\n",
      "Training epoch: 70 [480/900 (53%)]\tLoss: 0.000946\n",
      "Training epoch: 70 [640/900 (70%)]\tLoss: 0.001585\n",
      "Training epoch: 70 [800/900 (88%)]\tLoss: 0.000572\n",
      "=========> Epoch: 70 Average loss: 0.0013\n",
      "Correlation coefficient: 0.6464\n",
      "Training epoch: 71 [0/900 (0%)]\tLoss: 0.001089\n",
      "Training epoch: 71 [160/900 (18%)]\tLoss: 0.000728\n",
      "Training epoch: 71 [320/900 (35%)]\tLoss: 0.002635\n",
      "Training epoch: 71 [480/900 (53%)]\tLoss: 0.000266\n",
      "Training epoch: 71 [640/900 (70%)]\tLoss: 0.002540\n",
      "Training epoch: 71 [800/900 (88%)]\tLoss: 0.002830\n",
      "=========> Epoch: 71 Average loss: 0.0021\n",
      "Correlation coefficient: 0.6482\n",
      "Training epoch: 72 [0/900 (0%)]\tLoss: 0.005553\n",
      "Training epoch: 72 [160/900 (18%)]\tLoss: 0.007666\n",
      "Training epoch: 72 [320/900 (35%)]\tLoss: 0.004552\n",
      "Training epoch: 72 [480/900 (53%)]\tLoss: 0.003912\n",
      "Training epoch: 72 [640/900 (70%)]\tLoss: 0.001907\n",
      "Training epoch: 72 [800/900 (88%)]\tLoss: 0.001867\n",
      "=========> Epoch: 72 Average loss: 0.0052\n",
      "Correlation coefficient: 0.6563\n",
      "Training epoch: 73 [0/900 (0%)]\tLoss: 0.004355\n",
      "Training epoch: 73 [160/900 (18%)]\tLoss: 0.016791\n",
      "Training epoch: 73 [320/900 (35%)]\tLoss: 0.005593\n",
      "Training epoch: 73 [480/900 (53%)]\tLoss: 0.001784\n",
      "Training epoch: 73 [640/900 (70%)]\tLoss: 0.005562\n",
      "Training epoch: 73 [800/900 (88%)]\tLoss: 0.004584\n",
      "=========> Epoch: 73 Average loss: 0.0071\n",
      "Correlation coefficient: 0.6440\n",
      "Training epoch: 74 [0/900 (0%)]\tLoss: 0.003646\n",
      "Training epoch: 74 [160/900 (18%)]\tLoss: 0.009948\n",
      "Training epoch: 74 [320/900 (35%)]\tLoss: 0.009433\n",
      "Training epoch: 74 [480/900 (53%)]\tLoss: 0.015948\n",
      "Training epoch: 74 [640/900 (70%)]\tLoss: 0.007814\n",
      "Training epoch: 74 [800/900 (88%)]\tLoss: 0.011989\n",
      "=========> Epoch: 74 Average loss: 0.0077\n",
      "Correlation coefficient: 0.6457\n",
      "Training epoch: 75 [0/900 (0%)]\tLoss: 0.013669\n",
      "Training epoch: 75 [160/900 (18%)]\tLoss: 0.013571\n",
      "Training epoch: 75 [320/900 (35%)]\tLoss: 0.003539\n",
      "Training epoch: 75 [480/900 (53%)]\tLoss: 0.008313\n",
      "Training epoch: 75 [640/900 (70%)]\tLoss: 0.007125\n",
      "Training epoch: 75 [800/900 (88%)]\tLoss: 0.005901\n",
      "=========> Epoch: 75 Average loss: 0.0080\n",
      "Correlation coefficient: 0.6490\n",
      "Training epoch: 76 [0/900 (0%)]\tLoss: 0.015690\n",
      "Training epoch: 76 [160/900 (18%)]\tLoss: 0.003102\n",
      "Training epoch: 76 [320/900 (35%)]\tLoss: 0.010335\n",
      "Training epoch: 76 [480/900 (53%)]\tLoss: 0.003115\n",
      "Training epoch: 76 [640/900 (70%)]\tLoss: 0.006769\n",
      "Training epoch: 76 [800/900 (88%)]\tLoss: 0.004256\n",
      "=========> Epoch: 76 Average loss: 0.0074\n",
      "Correlation coefficient: 0.6434\n",
      "Training epoch: 77 [0/900 (0%)]\tLoss: 0.008824\n",
      "Training epoch: 77 [160/900 (18%)]\tLoss: 0.017997\n",
      "Training epoch: 77 [320/900 (35%)]\tLoss: 0.004886\n",
      "Training epoch: 77 [480/900 (53%)]\tLoss: 0.006745\n",
      "Training epoch: 77 [640/900 (70%)]\tLoss: 0.012748\n",
      "Training epoch: 77 [800/900 (88%)]\tLoss: 0.002751\n",
      "=========> Epoch: 77 Average loss: 0.0086\n",
      "Correlation coefficient: 0.6366\n",
      "Training epoch: 78 [0/900 (0%)]\tLoss: 0.008428\n",
      "Training epoch: 78 [160/900 (18%)]\tLoss: 0.007551\n",
      "Training epoch: 78 [320/900 (35%)]\tLoss: 0.011857\n",
      "Training epoch: 78 [480/900 (53%)]\tLoss: 0.009356\n",
      "Training epoch: 78 [640/900 (70%)]\tLoss: 0.009607\n",
      "Training epoch: 78 [800/900 (88%)]\tLoss: 0.004901\n",
      "=========> Epoch: 78 Average loss: 0.0134\n",
      "Correlation coefficient: 0.6390\n",
      "Training epoch: 79 [0/900 (0%)]\tLoss: 0.008812\n",
      "Training epoch: 79 [160/900 (18%)]\tLoss: 0.007739\n",
      "Training epoch: 79 [320/900 (35%)]\tLoss: 0.007592\n",
      "Training epoch: 79 [480/900 (53%)]\tLoss: 0.004846\n",
      "Training epoch: 79 [640/900 (70%)]\tLoss: 0.010180\n",
      "Training epoch: 79 [800/900 (88%)]\tLoss: 0.010852\n",
      "=========> Epoch: 79 Average loss: 0.0147\n",
      "Correlation coefficient: 0.6345\n",
      "Training epoch: 80 [0/900 (0%)]\tLoss: 0.019453\n",
      "Training epoch: 80 [160/900 (18%)]\tLoss: 0.003787\n",
      "Training epoch: 80 [320/900 (35%)]\tLoss: 0.008878\n",
      "Training epoch: 80 [480/900 (53%)]\tLoss: 0.015289\n",
      "Training epoch: 80 [640/900 (70%)]\tLoss: 0.010149\n",
      "Training epoch: 80 [800/900 (88%)]\tLoss: 0.016119\n",
      "=========> Epoch: 80 Average loss: 0.0121\n",
      "Correlation coefficient: 0.6410\n",
      "Training epoch: 81 [0/900 (0%)]\tLoss: 0.008251\n",
      "Training epoch: 81 [160/900 (18%)]\tLoss: 0.021039\n",
      "Training epoch: 81 [320/900 (35%)]\tLoss: 0.015751\n",
      "Training epoch: 81 [480/900 (53%)]\tLoss: 0.003530\n",
      "Training epoch: 81 [640/900 (70%)]\tLoss: 0.005201\n",
      "Training epoch: 81 [800/900 (88%)]\tLoss: 0.008284\n",
      "=========> Epoch: 81 Average loss: 0.0106\n",
      "Correlation coefficient: 0.6530\n",
      "Training epoch: 82 [0/900 (0%)]\tLoss: 0.008237\n",
      "Training epoch: 82 [160/900 (18%)]\tLoss: 0.007279\n",
      "Training epoch: 82 [320/900 (35%)]\tLoss: 0.005458\n",
      "Training epoch: 82 [480/900 (53%)]\tLoss: 0.020672\n",
      "Training epoch: 82 [640/900 (70%)]\tLoss: 0.006462\n",
      "Training epoch: 82 [800/900 (88%)]\tLoss: 0.005153\n",
      "=========> Epoch: 82 Average loss: 0.0146\n",
      "Correlation coefficient: 0.6296\n",
      "Training epoch: 83 [0/900 (0%)]\tLoss: 0.003491\n",
      "Training epoch: 83 [160/900 (18%)]\tLoss: 0.006659\n",
      "Training epoch: 83 [320/900 (35%)]\tLoss: 0.007122\n",
      "Training epoch: 83 [480/900 (53%)]\tLoss: 0.007967\n",
      "Training epoch: 83 [640/900 (70%)]\tLoss: 0.004031\n",
      "Training epoch: 83 [800/900 (88%)]\tLoss: 0.008471\n",
      "=========> Epoch: 83 Average loss: 0.0113\n",
      "Correlation coefficient: 0.6545\n",
      "Training epoch: 84 [0/900 (0%)]\tLoss: 0.003394\n",
      "Training epoch: 84 [160/900 (18%)]\tLoss: 0.050624\n",
      "Training epoch: 84 [320/900 (35%)]\tLoss: 0.011286\n",
      "Training epoch: 84 [480/900 (53%)]\tLoss: 0.004425\n",
      "Training epoch: 84 [640/900 (70%)]\tLoss: 0.008239\n",
      "Training epoch: 84 [800/900 (88%)]\tLoss: 0.008038\n",
      "=========> Epoch: 84 Average loss: 0.0082\n",
      "Correlation coefficient: 0.6571\n",
      "Training epoch: 85 [0/900 (0%)]\tLoss: 0.006716\n",
      "Training epoch: 85 [160/900 (18%)]\tLoss: 0.003135\n",
      "Training epoch: 85 [320/900 (35%)]\tLoss: 0.010769\n",
      "Training epoch: 85 [480/900 (53%)]\tLoss: 0.008549\n",
      "Training epoch: 85 [640/900 (70%)]\tLoss: 0.029080\n",
      "Training epoch: 85 [800/900 (88%)]\tLoss: 0.003526\n",
      "=========> Epoch: 85 Average loss: 0.0077\n",
      "Correlation coefficient: 0.6559\n",
      "Training epoch: 86 [0/900 (0%)]\tLoss: 0.009593\n",
      "Training epoch: 86 [160/900 (18%)]\tLoss: 0.009284\n",
      "Training epoch: 86 [320/900 (35%)]\tLoss: 0.005269\n",
      "Training epoch: 86 [480/900 (53%)]\tLoss: 0.003086\n",
      "Training epoch: 86 [640/900 (70%)]\tLoss: 0.008670\n",
      "Training epoch: 86 [800/900 (88%)]\tLoss: 0.003338\n",
      "=========> Epoch: 86 Average loss: 0.0053\n",
      "Correlation coefficient: 0.6532\n",
      "Training epoch: 87 [0/900 (0%)]\tLoss: 0.002049\n",
      "Training epoch: 87 [160/900 (18%)]\tLoss: 0.006869\n",
      "Training epoch: 87 [320/900 (35%)]\tLoss: 0.007951\n",
      "Training epoch: 87 [480/900 (53%)]\tLoss: 0.002476\n",
      "Training epoch: 87 [640/900 (70%)]\tLoss: 0.001502\n",
      "Training epoch: 87 [800/900 (88%)]\tLoss: 0.001873\n",
      "=========> Epoch: 87 Average loss: 0.0033\n",
      "Correlation coefficient: 0.6516\n",
      "Training epoch: 88 [0/900 (0%)]\tLoss: 0.002938\n",
      "Training epoch: 88 [160/900 (18%)]\tLoss: 0.003258\n",
      "Training epoch: 88 [320/900 (35%)]\tLoss: 0.001364\n",
      "Training epoch: 88 [480/900 (53%)]\tLoss: 0.001587\n",
      "Training epoch: 88 [640/900 (70%)]\tLoss: 0.002289\n",
      "Training epoch: 88 [800/900 (88%)]\tLoss: 0.001193\n",
      "=========> Epoch: 88 Average loss: 0.0020\n",
      "Correlation coefficient: 0.6556\n",
      "Training epoch: 89 [0/900 (0%)]\tLoss: 0.001787\n",
      "Training epoch: 89 [160/900 (18%)]\tLoss: 0.001589\n",
      "Training epoch: 89 [320/900 (35%)]\tLoss: 0.001157\n",
      "Training epoch: 89 [480/900 (53%)]\tLoss: 0.002579\n",
      "Training epoch: 89 [640/900 (70%)]\tLoss: 0.001400\n",
      "Training epoch: 89 [800/900 (88%)]\tLoss: 0.002286\n",
      "=========> Epoch: 89 Average loss: 0.0020\n",
      "Correlation coefficient: 0.6568\n",
      "Training epoch: 90 [0/900 (0%)]\tLoss: 0.002101\n",
      "Training epoch: 90 [160/900 (18%)]\tLoss: 0.003123\n",
      "Training epoch: 90 [320/900 (35%)]\tLoss: 0.002157\n",
      "Training epoch: 90 [480/900 (53%)]\tLoss: 0.001418\n",
      "Training epoch: 90 [640/900 (70%)]\tLoss: 0.000616\n",
      "Training epoch: 90 [800/900 (88%)]\tLoss: 0.002106\n",
      "=========> Epoch: 90 Average loss: 0.0023\n",
      "Correlation coefficient: 0.6545\n",
      "Training epoch: 91 [0/900 (0%)]\tLoss: 0.002606\n",
      "Training epoch: 91 [160/900 (18%)]\tLoss: 0.001196\n",
      "Training epoch: 91 [320/900 (35%)]\tLoss: 0.004098\n",
      "Training epoch: 91 [480/900 (53%)]\tLoss: 0.004378\n",
      "Training epoch: 91 [640/900 (70%)]\tLoss: 0.001577\n",
      "Training epoch: 91 [800/900 (88%)]\tLoss: 0.002190\n",
      "=========> Epoch: 91 Average loss: 0.0036\n",
      "Correlation coefficient: 0.6482\n",
      "Training epoch: 92 [0/900 (0%)]\tLoss: 0.003256\n",
      "Training epoch: 92 [160/900 (18%)]\tLoss: 0.010524\n",
      "Training epoch: 92 [320/900 (35%)]\tLoss: 0.001898\n",
      "Training epoch: 92 [480/900 (53%)]\tLoss: 0.004050\n",
      "Training epoch: 92 [640/900 (70%)]\tLoss: 0.005987\n",
      "Training epoch: 92 [800/900 (88%)]\tLoss: 0.003945\n",
      "=========> Epoch: 92 Average loss: 0.0047\n",
      "Correlation coefficient: 0.6514\n",
      "Training epoch: 93 [0/900 (0%)]\tLoss: 0.003454\n",
      "Training epoch: 93 [160/900 (18%)]\tLoss: 0.010715\n",
      "Training epoch: 93 [320/900 (35%)]\tLoss: 0.002435\n",
      "Training epoch: 93 [480/900 (53%)]\tLoss: 0.002261\n",
      "Training epoch: 93 [640/900 (70%)]\tLoss: 0.009879\n",
      "Training epoch: 93 [800/900 (88%)]\tLoss: 0.004105\n",
      "=========> Epoch: 93 Average loss: 0.0045\n",
      "Correlation coefficient: 0.6553\n",
      "Training epoch: 94 [0/900 (0%)]\tLoss: 0.002158\n",
      "Training epoch: 94 [160/900 (18%)]\tLoss: 0.005715\n",
      "Training epoch: 94 [320/900 (35%)]\tLoss: 0.013466\n",
      "Training epoch: 94 [480/900 (53%)]\tLoss: 0.007200\n",
      "Training epoch: 94 [640/900 (70%)]\tLoss: 0.001328\n",
      "Training epoch: 94 [800/900 (88%)]\tLoss: 0.004420\n",
      "=========> Epoch: 94 Average loss: 0.0046\n",
      "Correlation coefficient: 0.6432\n",
      "Training epoch: 95 [0/900 (0%)]\tLoss: 0.002383\n",
      "Training epoch: 95 [160/900 (18%)]\tLoss: 0.007347\n",
      "Training epoch: 95 [320/900 (35%)]\tLoss: 0.014945\n",
      "Training epoch: 95 [480/900 (53%)]\tLoss: 0.004395\n",
      "Training epoch: 95 [640/900 (70%)]\tLoss: 0.004093\n",
      "Training epoch: 95 [800/900 (88%)]\tLoss: 0.007557\n",
      "=========> Epoch: 95 Average loss: 0.0057\n",
      "Correlation coefficient: 0.6483\n",
      "Training epoch: 96 [0/900 (0%)]\tLoss: 0.011887\n",
      "Training epoch: 96 [160/900 (18%)]\tLoss: 0.005241\n",
      "Training epoch: 96 [320/900 (35%)]\tLoss: 0.004946\n",
      "Training epoch: 96 [480/900 (53%)]\tLoss: 0.003126\n",
      "Training epoch: 96 [640/900 (70%)]\tLoss: 0.003976\n",
      "Training epoch: 96 [800/900 (88%)]\tLoss: 0.001213\n",
      "=========> Epoch: 96 Average loss: 0.0061\n",
      "Correlation coefficient: 0.6507\n",
      "Training epoch: 97 [0/900 (0%)]\tLoss: 0.005757\n",
      "Training epoch: 97 [160/900 (18%)]\tLoss: 0.002543\n",
      "Training epoch: 97 [320/900 (35%)]\tLoss: 0.002661\n",
      "Training epoch: 97 [480/900 (53%)]\tLoss: 0.003674\n",
      "Training epoch: 97 [640/900 (70%)]\tLoss: 0.008564\n",
      "Training epoch: 97 [800/900 (88%)]\tLoss: 0.002463\n",
      "=========> Epoch: 97 Average loss: 0.0062\n",
      "Correlation coefficient: 0.6569\n",
      "Training epoch: 98 [0/900 (0%)]\tLoss: 0.009260\n",
      "Training epoch: 98 [160/900 (18%)]\tLoss: 0.008318\n",
      "Training epoch: 98 [320/900 (35%)]\tLoss: 0.011013\n",
      "Training epoch: 98 [480/900 (53%)]\tLoss: 0.006146\n",
      "Training epoch: 98 [640/900 (70%)]\tLoss: 0.007034\n",
      "Training epoch: 98 [800/900 (88%)]\tLoss: 0.006166\n",
      "=========> Epoch: 98 Average loss: 0.0058\n",
      "Correlation coefficient: 0.6451\n",
      "Training epoch: 99 [0/900 (0%)]\tLoss: 0.007924\n",
      "Training epoch: 99 [160/900 (18%)]\tLoss: 0.004246\n",
      "Training epoch: 99 [320/900 (35%)]\tLoss: 0.005119\n",
      "Training epoch: 99 [480/900 (53%)]\tLoss: 0.006016\n",
      "Training epoch: 99 [640/900 (70%)]\tLoss: 0.002762\n",
      "Training epoch: 99 [800/900 (88%)]\tLoss: 0.003020\n",
      "=========> Epoch: 99 Average loss: 0.0054\n",
      "Correlation coefficient: 0.6653\n",
      "✅ Epoch 99: New best correlation = 0.6653\n",
      "Training epoch: 100 [0/900 (0%)]\tLoss: 0.004549\n",
      "Training epoch: 100 [160/900 (18%)]\tLoss: 0.002637\n",
      "Training epoch: 100 [320/900 (35%)]\tLoss: 0.009115\n",
      "Training epoch: 100 [480/900 (53%)]\tLoss: 0.009040\n",
      "Training epoch: 100 [640/900 (70%)]\tLoss: 0.009016\n",
      "Training epoch: 100 [800/900 (88%)]\tLoss: 0.005374\n",
      "=========> Epoch: 100 Average loss: 0.0068\n",
      "Correlation coefficient: 0.6518\n",
      "Training epoch: 101 [0/900 (0%)]\tLoss: 0.005149\n",
      "Training epoch: 101 [160/900 (18%)]\tLoss: 0.007208\n",
      "Training epoch: 101 [320/900 (35%)]\tLoss: 0.006877\n",
      "Training epoch: 101 [480/900 (53%)]\tLoss: 0.007158\n",
      "Training epoch: 101 [640/900 (70%)]\tLoss: 0.005937\n",
      "Training epoch: 101 [800/900 (88%)]\tLoss: 0.003608\n",
      "=========> Epoch: 101 Average loss: 0.0071\n",
      "Correlation coefficient: 0.6496\n",
      "Training epoch: 102 [0/900 (0%)]\tLoss: 0.002812\n",
      "Training epoch: 102 [160/900 (18%)]\tLoss: 0.003545\n",
      "Training epoch: 102 [320/900 (35%)]\tLoss: 0.001715\n",
      "Training epoch: 102 [480/900 (53%)]\tLoss: 0.017241\n",
      "Training epoch: 102 [640/900 (70%)]\tLoss: 0.007148\n",
      "Training epoch: 102 [800/900 (88%)]\tLoss: 0.010521\n",
      "=========> Epoch: 102 Average loss: 0.0099\n",
      "Correlation coefficient: 0.6525\n",
      "Training epoch: 103 [0/900 (0%)]\tLoss: 0.012281\n",
      "Training epoch: 103 [160/900 (18%)]\tLoss: 0.004252\n",
      "Training epoch: 103 [320/900 (35%)]\tLoss: 0.010812\n",
      "Training epoch: 103 [480/900 (53%)]\tLoss: 0.006681\n",
      "Training epoch: 103 [640/900 (70%)]\tLoss: 0.010253\n",
      "Training epoch: 103 [800/900 (88%)]\tLoss: 0.004459\n",
      "=========> Epoch: 103 Average loss: 0.0091\n",
      "Correlation coefficient: 0.6531\n",
      "Training epoch: 104 [0/900 (0%)]\tLoss: 0.004313\n",
      "Training epoch: 104 [160/900 (18%)]\tLoss: 0.012455\n",
      "Training epoch: 104 [320/900 (35%)]\tLoss: 0.003508\n",
      "Training epoch: 104 [480/900 (53%)]\tLoss: 0.005503\n",
      "Training epoch: 104 [640/900 (70%)]\tLoss: 0.004340\n",
      "Training epoch: 104 [800/900 (88%)]\tLoss: 0.007868\n",
      "=========> Epoch: 104 Average loss: 0.0071\n",
      "Correlation coefficient: 0.6436\n",
      "Training epoch: 105 [0/900 (0%)]\tLoss: 0.002039\n",
      "Training epoch: 105 [160/900 (18%)]\tLoss: 0.006318\n",
      "Training epoch: 105 [320/900 (35%)]\tLoss: 0.005941\n",
      "Training epoch: 105 [480/900 (53%)]\tLoss: 0.002294\n",
      "Training epoch: 105 [640/900 (70%)]\tLoss: 0.006970\n",
      "Training epoch: 105 [800/900 (88%)]\tLoss: 0.005092\n",
      "=========> Epoch: 105 Average loss: 0.0071\n",
      "Correlation coefficient: 0.6324\n",
      "Training epoch: 106 [0/900 (0%)]\tLoss: 0.004411\n",
      "Training epoch: 106 [160/900 (18%)]\tLoss: 0.005653\n",
      "Training epoch: 106 [320/900 (35%)]\tLoss: 0.004586\n",
      "Training epoch: 106 [480/900 (53%)]\tLoss: 0.032966\n",
      "Training epoch: 106 [640/900 (70%)]\tLoss: 0.004036\n",
      "Training epoch: 106 [800/900 (88%)]\tLoss: 0.004592\n",
      "=========> Epoch: 106 Average loss: 0.0083\n",
      "Correlation coefficient: 0.6440\n",
      "Training epoch: 107 [0/900 (0%)]\tLoss: 0.006673\n",
      "Training epoch: 107 [160/900 (18%)]\tLoss: 0.003683\n",
      "Training epoch: 107 [320/900 (35%)]\tLoss: 0.010928\n",
      "Training epoch: 107 [480/900 (53%)]\tLoss: 0.009538\n",
      "Training epoch: 107 [640/900 (70%)]\tLoss: 0.011093\n",
      "Training epoch: 107 [800/900 (88%)]\tLoss: 0.017408\n",
      "=========> Epoch: 107 Average loss: 0.0108\n",
      "Correlation coefficient: 0.6353\n",
      "Training epoch: 108 [0/900 (0%)]\tLoss: 0.004191\n",
      "Training epoch: 108 [160/900 (18%)]\tLoss: 0.020342\n",
      "Training epoch: 108 [320/900 (35%)]\tLoss: 0.006856\n",
      "Training epoch: 108 [480/900 (53%)]\tLoss: 0.022543\n",
      "Training epoch: 108 [640/900 (70%)]\tLoss: 0.041450\n",
      "Training epoch: 108 [800/900 (88%)]\tLoss: 0.020746\n",
      "=========> Epoch: 108 Average loss: 0.0140\n",
      "Correlation coefficient: 0.6327\n",
      "Training epoch: 109 [0/900 (0%)]\tLoss: 0.007991\n",
      "Training epoch: 109 [160/900 (18%)]\tLoss: 0.015868\n",
      "Training epoch: 109 [320/900 (35%)]\tLoss: 0.005006\n",
      "Training epoch: 109 [480/900 (53%)]\tLoss: 0.009325\n",
      "Training epoch: 109 [640/900 (70%)]\tLoss: 0.003114\n",
      "Training epoch: 109 [800/900 (88%)]\tLoss: 0.009031\n",
      "=========> Epoch: 109 Average loss: 0.0114\n",
      "Correlation coefficient: 0.6514\n",
      "Training epoch: 110 [0/900 (0%)]\tLoss: 0.009070\n",
      "Training epoch: 110 [160/900 (18%)]\tLoss: 0.008005\n",
      "Training epoch: 110 [320/900 (35%)]\tLoss: 0.007553\n",
      "Training epoch: 110 [480/900 (53%)]\tLoss: 0.003548\n",
      "Training epoch: 110 [640/900 (70%)]\tLoss: 0.011068\n",
      "Training epoch: 110 [800/900 (88%)]\tLoss: 0.007891\n",
      "=========> Epoch: 110 Average loss: 0.0082\n",
      "Correlation coefficient: 0.6578\n",
      "Training epoch: 111 [0/900 (0%)]\tLoss: 0.012264\n",
      "Training epoch: 111 [160/900 (18%)]\tLoss: 0.004424\n",
      "Training epoch: 111 [320/900 (35%)]\tLoss: 0.010748\n",
      "Training epoch: 111 [480/900 (53%)]\tLoss: 0.005545\n",
      "Training epoch: 111 [640/900 (70%)]\tLoss: 0.018514\n",
      "Training epoch: 111 [800/900 (88%)]\tLoss: 0.005090\n",
      "=========> Epoch: 111 Average loss: 0.0093\n",
      "Correlation coefficient: 0.6499\n",
      "Training epoch: 112 [0/900 (0%)]\tLoss: 0.011859\n",
      "Training epoch: 112 [160/900 (18%)]\tLoss: 0.017408\n",
      "Training epoch: 112 [320/900 (35%)]\tLoss: 0.006115\n",
      "Training epoch: 112 [480/900 (53%)]\tLoss: 0.003238\n",
      "Training epoch: 112 [640/900 (70%)]\tLoss: 0.012308\n",
      "Training epoch: 112 [800/900 (88%)]\tLoss: 0.012776\n",
      "=========> Epoch: 112 Average loss: 0.0108\n",
      "Correlation coefficient: 0.6632\n",
      "Training epoch: 113 [0/900 (0%)]\tLoss: 0.003209\n",
      "Training epoch: 113 [160/900 (18%)]\tLoss: 0.002589\n",
      "Training epoch: 113 [320/900 (35%)]\tLoss: 0.011766\n",
      "Training epoch: 113 [480/900 (53%)]\tLoss: 0.019785\n",
      "Training epoch: 113 [640/900 (70%)]\tLoss: 0.008739\n",
      "Training epoch: 113 [800/900 (88%)]\tLoss: 0.002677\n",
      "=========> Epoch: 113 Average loss: 0.0109\n",
      "Correlation coefficient: 0.6474\n",
      "Training epoch: 114 [0/900 (0%)]\tLoss: 0.004571\n",
      "Training epoch: 114 [160/900 (18%)]\tLoss: 0.007357\n",
      "Training epoch: 114 [320/900 (35%)]\tLoss: 0.008995\n",
      "Training epoch: 114 [480/900 (53%)]\tLoss: 0.009588\n",
      "Training epoch: 114 [640/900 (70%)]\tLoss: 0.015789\n",
      "Training epoch: 114 [800/900 (88%)]\tLoss: 0.004894\n",
      "=========> Epoch: 114 Average loss: 0.0085\n",
      "Correlation coefficient: 0.6476\n",
      "Training epoch: 115 [0/900 (0%)]\tLoss: 0.007563\n",
      "Training epoch: 115 [160/900 (18%)]\tLoss: 0.003505\n",
      "Training epoch: 115 [320/900 (35%)]\tLoss: 0.003471\n",
      "Training epoch: 115 [480/900 (53%)]\tLoss: 0.002702\n",
      "Training epoch: 115 [640/900 (70%)]\tLoss: 0.003401\n",
      "Training epoch: 115 [800/900 (88%)]\tLoss: 0.004638\n",
      "=========> Epoch: 115 Average loss: 0.0053\n",
      "Correlation coefficient: 0.6472\n",
      "Training epoch: 116 [0/900 (0%)]\tLoss: 0.006999\n",
      "Training epoch: 116 [160/900 (18%)]\tLoss: 0.004229\n",
      "Training epoch: 116 [320/900 (35%)]\tLoss: 0.005353\n",
      "Training epoch: 116 [480/900 (53%)]\tLoss: 0.002805\n",
      "Training epoch: 116 [640/900 (70%)]\tLoss: 0.000753\n",
      "Training epoch: 116 [800/900 (88%)]\tLoss: 0.001713\n",
      "=========> Epoch: 116 Average loss: 0.0041\n",
      "Correlation coefficient: 0.6323\n",
      "Training epoch: 117 [0/900 (0%)]\tLoss: 0.005294\n",
      "Training epoch: 117 [160/900 (18%)]\tLoss: 0.003068\n",
      "Training epoch: 117 [320/900 (35%)]\tLoss: 0.005999\n",
      "Training epoch: 117 [480/900 (53%)]\tLoss: 0.003015\n",
      "Training epoch: 117 [640/900 (70%)]\tLoss: 0.013181\n",
      "Training epoch: 117 [800/900 (88%)]\tLoss: 0.001376\n",
      "=========> Epoch: 117 Average loss: 0.0038\n",
      "Correlation coefficient: 0.6496\n",
      "Training epoch: 118 [0/900 (0%)]\tLoss: 0.001094\n",
      "Training epoch: 118 [160/900 (18%)]\tLoss: 0.001898\n",
      "Training epoch: 118 [320/900 (35%)]\tLoss: 0.007293\n",
      "Training epoch: 118 [480/900 (53%)]\tLoss: 0.003381\n",
      "Training epoch: 118 [640/900 (70%)]\tLoss: 0.001729\n",
      "Training epoch: 118 [800/900 (88%)]\tLoss: 0.001371\n",
      "=========> Epoch: 118 Average loss: 0.0019\n",
      "Correlation coefficient: 0.6457\n",
      "Training epoch: 119 [0/900 (0%)]\tLoss: 0.002856\n",
      "Training epoch: 119 [160/900 (18%)]\tLoss: 0.001348\n",
      "Training epoch: 119 [320/900 (35%)]\tLoss: 0.002779\n",
      "Training epoch: 119 [480/900 (53%)]\tLoss: 0.001242\n",
      "Training epoch: 119 [640/900 (70%)]\tLoss: 0.000632\n",
      "Training epoch: 119 [800/900 (88%)]\tLoss: 0.000681\n",
      "=========> Epoch: 119 Average loss: 0.0014\n",
      "Correlation coefficient: 0.6476\n",
      "Training epoch: 120 [0/900 (0%)]\tLoss: 0.001136\n",
      "Training epoch: 120 [160/900 (18%)]\tLoss: 0.000330\n",
      "Training epoch: 120 [320/900 (35%)]\tLoss: 0.000736\n",
      "Training epoch: 120 [480/900 (53%)]\tLoss: 0.001115\n",
      "Training epoch: 120 [640/900 (70%)]\tLoss: 0.000942\n",
      "Training epoch: 120 [800/900 (88%)]\tLoss: 0.000815\n",
      "=========> Epoch: 120 Average loss: 0.0015\n",
      "Correlation coefficient: 0.6476\n",
      "Training epoch: 121 [0/900 (0%)]\tLoss: 0.000328\n",
      "Training epoch: 121 [160/900 (18%)]\tLoss: 0.000433\n",
      "Training epoch: 121 [320/900 (35%)]\tLoss: 0.000326\n",
      "Training epoch: 121 [480/900 (53%)]\tLoss: 0.001424\n",
      "Training epoch: 121 [640/900 (70%)]\tLoss: 0.000773\n",
      "Training epoch: 121 [800/900 (88%)]\tLoss: 0.000374\n",
      "=========> Epoch: 121 Average loss: 0.0010\n",
      "Correlation coefficient: 0.6476\n",
      "Training epoch: 122 [0/900 (0%)]\tLoss: 0.000684\n",
      "Training epoch: 122 [160/900 (18%)]\tLoss: 0.000915\n",
      "Training epoch: 122 [320/900 (35%)]\tLoss: 0.000664\n",
      "Training epoch: 122 [480/900 (53%)]\tLoss: 0.001463\n",
      "Training epoch: 122 [640/900 (70%)]\tLoss: 0.000495\n",
      "Training epoch: 122 [800/900 (88%)]\tLoss: 0.000259\n",
      "=========> Epoch: 122 Average loss: 0.0010\n",
      "Correlation coefficient: 0.6489\n",
      "Training epoch: 123 [0/900 (0%)]\tLoss: 0.000411\n",
      "Training epoch: 123 [160/900 (18%)]\tLoss: 0.000260\n",
      "Training epoch: 123 [320/900 (35%)]\tLoss: 0.001894\n",
      "Training epoch: 123 [480/900 (53%)]\tLoss: 0.002080\n",
      "Training epoch: 123 [640/900 (70%)]\tLoss: 0.000909\n",
      "Training epoch: 123 [800/900 (88%)]\tLoss: 0.000590\n",
      "=========> Epoch: 123 Average loss: 0.0011\n",
      "Correlation coefficient: 0.6491\n",
      "Training epoch: 124 [0/900 (0%)]\tLoss: 0.002250\n",
      "Training epoch: 124 [160/900 (18%)]\tLoss: 0.001341\n",
      "Training epoch: 124 [320/900 (35%)]\tLoss: 0.001602\n",
      "Training epoch: 124 [480/900 (53%)]\tLoss: 0.000936\n",
      "Training epoch: 124 [640/900 (70%)]\tLoss: 0.001793\n",
      "Training epoch: 124 [800/900 (88%)]\tLoss: 0.002397\n",
      "=========> Epoch: 124 Average loss: 0.0015\n",
      "Correlation coefficient: 0.6490\n",
      "Training epoch: 125 [0/900 (0%)]\tLoss: 0.001706\n",
      "Training epoch: 125 [160/900 (18%)]\tLoss: 0.000293\n",
      "Training epoch: 125 [320/900 (35%)]\tLoss: 0.000186\n",
      "Training epoch: 125 [480/900 (53%)]\tLoss: 0.001231\n",
      "Training epoch: 125 [640/900 (70%)]\tLoss: 0.003702\n",
      "Training epoch: 125 [800/900 (88%)]\tLoss: 0.000479\n",
      "=========> Epoch: 125 Average loss: 0.0019\n",
      "Correlation coefficient: 0.6523\n",
      "Training epoch: 126 [0/900 (0%)]\tLoss: 0.000960\n",
      "Training epoch: 126 [160/900 (18%)]\tLoss: 0.003678\n",
      "Training epoch: 126 [320/900 (35%)]\tLoss: 0.001419\n",
      "Training epoch: 126 [480/900 (53%)]\tLoss: 0.003364\n",
      "Training epoch: 126 [640/900 (70%)]\tLoss: 0.001157\n",
      "Training epoch: 126 [800/900 (88%)]\tLoss: 0.000901\n",
      "=========> Epoch: 126 Average loss: 0.0020\n",
      "Correlation coefficient: 0.6440\n",
      "Training epoch: 127 [0/900 (0%)]\tLoss: 0.006249\n",
      "Training epoch: 127 [160/900 (18%)]\tLoss: 0.002638\n",
      "Training epoch: 127 [320/900 (35%)]\tLoss: 0.001031\n",
      "Training epoch: 127 [480/900 (53%)]\tLoss: 0.004376\n",
      "Training epoch: 127 [640/900 (70%)]\tLoss: 0.002622\n",
      "Training epoch: 127 [800/900 (88%)]\tLoss: 0.000451\n",
      "=========> Epoch: 127 Average loss: 0.0026\n",
      "Correlation coefficient: 0.6462\n",
      "Training epoch: 128 [0/900 (0%)]\tLoss: 0.006197\n",
      "Training epoch: 128 [160/900 (18%)]\tLoss: 0.003842\n",
      "Training epoch: 128 [320/900 (35%)]\tLoss: 0.001761\n",
      "Training epoch: 128 [480/900 (53%)]\tLoss: 0.002300\n",
      "Training epoch: 128 [640/900 (70%)]\tLoss: 0.002069\n",
      "Training epoch: 128 [800/900 (88%)]\tLoss: 0.001401\n",
      "=========> Epoch: 128 Average loss: 0.0032\n",
      "Correlation coefficient: 0.6421\n",
      "Training epoch: 129 [0/900 (0%)]\tLoss: 0.001054\n",
      "Training epoch: 129 [160/900 (18%)]\tLoss: 0.003693\n",
      "Training epoch: 129 [320/900 (35%)]\tLoss: 0.004175\n",
      "Training epoch: 129 [480/900 (53%)]\tLoss: 0.002726\n",
      "Training epoch: 129 [640/900 (70%)]\tLoss: 0.002433\n",
      "Training epoch: 129 [800/900 (88%)]\tLoss: 0.000960\n",
      "=========> Epoch: 129 Average loss: 0.0032\n",
      "Correlation coefficient: 0.6481\n",
      "Training epoch: 130 [0/900 (0%)]\tLoss: 0.003529\n",
      "Training epoch: 130 [160/900 (18%)]\tLoss: 0.001474\n",
      "Training epoch: 130 [320/900 (35%)]\tLoss: 0.001133\n",
      "Training epoch: 130 [480/900 (53%)]\tLoss: 0.001840\n",
      "Training epoch: 130 [640/900 (70%)]\tLoss: 0.001093\n",
      "Training epoch: 130 [800/900 (88%)]\tLoss: 0.001285\n",
      "=========> Epoch: 130 Average loss: 0.0027\n",
      "Correlation coefficient: 0.6416\n",
      "Training epoch: 131 [0/900 (0%)]\tLoss: 0.003122\n",
      "Training epoch: 131 [160/900 (18%)]\tLoss: 0.002294\n",
      "Training epoch: 131 [320/900 (35%)]\tLoss: 0.004966\n",
      "Training epoch: 131 [480/900 (53%)]\tLoss: 0.004612\n",
      "Training epoch: 131 [640/900 (70%)]\tLoss: 0.008139\n",
      "Training epoch: 131 [800/900 (88%)]\tLoss: 0.005300\n",
      "=========> Epoch: 131 Average loss: 0.0054\n",
      "Correlation coefficient: 0.6441\n",
      "Training epoch: 132 [0/900 (0%)]\tLoss: 0.003038\n",
      "Training epoch: 132 [160/900 (18%)]\tLoss: 0.007865\n",
      "Training epoch: 132 [320/900 (35%)]\tLoss: 0.014868\n",
      "Training epoch: 132 [480/900 (53%)]\tLoss: 0.006420\n",
      "Training epoch: 132 [640/900 (70%)]\tLoss: 0.010206\n",
      "Training epoch: 132 [800/900 (88%)]\tLoss: 0.019190\n",
      "=========> Epoch: 132 Average loss: 0.0112\n",
      "Correlation coefficient: 0.6597\n",
      "Training epoch: 133 [0/900 (0%)]\tLoss: 0.017234\n",
      "Training epoch: 133 [160/900 (18%)]\tLoss: 0.021216\n",
      "Training epoch: 133 [320/900 (35%)]\tLoss: 0.015745\n",
      "Training epoch: 133 [480/900 (53%)]\tLoss: 0.011902\n",
      "Training epoch: 133 [640/900 (70%)]\tLoss: 0.015534\n",
      "Training epoch: 133 [800/900 (88%)]\tLoss: 0.013966\n",
      "=========> Epoch: 133 Average loss: 0.0186\n",
      "Correlation coefficient: 0.6324\n",
      "Training epoch: 134 [0/900 (0%)]\tLoss: 0.020584\n",
      "Training epoch: 134 [160/900 (18%)]\tLoss: 0.021867\n",
      "Training epoch: 134 [320/900 (35%)]\tLoss: 0.142064\n",
      "Training epoch: 134 [480/900 (53%)]\tLoss: 0.017483\n",
      "Training epoch: 134 [640/900 (70%)]\tLoss: 0.023101\n",
      "Training epoch: 134 [800/900 (88%)]\tLoss: 0.012380\n",
      "=========> Epoch: 134 Average loss: 0.0278\n",
      "Correlation coefficient: 0.6494\n",
      "Training epoch: 135 [0/900 (0%)]\tLoss: 0.032113\n",
      "Training epoch: 135 [160/900 (18%)]\tLoss: 0.031222\n",
      "Training epoch: 135 [320/900 (35%)]\tLoss: 0.037449\n",
      "Training epoch: 135 [480/900 (53%)]\tLoss: 0.025087\n",
      "Training epoch: 135 [640/900 (70%)]\tLoss: 0.033186\n",
      "Training epoch: 135 [800/900 (88%)]\tLoss: 0.015460\n",
      "=========> Epoch: 135 Average loss: 0.0310\n",
      "Correlation coefficient: 0.6364\n",
      "Training epoch: 136 [0/900 (0%)]\tLoss: 0.019853\n",
      "Training epoch: 136 [160/900 (18%)]\tLoss: 0.028383\n",
      "Training epoch: 136 [320/900 (35%)]\tLoss: 0.025715\n",
      "Training epoch: 136 [480/900 (53%)]\tLoss: 0.020962\n",
      "Training epoch: 136 [640/900 (70%)]\tLoss: 0.011205\n",
      "Training epoch: 136 [800/900 (88%)]\tLoss: 0.022989\n",
      "=========> Epoch: 136 Average loss: 0.0261\n",
      "Correlation coefficient: 0.5957\n",
      "Training epoch: 137 [0/900 (0%)]\tLoss: 0.014226\n",
      "Training epoch: 137 [160/900 (18%)]\tLoss: 0.048173\n",
      "Training epoch: 137 [320/900 (35%)]\tLoss: 0.044302\n",
      "Training epoch: 137 [480/900 (53%)]\tLoss: 0.020144\n",
      "Training epoch: 137 [640/900 (70%)]\tLoss: 0.017380\n",
      "Training epoch: 137 [800/900 (88%)]\tLoss: 0.021104\n",
      "=========> Epoch: 137 Average loss: 0.0293\n",
      "Correlation coefficient: 0.6443\n",
      "Training epoch: 138 [0/900 (0%)]\tLoss: 0.017122\n",
      "Training epoch: 138 [160/900 (18%)]\tLoss: 0.017685\n",
      "Training epoch: 138 [320/900 (35%)]\tLoss: 0.013312\n",
      "Training epoch: 138 [480/900 (53%)]\tLoss: 0.007819\n",
      "Training epoch: 138 [640/900 (70%)]\tLoss: 0.019122\n",
      "Training epoch: 138 [800/900 (88%)]\tLoss: 0.017081\n",
      "=========> Epoch: 138 Average loss: 0.0209\n",
      "Correlation coefficient: 0.6354\n",
      "Training epoch: 139 [0/900 (0%)]\tLoss: 0.015577\n",
      "Training epoch: 139 [160/900 (18%)]\tLoss: 0.014298\n",
      "Training epoch: 139 [320/900 (35%)]\tLoss: 0.020455\n",
      "Training epoch: 139 [480/900 (53%)]\tLoss: 0.003983\n",
      "Training epoch: 139 [640/900 (70%)]\tLoss: 0.015019\n",
      "Training epoch: 139 [800/900 (88%)]\tLoss: 0.005803\n",
      "=========> Epoch: 139 Average loss: 0.0140\n",
      "Correlation coefficient: 0.6582\n",
      "Training epoch: 140 [0/900 (0%)]\tLoss: 0.007591\n",
      "Training epoch: 140 [160/900 (18%)]\tLoss: 0.003822\n",
      "Training epoch: 140 [320/900 (35%)]\tLoss: 0.003716\n",
      "Training epoch: 140 [480/900 (53%)]\tLoss: 0.008953\n",
      "Training epoch: 140 [640/900 (70%)]\tLoss: 0.006192\n",
      "Training epoch: 140 [800/900 (88%)]\tLoss: 0.005768\n",
      "=========> Epoch: 140 Average loss: 0.0064\n",
      "Correlation coefficient: 0.6570\n",
      "Training epoch: 141 [0/900 (0%)]\tLoss: 0.005042\n",
      "Training epoch: 141 [160/900 (18%)]\tLoss: 0.005134\n",
      "Training epoch: 141 [320/900 (35%)]\tLoss: 0.002908\n",
      "Training epoch: 141 [480/900 (53%)]\tLoss: 0.001001\n",
      "Training epoch: 141 [640/900 (70%)]\tLoss: 0.001548\n",
      "Training epoch: 141 [800/900 (88%)]\tLoss: 0.018189\n",
      "=========> Epoch: 141 Average loss: 0.0032\n",
      "Correlation coefficient: 0.6679\n",
      "✅ Epoch 141: New best correlation = 0.6679\n",
      "Training epoch: 142 [0/900 (0%)]\tLoss: 0.000646\n",
      "Training epoch: 142 [160/900 (18%)]\tLoss: 0.002664\n",
      "Training epoch: 142 [320/900 (35%)]\tLoss: 0.002466\n",
      "Training epoch: 142 [480/900 (53%)]\tLoss: 0.002451\n",
      "Training epoch: 142 [640/900 (70%)]\tLoss: 0.000699\n",
      "Training epoch: 142 [800/900 (88%)]\tLoss: 0.000674\n",
      "=========> Epoch: 142 Average loss: 0.0015\n",
      "Correlation coefficient: 0.6620\n",
      "Training epoch: 143 [0/900 (0%)]\tLoss: 0.000550\n",
      "Training epoch: 143 [160/900 (18%)]\tLoss: 0.000533\n",
      "Training epoch: 143 [320/900 (35%)]\tLoss: 0.001021\n",
      "Training epoch: 143 [480/900 (53%)]\tLoss: 0.000192\n",
      "Training epoch: 143 [640/900 (70%)]\tLoss: 0.000428\n",
      "Training epoch: 143 [800/900 (88%)]\tLoss: 0.000367\n",
      "=========> Epoch: 143 Average loss: 0.0005\n",
      "Correlation coefficient: 0.6659\n",
      "Training epoch: 144 [0/900 (0%)]\tLoss: 0.000099\n",
      "Training epoch: 144 [160/900 (18%)]\tLoss: 0.000141\n",
      "Training epoch: 144 [320/900 (35%)]\tLoss: 0.000191\n",
      "Training epoch: 144 [480/900 (53%)]\tLoss: 0.000184\n",
      "Training epoch: 144 [640/900 (70%)]\tLoss: 0.000178\n",
      "Training epoch: 144 [800/900 (88%)]\tLoss: 0.000130\n",
      "=========> Epoch: 144 Average loss: 0.0002\n",
      "Correlation coefficient: 0.6643\n",
      "Training epoch: 145 [0/900 (0%)]\tLoss: 0.000078\n",
      "Training epoch: 145 [160/900 (18%)]\tLoss: 0.000093\n",
      "Training epoch: 145 [320/900 (35%)]\tLoss: 0.000494\n",
      "Training epoch: 145 [480/900 (53%)]\tLoss: 0.000238\n",
      "Training epoch: 145 [640/900 (70%)]\tLoss: 0.000357\n",
      "Training epoch: 145 [800/900 (88%)]\tLoss: 0.000036\n",
      "=========> Epoch: 145 Average loss: 0.0001\n",
      "Correlation coefficient: 0.6642\n",
      "Training epoch: 146 [0/900 (0%)]\tLoss: 0.000024\n",
      "Training epoch: 146 [160/900 (18%)]\tLoss: 0.000058\n",
      "Training epoch: 146 [320/900 (35%)]\tLoss: 0.000025\n",
      "Training epoch: 146 [480/900 (53%)]\tLoss: 0.000083\n",
      "Training epoch: 146 [640/900 (70%)]\tLoss: 0.000143\n",
      "Training epoch: 146 [800/900 (88%)]\tLoss: 0.000038\n",
      "=========> Epoch: 146 Average loss: 0.0001\n",
      "Correlation coefficient: 0.6630\n",
      "Training epoch: 147 [0/900 (0%)]\tLoss: 0.000050\n",
      "Training epoch: 147 [160/900 (18%)]\tLoss: 0.000019\n",
      "Training epoch: 147 [320/900 (35%)]\tLoss: 0.000023\n",
      "Training epoch: 147 [480/900 (53%)]\tLoss: 0.000065\n",
      "Training epoch: 147 [640/900 (70%)]\tLoss: 0.000038\n",
      "Training epoch: 147 [800/900 (88%)]\tLoss: 0.000055\n",
      "=========> Epoch: 147 Average loss: 0.0001\n",
      "Correlation coefficient: 0.6655\n",
      "Training epoch: 148 [0/900 (0%)]\tLoss: 0.000027\n",
      "Training epoch: 148 [160/900 (18%)]\tLoss: 0.000050\n",
      "Training epoch: 148 [320/900 (35%)]\tLoss: 0.000030\n",
      "Training epoch: 148 [480/900 (53%)]\tLoss: 0.000053\n",
      "Training epoch: 148 [640/900 (70%)]\tLoss: 0.000022\n",
      "Training epoch: 148 [800/900 (88%)]\tLoss: 0.000020\n",
      "=========> Epoch: 148 Average loss: 0.0000\n",
      "Correlation coefficient: 0.6636\n",
      "Training epoch: 149 [0/900 (0%)]\tLoss: 0.000018\n",
      "Training epoch: 149 [160/900 (18%)]\tLoss: 0.000019\n",
      "Training epoch: 149 [320/900 (35%)]\tLoss: 0.000012\n",
      "Training epoch: 149 [480/900 (53%)]\tLoss: 0.000072\n",
      "Training epoch: 149 [640/900 (70%)]\tLoss: 0.000026\n",
      "Training epoch: 149 [800/900 (88%)]\tLoss: 0.000022\n",
      "=========> Epoch: 149 Average loss: 0.0000\n",
      "Correlation coefficient: 0.6647\n",
      "Training epoch: 150 [0/900 (0%)]\tLoss: 0.000025\n",
      "Training epoch: 150 [160/900 (18%)]\tLoss: 0.000496\n",
      "Training epoch: 150 [320/900 (35%)]\tLoss: 0.000034\n",
      "Training epoch: 150 [480/900 (53%)]\tLoss: 0.000024\n",
      "Training epoch: 150 [640/900 (70%)]\tLoss: 0.000012\n",
      "Training epoch: 150 [800/900 (88%)]\tLoss: 0.000046\n",
      "=========> Epoch: 150 Average loss: 0.0000\n",
      "Correlation coefficient: 0.6639\n",
      "Training epoch: 151 [0/900 (0%)]\tLoss: 0.000074\n",
      "Training epoch: 151 [160/900 (18%)]\tLoss: 0.000079\n",
      "Training epoch: 151 [320/900 (35%)]\tLoss: 0.000050\n",
      "Training epoch: 151 [480/900 (53%)]\tLoss: 0.000027\n",
      "Training epoch: 151 [640/900 (70%)]\tLoss: 0.000089\n",
      "Training epoch: 151 [800/900 (88%)]\tLoss: 0.000015\n",
      "=========> Epoch: 151 Average loss: 0.0001\n",
      "Correlation coefficient: 0.6647\n",
      "Training epoch: 152 [0/900 (0%)]\tLoss: 0.000018\n",
      "Training epoch: 152 [160/900 (18%)]\tLoss: 0.000083\n",
      "Training epoch: 152 [320/900 (35%)]\tLoss: 0.000303\n",
      "Training epoch: 152 [480/900 (53%)]\tLoss: 0.000044\n",
      "Training epoch: 152 [640/900 (70%)]\tLoss: 0.000096\n",
      "Training epoch: 152 [800/900 (88%)]\tLoss: 0.000077\n",
      "=========> Epoch: 152 Average loss: 0.0001\n",
      "Correlation coefficient: 0.6642\n",
      "Training epoch: 153 [0/900 (0%)]\tLoss: 0.000090\n",
      "Training epoch: 153 [160/900 (18%)]\tLoss: 0.000065\n",
      "Training epoch: 153 [320/900 (35%)]\tLoss: 0.000151\n",
      "Training epoch: 153 [480/900 (53%)]\tLoss: 0.000090\n",
      "Training epoch: 153 [640/900 (70%)]\tLoss: 0.000033\n",
      "Training epoch: 153 [800/900 (88%)]\tLoss: 0.000134\n",
      "=========> Epoch: 153 Average loss: 0.0001\n",
      "Correlation coefficient: 0.6657\n",
      "Training epoch: 154 [0/900 (0%)]\tLoss: 0.000170\n",
      "Training epoch: 154 [160/900 (18%)]\tLoss: 0.000061\n",
      "Training epoch: 154 [320/900 (35%)]\tLoss: 0.000163\n",
      "Training epoch: 154 [480/900 (53%)]\tLoss: 0.000086\n",
      "Training epoch: 154 [640/900 (70%)]\tLoss: 0.000088\n",
      "Training epoch: 154 [800/900 (88%)]\tLoss: 0.000068\n",
      "=========> Epoch: 154 Average loss: 0.0001\n",
      "Correlation coefficient: 0.6645\n",
      "Training epoch: 155 [0/900 (0%)]\tLoss: 0.000185\n",
      "Training epoch: 155 [160/900 (18%)]\tLoss: 0.000095\n",
      "Training epoch: 155 [320/900 (35%)]\tLoss: 0.000086\n",
      "Training epoch: 155 [480/900 (53%)]\tLoss: 0.000085\n",
      "Training epoch: 155 [640/900 (70%)]\tLoss: 0.000165\n",
      "Training epoch: 155 [800/900 (88%)]\tLoss: 0.000052\n",
      "=========> Epoch: 155 Average loss: 0.0002\n",
      "Correlation coefficient: 0.6634\n",
      "Training epoch: 156 [0/900 (0%)]\tLoss: 0.000125\n",
      "Training epoch: 156 [160/900 (18%)]\tLoss: 0.000074\n",
      "Training epoch: 156 [320/900 (35%)]\tLoss: 0.000099\n",
      "Training epoch: 156 [480/900 (53%)]\tLoss: 0.000209\n",
      "Training epoch: 156 [640/900 (70%)]\tLoss: 0.000132\n",
      "Training epoch: 156 [800/900 (88%)]\tLoss: 0.000432\n",
      "=========> Epoch: 156 Average loss: 0.0002\n",
      "Correlation coefficient: 0.6646\n",
      "Training epoch: 157 [0/900 (0%)]\tLoss: 0.000070\n",
      "Training epoch: 157 [160/900 (18%)]\tLoss: 0.000066\n",
      "Training epoch: 157 [320/900 (35%)]\tLoss: 0.000647\n",
      "Training epoch: 157 [480/900 (53%)]\tLoss: 0.000101\n",
      "Training epoch: 157 [640/900 (70%)]\tLoss: 0.000316\n",
      "Training epoch: 157 [800/900 (88%)]\tLoss: 0.000292\n",
      "=========> Epoch: 157 Average loss: 0.0002\n",
      "Correlation coefficient: 0.6635\n",
      "Training epoch: 158 [0/900 (0%)]\tLoss: 0.000056\n",
      "Training epoch: 158 [160/900 (18%)]\tLoss: 0.000265\n",
      "Training epoch: 158 [320/900 (35%)]\tLoss: 0.000073\n",
      "Training epoch: 158 [480/900 (53%)]\tLoss: 0.000504\n",
      "Training epoch: 158 [640/900 (70%)]\tLoss: 0.000476\n",
      "Training epoch: 158 [800/900 (88%)]\tLoss: 0.000148\n",
      "=========> Epoch: 158 Average loss: 0.0002\n",
      "Correlation coefficient: 0.6655\n",
      "Training epoch: 159 [0/900 (0%)]\tLoss: 0.000373\n",
      "Training epoch: 159 [160/900 (18%)]\tLoss: 0.000035\n",
      "Training epoch: 159 [320/900 (35%)]\tLoss: 0.000821\n",
      "Training epoch: 159 [480/900 (53%)]\tLoss: 0.000199\n",
      "Training epoch: 159 [640/900 (70%)]\tLoss: 0.000445\n",
      "Training epoch: 159 [800/900 (88%)]\tLoss: 0.000221\n",
      "=========> Epoch: 159 Average loss: 0.0003\n",
      "Correlation coefficient: 0.6620\n",
      "Training epoch: 160 [0/900 (0%)]\tLoss: 0.000307\n",
      "Training epoch: 160 [160/900 (18%)]\tLoss: 0.000337\n",
      "Training epoch: 160 [320/900 (35%)]\tLoss: 0.000652\n",
      "Training epoch: 160 [480/900 (53%)]\tLoss: 0.000530\n",
      "Training epoch: 160 [640/900 (70%)]\tLoss: 0.000678\n",
      "Training epoch: 160 [800/900 (88%)]\tLoss: 0.000182\n",
      "=========> Epoch: 160 Average loss: 0.0007\n",
      "Correlation coefficient: 0.6709\n",
      "✅ Epoch 160: New best correlation = 0.6709\n",
      "Training epoch: 161 [0/900 (0%)]\tLoss: 0.000825\n",
      "Training epoch: 161 [160/900 (18%)]\tLoss: 0.000296\n",
      "Training epoch: 161 [320/900 (35%)]\tLoss: 0.001546\n",
      "Training epoch: 161 [480/900 (53%)]\tLoss: 0.001123\n",
      "Training epoch: 161 [640/900 (70%)]\tLoss: 0.001896\n",
      "Training epoch: 161 [800/900 (88%)]\tLoss: 0.002024\n",
      "=========> Epoch: 161 Average loss: 0.0014\n",
      "Correlation coefficient: 0.6678\n",
      "Training epoch: 162 [0/900 (0%)]\tLoss: 0.001116\n",
      "Training epoch: 162 [160/900 (18%)]\tLoss: 0.003120\n",
      "Training epoch: 162 [320/900 (35%)]\tLoss: 0.000878\n",
      "Training epoch: 162 [480/900 (53%)]\tLoss: 0.001949\n",
      "Training epoch: 162 [640/900 (70%)]\tLoss: 0.015781\n",
      "Training epoch: 162 [800/900 (88%)]\tLoss: 0.003271\n",
      "=========> Epoch: 162 Average loss: 0.0025\n",
      "Correlation coefficient: 0.6681\n",
      "Training epoch: 163 [0/900 (0%)]\tLoss: 0.001534\n",
      "Training epoch: 163 [160/900 (18%)]\tLoss: 0.001336\n",
      "Training epoch: 163 [320/900 (35%)]\tLoss: 0.001390\n",
      "Training epoch: 163 [480/900 (53%)]\tLoss: 0.002742\n",
      "Training epoch: 163 [640/900 (70%)]\tLoss: 0.002639\n",
      "Training epoch: 163 [800/900 (88%)]\tLoss: 0.003559\n",
      "=========> Epoch: 163 Average loss: 0.0032\n",
      "Correlation coefficient: 0.6592\n",
      "Training epoch: 164 [0/900 (0%)]\tLoss: 0.005193\n",
      "Training epoch: 164 [160/900 (18%)]\tLoss: 0.002734\n",
      "Training epoch: 164 [320/900 (35%)]\tLoss: 0.003048\n",
      "Training epoch: 164 [480/900 (53%)]\tLoss: 0.006947\n",
      "Training epoch: 164 [640/900 (70%)]\tLoss: 0.009078\n",
      "Training epoch: 164 [800/900 (88%)]\tLoss: 0.002332\n",
      "=========> Epoch: 164 Average loss: 0.0060\n",
      "Correlation coefficient: 0.6573\n",
      "Training epoch: 165 [0/900 (0%)]\tLoss: 0.007626\n",
      "Training epoch: 165 [160/900 (18%)]\tLoss: 0.008181\n",
      "Training epoch: 165 [320/900 (35%)]\tLoss: 0.005618\n",
      "Training epoch: 165 [480/900 (53%)]\tLoss: 0.006868\n",
      "Training epoch: 165 [640/900 (70%)]\tLoss: 0.005292\n",
      "Training epoch: 165 [800/900 (88%)]\tLoss: 0.037273\n",
      "=========> Epoch: 165 Average loss: 0.0080\n",
      "Correlation coefficient: 0.6544\n",
      "Training epoch: 166 [0/900 (0%)]\tLoss: 0.020781\n",
      "Training epoch: 166 [160/900 (18%)]\tLoss: 0.010189\n",
      "Training epoch: 166 [320/900 (35%)]\tLoss: 0.013296\n",
      "Training epoch: 166 [480/900 (53%)]\tLoss: 0.012356\n",
      "Training epoch: 166 [640/900 (70%)]\tLoss: 0.008615\n",
      "Training epoch: 166 [800/900 (88%)]\tLoss: 0.007753\n",
      "=========> Epoch: 166 Average loss: 0.0099\n",
      "Correlation coefficient: 0.6608\n",
      "Training epoch: 167 [0/900 (0%)]\tLoss: 0.004575\n",
      "Training epoch: 167 [160/900 (18%)]\tLoss: 0.015357\n",
      "Training epoch: 167 [320/900 (35%)]\tLoss: 0.008907\n",
      "Training epoch: 167 [480/900 (53%)]\tLoss: 0.006259\n",
      "Training epoch: 167 [640/900 (70%)]\tLoss: 0.002881\n",
      "Training epoch: 167 [800/900 (88%)]\tLoss: 0.003276\n",
      "=========> Epoch: 167 Average loss: 0.0078\n",
      "Correlation coefficient: 0.6608\n",
      "Training epoch: 168 [0/900 (0%)]\tLoss: 0.004487\n",
      "Training epoch: 168 [160/900 (18%)]\tLoss: 0.003256\n",
      "Training epoch: 168 [320/900 (35%)]\tLoss: 0.012487\n",
      "Training epoch: 168 [480/900 (53%)]\tLoss: 0.005315\n",
      "Training epoch: 168 [640/900 (70%)]\tLoss: 0.002314\n",
      "Training epoch: 168 [800/900 (88%)]\tLoss: 0.005621\n",
      "=========> Epoch: 168 Average loss: 0.0059\n",
      "Correlation coefficient: 0.6578\n",
      "Training epoch: 169 [0/900 (0%)]\tLoss: 0.002919\n",
      "Training epoch: 169 [160/900 (18%)]\tLoss: 0.006220\n",
      "Training epoch: 169 [320/900 (35%)]\tLoss: 0.003004\n",
      "Training epoch: 169 [480/900 (53%)]\tLoss: 0.002920\n",
      "Training epoch: 169 [640/900 (70%)]\tLoss: 0.003215\n",
      "Training epoch: 169 [800/900 (88%)]\tLoss: 0.003593\n",
      "=========> Epoch: 169 Average loss: 0.0046\n",
      "Correlation coefficient: 0.6602\n",
      "Training epoch: 170 [0/900 (0%)]\tLoss: 0.001841\n",
      "Training epoch: 170 [160/900 (18%)]\tLoss: 0.004483\n",
      "Training epoch: 170 [320/900 (35%)]\tLoss: 0.013787\n",
      "Training epoch: 170 [480/900 (53%)]\tLoss: 0.001977\n",
      "Training epoch: 170 [640/900 (70%)]\tLoss: 0.002570\n",
      "Training epoch: 170 [800/900 (88%)]\tLoss: 0.009124\n",
      "=========> Epoch: 170 Average loss: 0.0067\n",
      "Correlation coefficient: 0.6512\n",
      "Training epoch: 171 [0/900 (0%)]\tLoss: 0.001678\n",
      "Training epoch: 171 [160/900 (18%)]\tLoss: 0.005769\n",
      "Training epoch: 171 [320/900 (35%)]\tLoss: 0.001170\n",
      "Training epoch: 171 [480/900 (53%)]\tLoss: 0.002428\n",
      "Training epoch: 171 [640/900 (70%)]\tLoss: 0.005744\n",
      "Training epoch: 171 [800/900 (88%)]\tLoss: 0.002007\n",
      "=========> Epoch: 171 Average loss: 0.0048\n",
      "Correlation coefficient: 0.6581\n",
      "Training epoch: 172 [0/900 (0%)]\tLoss: 0.007953\n",
      "Training epoch: 172 [160/900 (18%)]\tLoss: 0.004580\n",
      "Training epoch: 172 [320/900 (35%)]\tLoss: 0.006005\n",
      "Training epoch: 172 [480/900 (53%)]\tLoss: 0.002955\n",
      "Training epoch: 172 [640/900 (70%)]\tLoss: 0.003165\n",
      "Training epoch: 172 [800/900 (88%)]\tLoss: 0.002632\n",
      "=========> Epoch: 172 Average loss: 0.0026\n",
      "Correlation coefficient: 0.6449\n",
      "Training epoch: 173 [0/900 (0%)]\tLoss: 0.000704\n",
      "Training epoch: 173 [160/900 (18%)]\tLoss: 0.001055\n",
      "Training epoch: 173 [320/900 (35%)]\tLoss: 0.000964\n",
      "Training epoch: 173 [480/900 (53%)]\tLoss: 0.000916\n",
      "Training epoch: 173 [640/900 (70%)]\tLoss: 0.001288\n",
      "Training epoch: 173 [800/900 (88%)]\tLoss: 0.001137\n",
      "=========> Epoch: 173 Average loss: 0.0019\n",
      "Correlation coefficient: 0.6566\n",
      "Training epoch: 174 [0/900 (0%)]\tLoss: 0.001197\n",
      "Training epoch: 174 [160/900 (18%)]\tLoss: 0.019536\n",
      "Training epoch: 174 [320/900 (35%)]\tLoss: 0.005033\n",
      "Training epoch: 174 [480/900 (53%)]\tLoss: 0.002647\n",
      "Training epoch: 174 [640/900 (70%)]\tLoss: 0.002064\n",
      "Training epoch: 174 [800/900 (88%)]\tLoss: 0.002252\n",
      "=========> Epoch: 174 Average loss: 0.0029\n",
      "Correlation coefficient: 0.6534\n",
      "Training epoch: 175 [0/900 (0%)]\tLoss: 0.001296\n",
      "Training epoch: 175 [160/900 (18%)]\tLoss: 0.000780\n",
      "Training epoch: 175 [320/900 (35%)]\tLoss: 0.003736\n",
      "Training epoch: 175 [480/900 (53%)]\tLoss: 0.002919\n",
      "Training epoch: 175 [640/900 (70%)]\tLoss: 0.004245\n",
      "Training epoch: 175 [800/900 (88%)]\tLoss: 0.006622\n",
      "=========> Epoch: 175 Average loss: 0.0028\n",
      "Correlation coefficient: 0.6571\n",
      "Training epoch: 176 [0/900 (0%)]\tLoss: 0.005445\n",
      "Training epoch: 176 [160/900 (18%)]\tLoss: 0.002585\n",
      "Training epoch: 176 [320/900 (35%)]\tLoss: 0.008737\n",
      "Training epoch: 176 [480/900 (53%)]\tLoss: 0.003203\n",
      "Training epoch: 176 [640/900 (70%)]\tLoss: 0.001178\n",
      "Training epoch: 176 [800/900 (88%)]\tLoss: 0.000816\n",
      "=========> Epoch: 176 Average loss: 0.0057\n",
      "Correlation coefficient: 0.6643\n",
      "Training epoch: 177 [0/900 (0%)]\tLoss: 0.002546\n",
      "Training epoch: 177 [160/900 (18%)]\tLoss: 0.002808\n",
      "Training epoch: 177 [320/900 (35%)]\tLoss: 0.009017\n",
      "Training epoch: 177 [480/900 (53%)]\tLoss: 0.002595\n",
      "Training epoch: 177 [640/900 (70%)]\tLoss: 0.007488\n",
      "Training epoch: 177 [800/900 (88%)]\tLoss: 0.002086\n",
      "=========> Epoch: 177 Average loss: 0.0041\n",
      "Correlation coefficient: 0.6580\n",
      "Training epoch: 178 [0/900 (0%)]\tLoss: 0.004816\n",
      "Training epoch: 178 [160/900 (18%)]\tLoss: 0.002436\n",
      "Training epoch: 178 [320/900 (35%)]\tLoss: 0.002489\n",
      "Training epoch: 178 [480/900 (53%)]\tLoss: 0.003964\n",
      "Training epoch: 178 [640/900 (70%)]\tLoss: 0.001123\n",
      "Training epoch: 178 [800/900 (88%)]\tLoss: 0.002652\n",
      "=========> Epoch: 178 Average loss: 0.0023\n",
      "Correlation coefficient: 0.6566\n",
      "Training epoch: 179 [0/900 (0%)]\tLoss: 0.001783\n",
      "Training epoch: 179 [160/900 (18%)]\tLoss: 0.001940\n",
      "Training epoch: 179 [320/900 (35%)]\tLoss: 0.001741\n",
      "Training epoch: 179 [480/900 (53%)]\tLoss: 0.000457\n",
      "Training epoch: 179 [640/900 (70%)]\tLoss: 0.001214\n",
      "Training epoch: 179 [800/900 (88%)]\tLoss: 0.000794\n",
      "=========> Epoch: 179 Average loss: 0.0016\n",
      "Correlation coefficient: 0.6563\n",
      "Training epoch: 180 [0/900 (0%)]\tLoss: 0.000914\n",
      "Training epoch: 180 [160/900 (18%)]\tLoss: 0.001495\n",
      "Training epoch: 180 [320/900 (35%)]\tLoss: 0.001127\n",
      "Training epoch: 180 [480/900 (53%)]\tLoss: 0.000703\n",
      "Training epoch: 180 [640/900 (70%)]\tLoss: 0.000771\n",
      "Training epoch: 180 [800/900 (88%)]\tLoss: 0.004366\n",
      "=========> Epoch: 180 Average loss: 0.0014\n",
      "Correlation coefficient: 0.6563\n",
      "Training epoch: 181 [0/900 (0%)]\tLoss: 0.000832\n",
      "Training epoch: 181 [160/900 (18%)]\tLoss: 0.001028\n",
      "Training epoch: 181 [320/900 (35%)]\tLoss: 0.000836\n",
      "Training epoch: 181 [480/900 (53%)]\tLoss: 0.000357\n",
      "Training epoch: 181 [640/900 (70%)]\tLoss: 0.001389\n",
      "Training epoch: 181 [800/900 (88%)]\tLoss: 0.001104\n",
      "=========> Epoch: 181 Average loss: 0.0011\n",
      "Correlation coefficient: 0.6574\n",
      "Training epoch: 182 [0/900 (0%)]\tLoss: 0.000658\n",
      "Training epoch: 182 [160/900 (18%)]\tLoss: 0.000540\n",
      "Training epoch: 182 [320/900 (35%)]\tLoss: 0.000525\n",
      "Training epoch: 182 [480/900 (53%)]\tLoss: 0.002278\n",
      "Training epoch: 182 [640/900 (70%)]\tLoss: 0.000596\n",
      "Training epoch: 182 [800/900 (88%)]\tLoss: 0.000483\n",
      "=========> Epoch: 182 Average loss: 0.0008\n",
      "Correlation coefficient: 0.6582\n",
      "Training epoch: 183 [0/900 (0%)]\tLoss: 0.000181\n",
      "Training epoch: 183 [160/900 (18%)]\tLoss: 0.000544\n",
      "Training epoch: 183 [320/900 (35%)]\tLoss: 0.000475\n",
      "Training epoch: 183 [480/900 (53%)]\tLoss: 0.000264\n",
      "Training epoch: 183 [640/900 (70%)]\tLoss: 0.000371\n",
      "Training epoch: 183 [800/900 (88%)]\tLoss: 0.000355\n",
      "=========> Epoch: 183 Average loss: 0.0004\n",
      "Correlation coefficient: 0.6592\n",
      "Training epoch: 184 [0/900 (0%)]\tLoss: 0.000217\n",
      "Training epoch: 184 [160/900 (18%)]\tLoss: 0.000100\n",
      "Training epoch: 184 [320/900 (35%)]\tLoss: 0.000172\n",
      "Training epoch: 184 [480/900 (53%)]\tLoss: 0.000450\n",
      "Training epoch: 184 [640/900 (70%)]\tLoss: 0.000114\n",
      "Training epoch: 184 [800/900 (88%)]\tLoss: 0.000188\n",
      "=========> Epoch: 184 Average loss: 0.0004\n",
      "Correlation coefficient: 0.6611\n",
      "Training epoch: 185 [0/900 (0%)]\tLoss: 0.000025\n",
      "Training epoch: 185 [160/900 (18%)]\tLoss: 0.000761\n",
      "Training epoch: 185 [320/900 (35%)]\tLoss: 0.000226\n",
      "Training epoch: 185 [480/900 (53%)]\tLoss: 0.000999\n",
      "Training epoch: 185 [640/900 (70%)]\tLoss: 0.000370\n",
      "Training epoch: 185 [800/900 (88%)]\tLoss: 0.000202\n",
      "=========> Epoch: 185 Average loss: 0.0004\n",
      "Correlation coefficient: 0.6588\n",
      "Training epoch: 186 [0/900 (0%)]\tLoss: 0.000326\n",
      "Training epoch: 186 [160/900 (18%)]\tLoss: 0.000226\n",
      "Training epoch: 186 [320/900 (35%)]\tLoss: 0.000373\n",
      "Training epoch: 186 [480/900 (53%)]\tLoss: 0.000214\n",
      "Training epoch: 186 [640/900 (70%)]\tLoss: 0.000114\n",
      "Training epoch: 186 [800/900 (88%)]\tLoss: 0.000641\n",
      "=========> Epoch: 186 Average loss: 0.0003\n",
      "Correlation coefficient: 0.6610\n",
      "Training epoch: 187 [0/900 (0%)]\tLoss: 0.000202\n",
      "Training epoch: 187 [160/900 (18%)]\tLoss: 0.000257\n",
      "Training epoch: 187 [320/900 (35%)]\tLoss: 0.000293\n",
      "Training epoch: 187 [480/900 (53%)]\tLoss: 0.000274\n",
      "Training epoch: 187 [640/900 (70%)]\tLoss: 0.000242\n",
      "Training epoch: 187 [800/900 (88%)]\tLoss: 0.000386\n",
      "=========> Epoch: 187 Average loss: 0.0003\n",
      "Correlation coefficient: 0.6600\n",
      "Training epoch: 188 [0/900 (0%)]\tLoss: 0.000239\n",
      "Training epoch: 188 [160/900 (18%)]\tLoss: 0.000167\n",
      "Training epoch: 188 [320/900 (35%)]\tLoss: 0.000402\n",
      "Training epoch: 188 [480/900 (53%)]\tLoss: 0.000337\n",
      "Training epoch: 188 [640/900 (70%)]\tLoss: 0.000070\n",
      "Training epoch: 188 [800/900 (88%)]\tLoss: 0.000276\n",
      "=========> Epoch: 188 Average loss: 0.0003\n",
      "Correlation coefficient: 0.6600\n",
      "Training epoch: 189 [0/900 (0%)]\tLoss: 0.000219\n",
      "Training epoch: 189 [160/900 (18%)]\tLoss: 0.000301\n",
      "Training epoch: 189 [320/900 (35%)]\tLoss: 0.000329\n",
      "Training epoch: 189 [480/900 (53%)]\tLoss: 0.000443\n",
      "Training epoch: 189 [640/900 (70%)]\tLoss: 0.000346\n",
      "Training epoch: 189 [800/900 (88%)]\tLoss: 0.000276\n",
      "=========> Epoch: 189 Average loss: 0.0003\n",
      "Correlation coefficient: 0.6587\n",
      "Training epoch: 190 [0/900 (0%)]\tLoss: 0.000615\n",
      "Training epoch: 190 [160/900 (18%)]\tLoss: 0.000473\n",
      "Training epoch: 190 [320/900 (35%)]\tLoss: 0.000309\n",
      "Training epoch: 190 [480/900 (53%)]\tLoss: 0.000174\n",
      "Training epoch: 190 [640/900 (70%)]\tLoss: 0.000216\n",
      "Training epoch: 190 [800/900 (88%)]\tLoss: 0.000310\n",
      "=========> Epoch: 190 Average loss: 0.0005\n",
      "Correlation coefficient: 0.6584\n",
      "Training epoch: 191 [0/900 (0%)]\tLoss: 0.000094\n",
      "Training epoch: 191 [160/900 (18%)]\tLoss: 0.001577\n",
      "Training epoch: 191 [320/900 (35%)]\tLoss: 0.001160\n",
      "Training epoch: 191 [480/900 (53%)]\tLoss: 0.000769\n",
      "Training epoch: 191 [640/900 (70%)]\tLoss: 0.000593\n",
      "Training epoch: 191 [800/900 (88%)]\tLoss: 0.000914\n",
      "=========> Epoch: 191 Average loss: 0.0010\n",
      "Correlation coefficient: 0.6586\n",
      "Training epoch: 192 [0/900 (0%)]\tLoss: 0.003668\n",
      "Training epoch: 192 [160/900 (18%)]\tLoss: 0.001128\n",
      "Training epoch: 192 [320/900 (35%)]\tLoss: 0.002377\n",
      "Training epoch: 192 [480/900 (53%)]\tLoss: 0.000784\n",
      "Training epoch: 192 [640/900 (70%)]\tLoss: 0.000445\n",
      "Training epoch: 192 [800/900 (88%)]\tLoss: 0.000968\n",
      "=========> Epoch: 192 Average loss: 0.0014\n",
      "Correlation coefficient: 0.6621\n",
      "Training epoch: 193 [0/900 (0%)]\tLoss: 0.000928\n",
      "Training epoch: 193 [160/900 (18%)]\tLoss: 0.001638\n",
      "Training epoch: 193 [320/900 (35%)]\tLoss: 0.002422\n",
      "Training epoch: 193 [480/900 (53%)]\tLoss: 0.002599\n",
      "Training epoch: 193 [640/900 (70%)]\tLoss: 0.000810\n",
      "Training epoch: 193 [800/900 (88%)]\tLoss: 0.001431\n",
      "=========> Epoch: 193 Average loss: 0.0015\n",
      "Correlation coefficient: 0.6535\n",
      "Training epoch: 194 [0/900 (0%)]\tLoss: 0.001590\n",
      "Training epoch: 194 [160/900 (18%)]\tLoss: 0.002558\n",
      "Training epoch: 194 [320/900 (35%)]\tLoss: 0.002999\n",
      "Training epoch: 194 [480/900 (53%)]\tLoss: 0.001656\n",
      "Training epoch: 194 [640/900 (70%)]\tLoss: 0.002021\n",
      "Training epoch: 194 [800/900 (88%)]\tLoss: 0.005468\n",
      "=========> Epoch: 194 Average loss: 0.0019\n",
      "Correlation coefficient: 0.6573\n",
      "Training epoch: 195 [0/900 (0%)]\tLoss: 0.002475\n",
      "Training epoch: 195 [160/900 (18%)]\tLoss: 0.001832\n",
      "Training epoch: 195 [320/900 (35%)]\tLoss: 0.001570\n",
      "Training epoch: 195 [480/900 (53%)]\tLoss: 0.001784\n",
      "Training epoch: 195 [640/900 (70%)]\tLoss: 0.001715\n",
      "Training epoch: 195 [800/900 (88%)]\tLoss: 0.002537\n",
      "=========> Epoch: 195 Average loss: 0.0024\n",
      "Correlation coefficient: 0.6502\n",
      "Training epoch: 196 [0/900 (0%)]\tLoss: 0.001920\n",
      "Training epoch: 196 [160/900 (18%)]\tLoss: 0.017592\n",
      "Training epoch: 196 [320/900 (35%)]\tLoss: 0.006145\n",
      "Training epoch: 196 [480/900 (53%)]\tLoss: 0.001511\n",
      "Training epoch: 196 [640/900 (70%)]\tLoss: 0.002831\n",
      "Training epoch: 196 [800/900 (88%)]\tLoss: 0.002374\n",
      "=========> Epoch: 196 Average loss: 0.0034\n",
      "Correlation coefficient: 0.6586\n",
      "Training epoch: 197 [0/900 (0%)]\tLoss: 0.000734\n",
      "Training epoch: 197 [160/900 (18%)]\tLoss: 0.002279\n",
      "Training epoch: 197 [320/900 (35%)]\tLoss: 0.008329\n",
      "Training epoch: 197 [480/900 (53%)]\tLoss: 0.004403\n",
      "Training epoch: 197 [640/900 (70%)]\tLoss: 0.004905\n",
      "Training epoch: 197 [800/900 (88%)]\tLoss: 0.002920\n",
      "=========> Epoch: 197 Average loss: 0.0034\n",
      "Correlation coefficient: 0.6546\n",
      "Training epoch: 198 [0/900 (0%)]\tLoss: 0.006893\n",
      "Training epoch: 198 [160/900 (18%)]\tLoss: 0.002141\n",
      "Training epoch: 198 [320/900 (35%)]\tLoss: 0.002758\n",
      "Training epoch: 198 [480/900 (53%)]\tLoss: 0.001892\n",
      "Training epoch: 198 [640/900 (70%)]\tLoss: 0.002207\n",
      "Training epoch: 198 [800/900 (88%)]\tLoss: 0.004520\n",
      "=========> Epoch: 198 Average loss: 0.0040\n",
      "Correlation coefficient: 0.6518\n",
      "Training epoch: 199 [0/900 (0%)]\tLoss: 0.005782\n",
      "Training epoch: 199 [160/900 (18%)]\tLoss: 0.002870\n",
      "Training epoch: 199 [320/900 (35%)]\tLoss: 0.005321\n",
      "Training epoch: 199 [480/900 (53%)]\tLoss: 0.008665\n",
      "Training epoch: 199 [640/900 (70%)]\tLoss: 0.002418\n",
      "Training epoch: 199 [800/900 (88%)]\tLoss: 0.004215\n",
      "=========> Epoch: 199 Average loss: 0.0050\n",
      "Correlation coefficient: 0.6497\n",
      "Training epoch: 200 [0/900 (0%)]\tLoss: 0.007986\n",
      "Training epoch: 200 [160/900 (18%)]\tLoss: 0.004382\n",
      "Training epoch: 200 [320/900 (35%)]\tLoss: 0.003481\n",
      "Training epoch: 200 [480/900 (53%)]\tLoss: 0.007940\n",
      "Training epoch: 200 [640/900 (70%)]\tLoss: 0.007048\n",
      "Training epoch: 200 [800/900 (88%)]\tLoss: 0.005352\n",
      "=========> Epoch: 200 Average loss: 0.0050\n",
      "Correlation coefficient: 0.6728\n",
      "✅ Epoch 200: New best correlation = 0.6728\n",
      "Training epoch: 201 [0/900 (0%)]\tLoss: 0.001639\n",
      "Training epoch: 201 [160/900 (18%)]\tLoss: 0.004228\n",
      "Training epoch: 201 [320/900 (35%)]\tLoss: 0.004528\n",
      "Training epoch: 201 [480/900 (53%)]\tLoss: 0.003933\n",
      "Training epoch: 201 [640/900 (70%)]\tLoss: 0.004147\n",
      "Training epoch: 201 [800/900 (88%)]\tLoss: 0.007867\n",
      "=========> Epoch: 201 Average loss: 0.0047\n",
      "Correlation coefficient: 0.6463\n",
      "Training epoch: 202 [0/900 (0%)]\tLoss: 0.002909\n",
      "Training epoch: 202 [160/900 (18%)]\tLoss: 0.005036\n",
      "Training epoch: 202 [320/900 (35%)]\tLoss: 0.004083\n",
      "Training epoch: 202 [480/900 (53%)]\tLoss: 0.017728\n",
      "Training epoch: 202 [640/900 (70%)]\tLoss: 0.002428\n",
      "Training epoch: 202 [800/900 (88%)]\tLoss: 0.003434\n",
      "=========> Epoch: 202 Average loss: 0.0059\n",
      "Correlation coefficient: 0.6457\n",
      "Training epoch: 203 [0/900 (0%)]\tLoss: 0.003219\n",
      "Training epoch: 203 [160/900 (18%)]\tLoss: 0.006445\n",
      "Training epoch: 203 [320/900 (35%)]\tLoss: 0.002116\n",
      "Training epoch: 203 [480/900 (53%)]\tLoss: 0.009046\n",
      "Training epoch: 203 [640/900 (70%)]\tLoss: 0.001841\n",
      "Training epoch: 203 [800/900 (88%)]\tLoss: 0.006225\n",
      "=========> Epoch: 203 Average loss: 0.0053\n",
      "Correlation coefficient: 0.6452\n",
      "Training epoch: 204 [0/900 (0%)]\tLoss: 0.003076\n",
      "Training epoch: 204 [160/900 (18%)]\tLoss: 0.011596\n",
      "Training epoch: 204 [320/900 (35%)]\tLoss: 0.002329\n",
      "Training epoch: 204 [480/900 (53%)]\tLoss: 0.001723\n",
      "Training epoch: 204 [640/900 (70%)]\tLoss: 0.001086\n",
      "Training epoch: 204 [800/900 (88%)]\tLoss: 0.004297\n",
      "=========> Epoch: 204 Average loss: 0.0060\n",
      "Correlation coefficient: 0.6617\n",
      "Training epoch: 205 [0/900 (0%)]\tLoss: 0.005212\n",
      "Training epoch: 205 [160/900 (18%)]\tLoss: 0.009673\n",
      "Training epoch: 205 [320/900 (35%)]\tLoss: 0.056157\n",
      "Training epoch: 205 [480/900 (53%)]\tLoss: 0.021574\n",
      "Training epoch: 205 [640/900 (70%)]\tLoss: 0.009977\n",
      "Training epoch: 205 [800/900 (88%)]\tLoss: 0.010516\n",
      "=========> Epoch: 205 Average loss: 0.0101\n",
      "Correlation coefficient: 0.6372\n",
      "Training epoch: 206 [0/900 (0%)]\tLoss: 0.008345\n",
      "Training epoch: 206 [160/900 (18%)]\tLoss: 0.007194\n",
      "Training epoch: 206 [320/900 (35%)]\tLoss: 0.009792\n",
      "Training epoch: 206 [480/900 (53%)]\tLoss: 0.008643\n",
      "Training epoch: 206 [640/900 (70%)]\tLoss: 0.008486\n",
      "Training epoch: 206 [800/900 (88%)]\tLoss: 0.017876\n",
      "=========> Epoch: 206 Average loss: 0.0097\n",
      "Correlation coefficient: 0.6534\n",
      "Training epoch: 207 [0/900 (0%)]\tLoss: 0.009571\n",
      "Training epoch: 207 [160/900 (18%)]\tLoss: 0.010428\n",
      "Training epoch: 207 [320/900 (35%)]\tLoss: 0.008853\n",
      "Training epoch: 207 [480/900 (53%)]\tLoss: 0.004909\n",
      "Training epoch: 207 [640/900 (70%)]\tLoss: 0.002992\n",
      "Training epoch: 207 [800/900 (88%)]\tLoss: 0.005507\n",
      "=========> Epoch: 207 Average loss: 0.0117\n",
      "Correlation coefficient: 0.6692\n",
      "Training epoch: 208 [0/900 (0%)]\tLoss: 0.026132\n",
      "Training epoch: 208 [160/900 (18%)]\tLoss: 0.004045\n",
      "Training epoch: 208 [320/900 (35%)]\tLoss: 0.010404\n",
      "Training epoch: 208 [480/900 (53%)]\tLoss: 0.005248\n",
      "Training epoch: 208 [640/900 (70%)]\tLoss: 0.012714\n",
      "Training epoch: 208 [800/900 (88%)]\tLoss: 0.032300\n",
      "=========> Epoch: 208 Average loss: 0.0125\n",
      "Correlation coefficient: 0.6661\n",
      "Training epoch: 209 [0/900 (0%)]\tLoss: 0.001839\n",
      "Training epoch: 209 [160/900 (18%)]\tLoss: 0.008099\n",
      "Training epoch: 209 [320/900 (35%)]\tLoss: 0.007825\n",
      "Training epoch: 209 [480/900 (53%)]\tLoss: 0.016010\n",
      "Training epoch: 209 [640/900 (70%)]\tLoss: 0.006157\n",
      "Training epoch: 209 [800/900 (88%)]\tLoss: 0.002124\n",
      "=========> Epoch: 209 Average loss: 0.0085\n",
      "Correlation coefficient: 0.6127\n",
      "Training epoch: 210 [0/900 (0%)]\tLoss: 0.003716\n",
      "Training epoch: 210 [160/900 (18%)]\tLoss: 0.004847\n",
      "Training epoch: 210 [320/900 (35%)]\tLoss: 0.024249\n",
      "Training epoch: 210 [480/900 (53%)]\tLoss: 0.004741\n",
      "Training epoch: 210 [640/900 (70%)]\tLoss: 0.005207\n",
      "Training epoch: 210 [800/900 (88%)]\tLoss: 0.001585\n",
      "=========> Epoch: 210 Average loss: 0.0059\n",
      "Correlation coefficient: 0.6588\n",
      "Training epoch: 211 [0/900 (0%)]\tLoss: 0.003058\n",
      "Training epoch: 211 [160/900 (18%)]\tLoss: 0.005548\n",
      "Training epoch: 211 [320/900 (35%)]\tLoss: 0.001880\n",
      "Training epoch: 211 [480/900 (53%)]\tLoss: 0.004345\n",
      "Training epoch: 211 [640/900 (70%)]\tLoss: 0.004146\n",
      "Training epoch: 211 [800/900 (88%)]\tLoss: 0.002912\n",
      "=========> Epoch: 211 Average loss: 0.0036\n",
      "Correlation coefficient: 0.6519\n",
      "Training epoch: 212 [0/900 (0%)]\tLoss: 0.001595\n",
      "Training epoch: 212 [160/900 (18%)]\tLoss: 0.000675\n",
      "Training epoch: 212 [320/900 (35%)]\tLoss: 0.000551\n",
      "Training epoch: 212 [480/900 (53%)]\tLoss: 0.001707\n",
      "Training epoch: 212 [640/900 (70%)]\tLoss: 0.001330\n",
      "Training epoch: 212 [800/900 (88%)]\tLoss: 0.001000\n",
      "=========> Epoch: 212 Average loss: 0.0019\n",
      "Correlation coefficient: 0.6573\n",
      "Training epoch: 213 [0/900 (0%)]\tLoss: 0.001411\n",
      "Training epoch: 213 [160/900 (18%)]\tLoss: 0.000362\n",
      "Training epoch: 213 [320/900 (35%)]\tLoss: 0.000552\n",
      "Training epoch: 213 [480/900 (53%)]\tLoss: 0.000467\n",
      "Training epoch: 213 [640/900 (70%)]\tLoss: 0.001213\n",
      "Training epoch: 213 [800/900 (88%)]\tLoss: 0.001019\n",
      "=========> Epoch: 213 Average loss: 0.0011\n",
      "Correlation coefficient: 0.6558\n",
      "Training epoch: 214 [0/900 (0%)]\tLoss: 0.000346\n",
      "Training epoch: 214 [160/900 (18%)]\tLoss: 0.000455\n",
      "Training epoch: 214 [320/900 (35%)]\tLoss: 0.000303\n",
      "Training epoch: 214 [480/900 (53%)]\tLoss: 0.000640\n",
      "Training epoch: 214 [640/900 (70%)]\tLoss: 0.000485\n",
      "Training epoch: 214 [800/900 (88%)]\tLoss: 0.000773\n",
      "=========> Epoch: 214 Average loss: 0.0005\n",
      "Correlation coefficient: 0.6535\n",
      "Training epoch: 215 [0/900 (0%)]\tLoss: 0.000130\n",
      "Training epoch: 215 [160/900 (18%)]\tLoss: 0.000407\n",
      "Training epoch: 215 [320/900 (35%)]\tLoss: 0.000255\n",
      "Training epoch: 215 [480/900 (53%)]\tLoss: 0.000287\n",
      "Training epoch: 215 [640/900 (70%)]\tLoss: 0.000170\n",
      "Training epoch: 215 [800/900 (88%)]\tLoss: 0.000089\n",
      "=========> Epoch: 215 Average loss: 0.0003\n",
      "Correlation coefficient: 0.6547\n",
      "Training epoch: 216 [0/900 (0%)]\tLoss: 0.000131\n",
      "Training epoch: 216 [160/900 (18%)]\tLoss: 0.000237\n",
      "Training epoch: 216 [320/900 (35%)]\tLoss: 0.000073\n",
      "Training epoch: 216 [480/900 (53%)]\tLoss: 0.000092\n",
      "Training epoch: 216 [640/900 (70%)]\tLoss: 0.000128\n",
      "Training epoch: 216 [800/900 (88%)]\tLoss: 0.000097\n",
      "=========> Epoch: 216 Average loss: 0.0002\n",
      "Correlation coefficient: 0.6554\n",
      "Training epoch: 217 [0/900 (0%)]\tLoss: 0.000067\n",
      "Training epoch: 217 [160/900 (18%)]\tLoss: 0.000284\n",
      "Training epoch: 217 [320/900 (35%)]\tLoss: 0.000042\n",
      "Training epoch: 217 [480/900 (53%)]\tLoss: 0.000087\n",
      "Training epoch: 217 [640/900 (70%)]\tLoss: 0.000022\n",
      "Training epoch: 217 [800/900 (88%)]\tLoss: 0.000060\n",
      "=========> Epoch: 217 Average loss: 0.0002\n",
      "Correlation coefficient: 0.6533\n",
      "Training epoch: 218 [0/900 (0%)]\tLoss: 0.000106\n",
      "Training epoch: 218 [160/900 (18%)]\tLoss: 0.000040\n",
      "Training epoch: 218 [320/900 (35%)]\tLoss: 0.000086\n",
      "Training epoch: 218 [480/900 (53%)]\tLoss: 0.000038\n",
      "Training epoch: 218 [640/900 (70%)]\tLoss: 0.000083\n",
      "Training epoch: 218 [800/900 (88%)]\tLoss: 0.000084\n",
      "=========> Epoch: 218 Average loss: 0.0001\n",
      "Correlation coefficient: 0.6551\n",
      "Training epoch: 219 [0/900 (0%)]\tLoss: 0.000035\n",
      "Training epoch: 219 [160/900 (18%)]\tLoss: 0.000065\n",
      "Training epoch: 219 [320/900 (35%)]\tLoss: 0.000023\n",
      "Training epoch: 219 [480/900 (53%)]\tLoss: 0.000086\n",
      "Training epoch: 219 [640/900 (70%)]\tLoss: 0.000034\n",
      "Training epoch: 219 [800/900 (88%)]\tLoss: 0.000024\n",
      "=========> Epoch: 219 Average loss: 0.0001\n",
      "Correlation coefficient: 0.6548\n",
      "Training epoch: 220 [0/900 (0%)]\tLoss: 0.000021\n",
      "Training epoch: 220 [160/900 (18%)]\tLoss: 0.000053\n",
      "Training epoch: 220 [320/900 (35%)]\tLoss: 0.000111\n",
      "Training epoch: 220 [480/900 (53%)]\tLoss: 0.000045\n",
      "Training epoch: 220 [640/900 (70%)]\tLoss: 0.000025\n",
      "Training epoch: 220 [800/900 (88%)]\tLoss: 0.000025\n",
      "=========> Epoch: 220 Average loss: 0.0000\n",
      "Correlation coefficient: 0.6552\n",
      "Training epoch: 221 [0/900 (0%)]\tLoss: 0.000144\n",
      "Training epoch: 221 [160/900 (18%)]\tLoss: 0.000221\n",
      "Training epoch: 221 [320/900 (35%)]\tLoss: 0.000013\n",
      "Training epoch: 221 [480/900 (53%)]\tLoss: 0.000035\n",
      "Training epoch: 221 [640/900 (70%)]\tLoss: 0.000031\n",
      "Training epoch: 221 [800/900 (88%)]\tLoss: 0.000029\n",
      "=========> Epoch: 221 Average loss: 0.0001\n",
      "Correlation coefficient: 0.6541\n",
      "Training epoch: 222 [0/900 (0%)]\tLoss: 0.000064\n",
      "Training epoch: 222 [160/900 (18%)]\tLoss: 0.000181\n",
      "Training epoch: 222 [320/900 (35%)]\tLoss: 0.000053\n",
      "Training epoch: 222 [480/900 (53%)]\tLoss: 0.000053\n",
      "Training epoch: 222 [640/900 (70%)]\tLoss: 0.000032\n",
      "Training epoch: 222 [800/900 (88%)]\tLoss: 0.000043\n",
      "=========> Epoch: 222 Average loss: 0.0001\n",
      "Correlation coefficient: 0.6545\n",
      "Training epoch: 223 [0/900 (0%)]\tLoss: 0.000082\n",
      "Training epoch: 223 [160/900 (18%)]\tLoss: 0.000103\n",
      "Training epoch: 223 [320/900 (35%)]\tLoss: 0.000024\n",
      "Training epoch: 223 [480/900 (53%)]\tLoss: 0.000606\n",
      "Training epoch: 223 [640/900 (70%)]\tLoss: 0.000062\n",
      "Training epoch: 223 [800/900 (88%)]\tLoss: 0.000037\n",
      "=========> Epoch: 223 Average loss: 0.0001\n",
      "Correlation coefficient: 0.6557\n",
      "Training epoch: 224 [0/900 (0%)]\tLoss: 0.000125\n",
      "Training epoch: 224 [160/900 (18%)]\tLoss: 0.000080\n",
      "Training epoch: 224 [320/900 (35%)]\tLoss: 0.000118\n",
      "Training epoch: 224 [480/900 (53%)]\tLoss: 0.000088\n",
      "Training epoch: 224 [640/900 (70%)]\tLoss: 0.000129\n",
      "Training epoch: 224 [800/900 (88%)]\tLoss: 0.000057\n",
      "=========> Epoch: 224 Average loss: 0.0001\n",
      "Correlation coefficient: 0.6535\n",
      "Training epoch: 225 [0/900 (0%)]\tLoss: 0.000115\n",
      "Training epoch: 225 [160/900 (18%)]\tLoss: 0.000088\n",
      "Training epoch: 225 [320/900 (35%)]\tLoss: 0.000106\n",
      "Training epoch: 225 [480/900 (53%)]\tLoss: 0.000047\n",
      "Training epoch: 225 [640/900 (70%)]\tLoss: 0.000314\n",
      "Training epoch: 225 [800/900 (88%)]\tLoss: 0.000175\n",
      "=========> Epoch: 225 Average loss: 0.0002\n",
      "Correlation coefficient: 0.6575\n",
      "Training epoch: 226 [0/900 (0%)]\tLoss: 0.000176\n",
      "Training epoch: 226 [160/900 (18%)]\tLoss: 0.000148\n",
      "Training epoch: 226 [320/900 (35%)]\tLoss: 0.000373\n",
      "Training epoch: 226 [480/900 (53%)]\tLoss: 0.000244\n",
      "Training epoch: 226 [640/900 (70%)]\tLoss: 0.000390\n",
      "Training epoch: 226 [800/900 (88%)]\tLoss: 0.000238\n",
      "=========> Epoch: 226 Average loss: 0.0003\n",
      "Correlation coefficient: 0.6520\n",
      "Training epoch: 227 [0/900 (0%)]\tLoss: 0.000082\n",
      "Training epoch: 227 [160/900 (18%)]\tLoss: 0.000323\n",
      "Training epoch: 227 [320/900 (35%)]\tLoss: 0.000148\n",
      "Training epoch: 227 [480/900 (53%)]\tLoss: 0.000200\n",
      "Training epoch: 227 [640/900 (70%)]\tLoss: 0.000358\n",
      "Training epoch: 227 [800/900 (88%)]\tLoss: 0.000388\n",
      "=========> Epoch: 227 Average loss: 0.0006\n",
      "Correlation coefficient: 0.6550\n",
      "Training epoch: 228 [0/900 (0%)]\tLoss: 0.001213\n",
      "Training epoch: 228 [160/900 (18%)]\tLoss: 0.000176\n",
      "Training epoch: 228 [320/900 (35%)]\tLoss: 0.000685\n",
      "Training epoch: 228 [480/900 (53%)]\tLoss: 0.000175\n",
      "Training epoch: 228 [640/900 (70%)]\tLoss: 0.000752\n",
      "Training epoch: 228 [800/900 (88%)]\tLoss: 0.000635\n",
      "=========> Epoch: 228 Average loss: 0.0011\n",
      "Correlation coefficient: 0.6509\n",
      "Training epoch: 229 [0/900 (0%)]\tLoss: 0.000784\n",
      "Training epoch: 229 [160/900 (18%)]\tLoss: 0.001028\n",
      "Training epoch: 229 [320/900 (35%)]\tLoss: 0.000507\n",
      "Training epoch: 229 [480/900 (53%)]\tLoss: 0.000233\n",
      "Training epoch: 229 [640/900 (70%)]\tLoss: 0.000957\n",
      "Training epoch: 229 [800/900 (88%)]\tLoss: 0.000774\n",
      "=========> Epoch: 229 Average loss: 0.0011\n",
      "Correlation coefficient: 0.6534\n",
      "Training epoch: 230 [0/900 (0%)]\tLoss: 0.001353\n",
      "Training epoch: 230 [160/900 (18%)]\tLoss: 0.000928\n",
      "Training epoch: 230 [320/900 (35%)]\tLoss: 0.001754\n",
      "Training epoch: 230 [480/900 (53%)]\tLoss: 0.000399\n",
      "Training epoch: 230 [640/900 (70%)]\tLoss: 0.001624\n",
      "Training epoch: 230 [800/900 (88%)]\tLoss: 0.000667\n",
      "=========> Epoch: 230 Average loss: 0.0012\n",
      "Correlation coefficient: 0.6560\n",
      "Training epoch: 231 [0/900 (0%)]\tLoss: 0.006020\n",
      "Training epoch: 231 [160/900 (18%)]\tLoss: 0.001371\n",
      "Training epoch: 231 [320/900 (35%)]\tLoss: 0.001711\n",
      "Training epoch: 231 [480/900 (53%)]\tLoss: 0.001457\n",
      "Training epoch: 231 [640/900 (70%)]\tLoss: 0.002531\n",
      "Training epoch: 231 [800/900 (88%)]\tLoss: 0.000645\n",
      "=========> Epoch: 231 Average loss: 0.0016\n",
      "Correlation coefficient: 0.6605\n",
      "Training epoch: 232 [0/900 (0%)]\tLoss: 0.003126\n",
      "Training epoch: 232 [160/900 (18%)]\tLoss: 0.000921\n",
      "Training epoch: 232 [320/900 (35%)]\tLoss: 0.002586\n",
      "Training epoch: 232 [480/900 (53%)]\tLoss: 0.001413\n",
      "Training epoch: 232 [640/900 (70%)]\tLoss: 0.001765\n",
      "Training epoch: 232 [800/900 (88%)]\tLoss: 0.000997\n",
      "=========> Epoch: 232 Average loss: 0.0025\n",
      "Correlation coefficient: 0.6521\n",
      "Training epoch: 233 [0/900 (0%)]\tLoss: 0.008626\n",
      "Training epoch: 233 [160/900 (18%)]\tLoss: 0.005921\n",
      "Training epoch: 233 [320/900 (35%)]\tLoss: 0.002392\n",
      "Training epoch: 233 [480/900 (53%)]\tLoss: 0.001811\n",
      "Training epoch: 233 [640/900 (70%)]\tLoss: 0.003636\n",
      "Training epoch: 233 [800/900 (88%)]\tLoss: 0.005103\n",
      "=========> Epoch: 233 Average loss: 0.0048\n",
      "Correlation coefficient: 0.6656\n",
      "Training epoch: 234 [0/900 (0%)]\tLoss: 0.003978\n",
      "Training epoch: 234 [160/900 (18%)]\tLoss: 0.006974\n",
      "Training epoch: 234 [320/900 (35%)]\tLoss: 0.001999\n",
      "Training epoch: 234 [480/900 (53%)]\tLoss: 0.004073\n",
      "Training epoch: 234 [640/900 (70%)]\tLoss: 0.004089\n",
      "Training epoch: 234 [800/900 (88%)]\tLoss: 0.008705\n",
      "=========> Epoch: 234 Average loss: 0.0055\n",
      "Correlation coefficient: 0.6544\n",
      "Training epoch: 235 [0/900 (0%)]\tLoss: 0.002894\n",
      "Training epoch: 235 [160/900 (18%)]\tLoss: 0.002925\n",
      "Training epoch: 235 [320/900 (35%)]\tLoss: 0.008022\n",
      "Training epoch: 235 [480/900 (53%)]\tLoss: 0.002871\n",
      "Training epoch: 235 [640/900 (70%)]\tLoss: 0.003288\n",
      "Training epoch: 235 [800/900 (88%)]\tLoss: 0.003053\n",
      "=========> Epoch: 235 Average loss: 0.0047\n",
      "Correlation coefficient: 0.6507\n",
      "Training epoch: 236 [0/900 (0%)]\tLoss: 0.003832\n",
      "Training epoch: 236 [160/900 (18%)]\tLoss: 0.009250\n",
      "Training epoch: 236 [320/900 (35%)]\tLoss: 0.003885\n",
      "Training epoch: 236 [480/900 (53%)]\tLoss: 0.001104\n",
      "Training epoch: 236 [640/900 (70%)]\tLoss: 0.008139\n",
      "Training epoch: 236 [800/900 (88%)]\tLoss: 0.002180\n",
      "=========> Epoch: 236 Average loss: 0.0046\n",
      "Correlation coefficient: 0.6607\n",
      "Training epoch: 237 [0/900 (0%)]\tLoss: 0.004656\n",
      "Training epoch: 237 [160/900 (18%)]\tLoss: 0.012652\n",
      "Training epoch: 237 [320/900 (35%)]\tLoss: 0.006564\n",
      "Training epoch: 237 [480/900 (53%)]\tLoss: 0.002607\n",
      "Training epoch: 237 [640/900 (70%)]\tLoss: 0.003198\n",
      "Training epoch: 237 [800/900 (88%)]\tLoss: 0.002843\n",
      "=========> Epoch: 237 Average loss: 0.0047\n",
      "Correlation coefficient: 0.6513\n",
      "Training epoch: 238 [0/900 (0%)]\tLoss: 0.004789\n",
      "Training epoch: 238 [160/900 (18%)]\tLoss: 0.002713\n",
      "Training epoch: 238 [320/900 (35%)]\tLoss: 0.001156\n",
      "Training epoch: 238 [480/900 (53%)]\tLoss: 0.003599\n",
      "Training epoch: 238 [640/900 (70%)]\tLoss: 0.001583\n",
      "Training epoch: 238 [800/900 (88%)]\tLoss: 0.001242\n",
      "=========> Epoch: 238 Average loss: 0.0035\n",
      "Correlation coefficient: 0.6502\n",
      "Training epoch: 239 [0/900 (0%)]\tLoss: 0.001201\n",
      "Training epoch: 239 [160/900 (18%)]\tLoss: 0.003929\n",
      "Training epoch: 239 [320/900 (35%)]\tLoss: 0.010012\n",
      "Training epoch: 239 [480/900 (53%)]\tLoss: 0.000854\n",
      "Training epoch: 239 [640/900 (70%)]\tLoss: 0.003662\n",
      "Training epoch: 239 [800/900 (88%)]\tLoss: 0.001963\n",
      "=========> Epoch: 239 Average loss: 0.0032\n",
      "Correlation coefficient: 0.6455\n",
      "Training epoch: 240 [0/900 (0%)]\tLoss: 0.005910\n",
      "Training epoch: 240 [160/900 (18%)]\tLoss: 0.001539\n",
      "Training epoch: 240 [320/900 (35%)]\tLoss: 0.003192\n",
      "Training epoch: 240 [480/900 (53%)]\tLoss: 0.003832\n",
      "Training epoch: 240 [640/900 (70%)]\tLoss: 0.002313\n",
      "Training epoch: 240 [800/900 (88%)]\tLoss: 0.004625\n",
      "=========> Epoch: 240 Average loss: 0.0037\n",
      "Correlation coefficient: 0.6582\n",
      "Training epoch: 241 [0/900 (0%)]\tLoss: 0.004353\n",
      "Training epoch: 241 [160/900 (18%)]\tLoss: 0.002859\n",
      "Training epoch: 241 [320/900 (35%)]\tLoss: 0.003191\n",
      "Training epoch: 241 [480/900 (53%)]\tLoss: 0.001953\n",
      "Training epoch: 241 [640/900 (70%)]\tLoss: 0.010160\n",
      "Training epoch: 241 [800/900 (88%)]\tLoss: 0.002320\n",
      "=========> Epoch: 241 Average loss: 0.0030\n",
      "Correlation coefficient: 0.6616\n",
      "Training epoch: 242 [0/900 (0%)]\tLoss: 0.002059\n",
      "Training epoch: 242 [160/900 (18%)]\tLoss: 0.002016\n",
      "Training epoch: 242 [320/900 (35%)]\tLoss: 0.001332\n",
      "Training epoch: 242 [480/900 (53%)]\tLoss: 0.001528\n",
      "Training epoch: 242 [640/900 (70%)]\tLoss: 0.002639\n",
      "Training epoch: 242 [800/900 (88%)]\tLoss: 0.000883\n",
      "=========> Epoch: 242 Average loss: 0.0016\n",
      "Correlation coefficient: 0.6589\n",
      "Training epoch: 243 [0/900 (0%)]\tLoss: 0.000687\n",
      "Training epoch: 243 [160/900 (18%)]\tLoss: 0.000708\n",
      "Training epoch: 243 [320/900 (35%)]\tLoss: 0.000384\n",
      "Training epoch: 243 [480/900 (53%)]\tLoss: 0.000407\n",
      "Training epoch: 243 [640/900 (70%)]\tLoss: 0.001170\n",
      "Training epoch: 243 [800/900 (88%)]\tLoss: 0.000841\n",
      "=========> Epoch: 243 Average loss: 0.0011\n",
      "Correlation coefficient: 0.6622\n",
      "Training epoch: 244 [0/900 (0%)]\tLoss: 0.000502\n",
      "Training epoch: 244 [160/900 (18%)]\tLoss: 0.000997\n",
      "Training epoch: 244 [320/900 (35%)]\tLoss: 0.000518\n",
      "Training epoch: 244 [480/900 (53%)]\tLoss: 0.000140\n",
      "Training epoch: 244 [640/900 (70%)]\tLoss: 0.000679\n",
      "Training epoch: 244 [800/900 (88%)]\tLoss: 0.001201\n",
      "=========> Epoch: 244 Average loss: 0.0007\n",
      "Correlation coefficient: 0.6622\n",
      "Training epoch: 245 [0/900 (0%)]\tLoss: 0.000884\n",
      "Training epoch: 245 [160/900 (18%)]\tLoss: 0.000105\n",
      "Training epoch: 245 [320/900 (35%)]\tLoss: 0.000720\n",
      "Training epoch: 245 [480/900 (53%)]\tLoss: 0.000662\n",
      "Training epoch: 245 [640/900 (70%)]\tLoss: 0.000393\n",
      "Training epoch: 245 [800/900 (88%)]\tLoss: 0.000158\n",
      "=========> Epoch: 245 Average loss: 0.0005\n",
      "Correlation coefficient: 0.6610\n",
      "Training epoch: 246 [0/900 (0%)]\tLoss: 0.000962\n",
      "Training epoch: 246 [160/900 (18%)]\tLoss: 0.000274\n",
      "Training epoch: 246 [320/900 (35%)]\tLoss: 0.000564\n",
      "Training epoch: 246 [480/900 (53%)]\tLoss: 0.000478\n",
      "Training epoch: 246 [640/900 (70%)]\tLoss: 0.000264\n",
      "Training epoch: 246 [800/900 (88%)]\tLoss: 0.000066\n",
      "=========> Epoch: 246 Average loss: 0.0004\n",
      "Correlation coefficient: 0.6598\n",
      "Training epoch: 247 [0/900 (0%)]\tLoss: 0.000153\n",
      "Training epoch: 247 [160/900 (18%)]\tLoss: 0.000164\n",
      "Training epoch: 247 [320/900 (35%)]\tLoss: 0.000105\n",
      "Training epoch: 247 [480/900 (53%)]\tLoss: 0.000264\n",
      "Training epoch: 247 [640/900 (70%)]\tLoss: 0.000478\n",
      "Training epoch: 247 [800/900 (88%)]\tLoss: 0.000129\n",
      "=========> Epoch: 247 Average loss: 0.0003\n",
      "Correlation coefficient: 0.6608\n",
      "Training epoch: 248 [0/900 (0%)]\tLoss: 0.000171\n",
      "Training epoch: 248 [160/900 (18%)]\tLoss: 0.000195\n",
      "Training epoch: 248 [320/900 (35%)]\tLoss: 0.000321\n",
      "Training epoch: 248 [480/900 (53%)]\tLoss: 0.000155\n",
      "Training epoch: 248 [640/900 (70%)]\tLoss: 0.000094\n",
      "Training epoch: 248 [800/900 (88%)]\tLoss: 0.000064\n",
      "=========> Epoch: 248 Average loss: 0.0002\n",
      "Correlation coefficient: 0.6622\n",
      "Training epoch: 249 [0/900 (0%)]\tLoss: 0.000052\n",
      "Training epoch: 249 [160/900 (18%)]\tLoss: 0.000186\n",
      "Training epoch: 249 [320/900 (35%)]\tLoss: 0.000051\n",
      "Training epoch: 249 [480/900 (53%)]\tLoss: 0.000388\n",
      "Training epoch: 249 [640/900 (70%)]\tLoss: 0.000127\n",
      "Training epoch: 249 [800/900 (88%)]\tLoss: 0.000138\n",
      "=========> Epoch: 249 Average loss: 0.0002\n",
      "Correlation coefficient: 0.6597\n",
      "Training epoch: 250 [0/900 (0%)]\tLoss: 0.000032\n",
      "Training epoch: 250 [160/900 (18%)]\tLoss: 0.000148\n",
      "Training epoch: 250 [320/900 (35%)]\tLoss: 0.000092\n",
      "Training epoch: 250 [480/900 (53%)]\tLoss: 0.000060\n",
      "Training epoch: 250 [640/900 (70%)]\tLoss: 0.000150\n",
      "Training epoch: 250 [800/900 (88%)]\tLoss: 0.000139\n",
      "=========> Epoch: 250 Average loss: 0.0001\n",
      "Correlation coefficient: 0.6612\n",
      "⏹️  Epoch 250 early stopping (no improvement for 50 epochs)\n",
      "🏁 Fold 9 best correlation: 0.6728\n",
      "\n",
      "========== Cross-validation Fold 10/10 ==========\n",
      "🔄 Fold 10: Using random initialization\n",
      "Training epoch: 1 [0/900 (0%)]\tLoss: 0.482575\n",
      "Training epoch: 1 [160/900 (18%)]\tLoss: 1.496520\n",
      "Training epoch: 1 [320/900 (35%)]\tLoss: 0.484488\n",
      "Training epoch: 1 [480/900 (53%)]\tLoss: 0.865336\n",
      "Training epoch: 1 [640/900 (70%)]\tLoss: 0.386662\n",
      "Training epoch: 1 [800/900 (88%)]\tLoss: 0.528973\n",
      "=========> Epoch: 1 Average loss: 0.8017\n",
      "Correlation coefficient: 0.6505\n",
      "✅ Epoch 1: New best correlation = 0.6505\n",
      "Training epoch: 2 [0/900 (0%)]\tLoss: 0.216717\n",
      "Training epoch: 2 [160/900 (18%)]\tLoss: 0.853709\n",
      "Training epoch: 2 [320/900 (35%)]\tLoss: 0.388708\n",
      "Training epoch: 2 [480/900 (53%)]\tLoss: 0.287546\n",
      "Training epoch: 2 [640/900 (70%)]\tLoss: 0.531053\n",
      "Training epoch: 2 [800/900 (88%)]\tLoss: 0.272298\n",
      "=========> Epoch: 2 Average loss: 0.3799\n",
      "Correlation coefficient: 0.6110\n",
      "Training epoch: 3 [0/900 (0%)]\tLoss: 0.149302\n",
      "Training epoch: 3 [160/900 (18%)]\tLoss: 0.115336\n",
      "Training epoch: 3 [320/900 (35%)]\tLoss: 0.079082\n",
      "Training epoch: 3 [480/900 (53%)]\tLoss: 0.130220\n",
      "Training epoch: 3 [640/900 (70%)]\tLoss: 0.097118\n",
      "Training epoch: 3 [800/900 (88%)]\tLoss: 0.082357\n",
      "=========> Epoch: 3 Average loss: 0.1485\n",
      "Correlation coefficient: 0.6272\n",
      "Training epoch: 4 [0/900 (0%)]\tLoss: 0.066939\n",
      "Training epoch: 4 [160/900 (18%)]\tLoss: 0.034449\n",
      "Training epoch: 4 [320/900 (35%)]\tLoss: 0.049861\n",
      "Training epoch: 4 [480/900 (53%)]\tLoss: 0.106241\n",
      "Training epoch: 4 [640/900 (70%)]\tLoss: 0.034094\n",
      "Training epoch: 4 [800/900 (88%)]\tLoss: 0.026109\n",
      "=========> Epoch: 4 Average loss: 0.0616\n",
      "Correlation coefficient: 0.6297\n",
      "Training epoch: 5 [0/900 (0%)]\tLoss: 0.044257\n",
      "Training epoch: 5 [160/900 (18%)]\tLoss: 0.031667\n",
      "Training epoch: 5 [320/900 (35%)]\tLoss: 0.018697\n",
      "Training epoch: 5 [480/900 (53%)]\tLoss: 0.032337\n",
      "Training epoch: 5 [640/900 (70%)]\tLoss: 0.037364\n",
      "Training epoch: 5 [800/900 (88%)]\tLoss: 0.013439\n",
      "=========> Epoch: 5 Average loss: 0.0332\n",
      "Correlation coefficient: 0.6213\n",
      "Training epoch: 6 [0/900 (0%)]\tLoss: 0.017144\n",
      "Training epoch: 6 [160/900 (18%)]\tLoss: 0.046188\n",
      "Training epoch: 6 [320/900 (35%)]\tLoss: 0.019575\n",
      "Training epoch: 6 [480/900 (53%)]\tLoss: 0.038455\n",
      "Training epoch: 6 [640/900 (70%)]\tLoss: 0.011313\n",
      "Training epoch: 6 [800/900 (88%)]\tLoss: 0.033573\n",
      "=========> Epoch: 6 Average loss: 0.0224\n",
      "Correlation coefficient: 0.6427\n",
      "Training epoch: 7 [0/900 (0%)]\tLoss: 0.003049\n",
      "Training epoch: 7 [160/900 (18%)]\tLoss: 0.073306\n",
      "Training epoch: 7 [320/900 (35%)]\tLoss: 0.005019\n",
      "Training epoch: 7 [480/900 (53%)]\tLoss: 0.018259\n",
      "Training epoch: 7 [640/900 (70%)]\tLoss: 0.009339\n",
      "Training epoch: 7 [800/900 (88%)]\tLoss: 0.005135\n",
      "=========> Epoch: 7 Average loss: 0.0141\n",
      "Correlation coefficient: 0.6417\n",
      "Training epoch: 8 [0/900 (0%)]\tLoss: 0.001762\n",
      "Training epoch: 8 [160/900 (18%)]\tLoss: 0.007765\n",
      "Training epoch: 8 [320/900 (35%)]\tLoss: 0.003477\n",
      "Training epoch: 8 [480/900 (53%)]\tLoss: 0.007493\n",
      "Training epoch: 8 [640/900 (70%)]\tLoss: 0.001755\n",
      "Training epoch: 8 [800/900 (88%)]\tLoss: 0.017368\n",
      "=========> Epoch: 8 Average loss: 0.0074\n",
      "Correlation coefficient: 0.6426\n",
      "Training epoch: 9 [0/900 (0%)]\tLoss: 0.002742\n",
      "Training epoch: 9 [160/900 (18%)]\tLoss: 0.004698\n",
      "Training epoch: 9 [320/900 (35%)]\tLoss: 0.010227\n",
      "Training epoch: 9 [480/900 (53%)]\tLoss: 0.007877\n",
      "Training epoch: 9 [640/900 (70%)]\tLoss: 0.002262\n",
      "Training epoch: 9 [800/900 (88%)]\tLoss: 0.002441\n",
      "=========> Epoch: 9 Average loss: 0.0055\n",
      "Correlation coefficient: 0.6432\n",
      "Training epoch: 10 [0/900 (0%)]\tLoss: 0.002446\n",
      "Training epoch: 10 [160/900 (18%)]\tLoss: 0.003758\n",
      "Training epoch: 10 [320/900 (35%)]\tLoss: 0.003268\n",
      "Training epoch: 10 [480/900 (53%)]\tLoss: 0.006320\n",
      "Training epoch: 10 [640/900 (70%)]\tLoss: 0.003817\n",
      "Training epoch: 10 [800/900 (88%)]\tLoss: 0.005001\n",
      "=========> Epoch: 10 Average loss: 0.0049\n",
      "Correlation coefficient: 0.6358\n",
      "Training epoch: 11 [0/900 (0%)]\tLoss: 0.001714\n",
      "Training epoch: 11 [160/900 (18%)]\tLoss: 0.001562\n",
      "Training epoch: 11 [320/900 (35%)]\tLoss: 0.007740\n",
      "Training epoch: 11 [480/900 (53%)]\tLoss: 0.005661\n",
      "Training epoch: 11 [640/900 (70%)]\tLoss: 0.003482\n",
      "Training epoch: 11 [800/900 (88%)]\tLoss: 0.005552\n",
      "=========> Epoch: 11 Average loss: 0.0049\n",
      "Correlation coefficient: 0.6502\n",
      "Training epoch: 12 [0/900 (0%)]\tLoss: 0.004728\n",
      "Training epoch: 12 [160/900 (18%)]\tLoss: 0.010038\n",
      "Training epoch: 12 [320/900 (35%)]\tLoss: 0.003424\n",
      "Training epoch: 12 [480/900 (53%)]\tLoss: 0.005023\n",
      "Training epoch: 12 [640/900 (70%)]\tLoss: 0.003652\n",
      "Training epoch: 12 [800/900 (88%)]\tLoss: 0.004945\n",
      "=========> Epoch: 12 Average loss: 0.0054\n",
      "Correlation coefficient: 0.6348\n",
      "Training epoch: 13 [0/900 (0%)]\tLoss: 0.002754\n",
      "Training epoch: 13 [160/900 (18%)]\tLoss: 0.005210\n",
      "Training epoch: 13 [320/900 (35%)]\tLoss: 0.006920\n",
      "Training epoch: 13 [480/900 (53%)]\tLoss: 0.004077\n",
      "Training epoch: 13 [640/900 (70%)]\tLoss: 0.010296\n",
      "Training epoch: 13 [800/900 (88%)]\tLoss: 0.003062\n",
      "=========> Epoch: 13 Average loss: 0.0077\n",
      "Correlation coefficient: 0.6514\n",
      "✅ Epoch 13: New best correlation = 0.6514\n",
      "Training epoch: 14 [0/900 (0%)]\tLoss: 0.009612\n",
      "Training epoch: 14 [160/900 (18%)]\tLoss: 0.003580\n",
      "Training epoch: 14 [320/900 (35%)]\tLoss: 0.017118\n",
      "Training epoch: 14 [480/900 (53%)]\tLoss: 0.004857\n",
      "Training epoch: 14 [640/900 (70%)]\tLoss: 0.003764\n",
      "Training epoch: 14 [800/900 (88%)]\tLoss: 0.013833\n",
      "=========> Epoch: 14 Average loss: 0.0087\n",
      "Correlation coefficient: 0.6396\n",
      "Training epoch: 15 [0/900 (0%)]\tLoss: 0.008819\n",
      "Training epoch: 15 [160/900 (18%)]\tLoss: 0.005649\n",
      "Training epoch: 15 [320/900 (35%)]\tLoss: 0.028216\n",
      "Training epoch: 15 [480/900 (53%)]\tLoss: 0.028728\n",
      "Training epoch: 15 [640/900 (70%)]\tLoss: 0.010657\n",
      "Training epoch: 15 [800/900 (88%)]\tLoss: 0.015287\n",
      "=========> Epoch: 15 Average loss: 0.0138\n",
      "Correlation coefficient: 0.6529\n",
      "✅ Epoch 15: New best correlation = 0.6529\n",
      "Training epoch: 16 [0/900 (0%)]\tLoss: 0.016086\n",
      "Training epoch: 16 [160/900 (18%)]\tLoss: 0.012083\n",
      "Training epoch: 16 [320/900 (35%)]\tLoss: 0.014608\n",
      "Training epoch: 16 [480/900 (53%)]\tLoss: 0.020337\n",
      "Training epoch: 16 [640/900 (70%)]\tLoss: 0.021484\n",
      "Training epoch: 16 [800/900 (88%)]\tLoss: 0.008079\n",
      "=========> Epoch: 16 Average loss: 0.0226\n",
      "Correlation coefficient: 0.6204\n",
      "Training epoch: 17 [0/900 (0%)]\tLoss: 0.029188\n",
      "Training epoch: 17 [160/900 (18%)]\tLoss: 0.038659\n",
      "Training epoch: 17 [320/900 (35%)]\tLoss: 0.023080\n",
      "Training epoch: 17 [480/900 (53%)]\tLoss: 0.034239\n",
      "Training epoch: 17 [640/900 (70%)]\tLoss: 0.012479\n",
      "Training epoch: 17 [800/900 (88%)]\tLoss: 0.025541\n",
      "=========> Epoch: 17 Average loss: 0.0255\n",
      "Correlation coefficient: 0.6572\n",
      "✅ Epoch 17: New best correlation = 0.6572\n",
      "Training epoch: 18 [0/900 (0%)]\tLoss: 0.025033\n",
      "Training epoch: 18 [160/900 (18%)]\tLoss: 0.010974\n",
      "Training epoch: 18 [320/900 (35%)]\tLoss: 0.033311\n",
      "Training epoch: 18 [480/900 (53%)]\tLoss: 0.019074\n",
      "Training epoch: 18 [640/900 (70%)]\tLoss: 0.007071\n",
      "Training epoch: 18 [800/900 (88%)]\tLoss: 0.021282\n",
      "=========> Epoch: 18 Average loss: 0.0172\n",
      "Correlation coefficient: 0.6321\n",
      "Training epoch: 19 [0/900 (0%)]\tLoss: 0.018973\n",
      "Training epoch: 19 [160/900 (18%)]\tLoss: 0.005597\n",
      "Training epoch: 19 [320/900 (35%)]\tLoss: 0.005887\n",
      "Training epoch: 19 [480/900 (53%)]\tLoss: 0.014774\n",
      "Training epoch: 19 [640/900 (70%)]\tLoss: 0.005738\n",
      "Training epoch: 19 [800/900 (88%)]\tLoss: 0.007454\n",
      "=========> Epoch: 19 Average loss: 0.0137\n",
      "Correlation coefficient: 0.6400\n",
      "Training epoch: 20 [0/900 (0%)]\tLoss: 0.007549\n",
      "Training epoch: 20 [160/900 (18%)]\tLoss: 0.003838\n",
      "Training epoch: 20 [320/900 (35%)]\tLoss: 0.020133\n",
      "Training epoch: 20 [480/900 (53%)]\tLoss: 0.014388\n",
      "Training epoch: 20 [640/900 (70%)]\tLoss: 0.012963\n",
      "Training epoch: 20 [800/900 (88%)]\tLoss: 0.017307\n",
      "=========> Epoch: 20 Average loss: 0.0105\n",
      "Correlation coefficient: 0.6335\n",
      "Training epoch: 21 [0/900 (0%)]\tLoss: 0.011345\n",
      "Training epoch: 21 [160/900 (18%)]\tLoss: 0.005457\n",
      "Training epoch: 21 [320/900 (35%)]\tLoss: 0.028537\n",
      "Training epoch: 21 [480/900 (53%)]\tLoss: 0.003618\n",
      "Training epoch: 21 [640/900 (70%)]\tLoss: 0.005264\n",
      "Training epoch: 21 [800/900 (88%)]\tLoss: 0.004787\n",
      "=========> Epoch: 21 Average loss: 0.0083\n",
      "Correlation coefficient: 0.6442\n",
      "Training epoch: 22 [0/900 (0%)]\tLoss: 0.005510\n",
      "Training epoch: 22 [160/900 (18%)]\tLoss: 0.008335\n",
      "Training epoch: 22 [320/900 (35%)]\tLoss: 0.020587\n",
      "Training epoch: 22 [480/900 (53%)]\tLoss: 0.005889\n",
      "Training epoch: 22 [640/900 (70%)]\tLoss: 0.008619\n",
      "Training epoch: 22 [800/900 (88%)]\tLoss: 0.002236\n",
      "=========> Epoch: 22 Average loss: 0.0089\n",
      "Correlation coefficient: 0.6376\n",
      "Training epoch: 23 [0/900 (0%)]\tLoss: 0.002852\n",
      "Training epoch: 23 [160/900 (18%)]\tLoss: 0.003926\n",
      "Training epoch: 23 [320/900 (35%)]\tLoss: 0.003825\n",
      "Training epoch: 23 [480/900 (53%)]\tLoss: 0.007334\n",
      "Training epoch: 23 [640/900 (70%)]\tLoss: 0.005965\n",
      "Training epoch: 23 [800/900 (88%)]\tLoss: 0.006435\n",
      "=========> Epoch: 23 Average loss: 0.0094\n",
      "Correlation coefficient: 0.6476\n",
      "Training epoch: 24 [0/900 (0%)]\tLoss: 0.002794\n",
      "Training epoch: 24 [160/900 (18%)]\tLoss: 0.005548\n",
      "Training epoch: 24 [320/900 (35%)]\tLoss: 0.007654\n",
      "Training epoch: 24 [480/900 (53%)]\tLoss: 0.010332\n",
      "Training epoch: 24 [640/900 (70%)]\tLoss: 0.006724\n",
      "Training epoch: 24 [800/900 (88%)]\tLoss: 0.039479\n",
      "=========> Epoch: 24 Average loss: 0.0100\n",
      "Correlation coefficient: 0.6467\n",
      "Training epoch: 25 [0/900 (0%)]\tLoss: 0.008435\n",
      "Training epoch: 25 [160/900 (18%)]\tLoss: 0.014714\n",
      "Training epoch: 25 [320/900 (35%)]\tLoss: 0.009588\n",
      "Training epoch: 25 [480/900 (53%)]\tLoss: 0.005280\n",
      "Training epoch: 25 [640/900 (70%)]\tLoss: 0.002084\n",
      "Training epoch: 25 [800/900 (88%)]\tLoss: 0.019669\n",
      "=========> Epoch: 25 Average loss: 0.0099\n",
      "Correlation coefficient: 0.6523\n",
      "Training epoch: 26 [0/900 (0%)]\tLoss: 0.008700\n",
      "Training epoch: 26 [160/900 (18%)]\tLoss: 0.005383\n",
      "Training epoch: 26 [320/900 (35%)]\tLoss: 0.003955\n",
      "Training epoch: 26 [480/900 (53%)]\tLoss: 0.009069\n",
      "Training epoch: 26 [640/900 (70%)]\tLoss: 0.003073\n",
      "Training epoch: 26 [800/900 (88%)]\tLoss: 0.007366\n",
      "=========> Epoch: 26 Average loss: 0.0086\n",
      "Correlation coefficient: 0.6517\n",
      "Training epoch: 27 [0/900 (0%)]\tLoss: 0.006665\n",
      "Training epoch: 27 [160/900 (18%)]\tLoss: 0.005044\n",
      "Training epoch: 27 [320/900 (35%)]\tLoss: 0.018885\n",
      "Training epoch: 27 [480/900 (53%)]\tLoss: 0.003660\n",
      "Training epoch: 27 [640/900 (70%)]\tLoss: 0.007341\n",
      "Training epoch: 27 [800/900 (88%)]\tLoss: 0.012637\n",
      "=========> Epoch: 27 Average loss: 0.0076\n",
      "Correlation coefficient: 0.6588\n",
      "✅ Epoch 27: New best correlation = 0.6588\n",
      "Training epoch: 28 [0/900 (0%)]\tLoss: 0.007501\n",
      "Training epoch: 28 [160/900 (18%)]\tLoss: 0.005071\n",
      "Training epoch: 28 [320/900 (35%)]\tLoss: 0.002847\n",
      "Training epoch: 28 [480/900 (53%)]\tLoss: 0.008080\n",
      "Training epoch: 28 [640/900 (70%)]\tLoss: 0.009499\n",
      "Training epoch: 28 [800/900 (88%)]\tLoss: 0.016854\n",
      "=========> Epoch: 28 Average loss: 0.0091\n",
      "Correlation coefficient: 0.6479\n",
      "Training epoch: 29 [0/900 (0%)]\tLoss: 0.003244\n",
      "Training epoch: 29 [160/900 (18%)]\tLoss: 0.012786\n",
      "Training epoch: 29 [320/900 (35%)]\tLoss: 0.002833\n",
      "Training epoch: 29 [480/900 (53%)]\tLoss: 0.003531\n",
      "Training epoch: 29 [640/900 (70%)]\tLoss: 0.005880\n",
      "Training epoch: 29 [800/900 (88%)]\tLoss: 0.003644\n",
      "=========> Epoch: 29 Average loss: 0.0074\n",
      "Correlation coefficient: 0.6485\n",
      "Training epoch: 30 [0/900 (0%)]\tLoss: 0.003818\n",
      "Training epoch: 30 [160/900 (18%)]\tLoss: 0.004486\n",
      "Training epoch: 30 [320/900 (35%)]\tLoss: 0.004505\n",
      "Training epoch: 30 [480/900 (53%)]\tLoss: 0.004093\n",
      "Training epoch: 30 [640/900 (70%)]\tLoss: 0.013207\n",
      "Training epoch: 30 [800/900 (88%)]\tLoss: 0.012382\n",
      "=========> Epoch: 30 Average loss: 0.0066\n",
      "Correlation coefficient: 0.6495\n",
      "Training epoch: 31 [0/900 (0%)]\tLoss: 0.024945\n",
      "Training epoch: 31 [160/900 (18%)]\tLoss: 0.009808\n",
      "Training epoch: 31 [320/900 (35%)]\tLoss: 0.001304\n",
      "Training epoch: 31 [480/900 (53%)]\tLoss: 0.009571\n",
      "Training epoch: 31 [640/900 (70%)]\tLoss: 0.010748\n",
      "Training epoch: 31 [800/900 (88%)]\tLoss: 0.003427\n",
      "=========> Epoch: 31 Average loss: 0.0063\n",
      "Correlation coefficient: 0.6517\n",
      "Training epoch: 32 [0/900 (0%)]\tLoss: 0.008858\n",
      "Training epoch: 32 [160/900 (18%)]\tLoss: 0.001051\n",
      "Training epoch: 32 [320/900 (35%)]\tLoss: 0.007848\n",
      "Training epoch: 32 [480/900 (53%)]\tLoss: 0.007414\n",
      "Training epoch: 32 [640/900 (70%)]\tLoss: 0.006644\n",
      "Training epoch: 32 [800/900 (88%)]\tLoss: 0.007699\n",
      "=========> Epoch: 32 Average loss: 0.0064\n",
      "Correlation coefficient: 0.6498\n",
      "Training epoch: 33 [0/900 (0%)]\tLoss: 0.004461\n",
      "Training epoch: 33 [160/900 (18%)]\tLoss: 0.066571\n",
      "Training epoch: 33 [320/900 (35%)]\tLoss: 0.019336\n",
      "Training epoch: 33 [480/900 (53%)]\tLoss: 0.010406\n",
      "Training epoch: 33 [640/900 (70%)]\tLoss: 0.003632\n",
      "Training epoch: 33 [800/900 (88%)]\tLoss: 0.004453\n",
      "=========> Epoch: 33 Average loss: 0.0105\n",
      "Correlation coefficient: 0.6378\n",
      "Training epoch: 34 [0/900 (0%)]\tLoss: 0.022701\n",
      "Training epoch: 34 [160/900 (18%)]\tLoss: 0.020392\n",
      "Training epoch: 34 [320/900 (35%)]\tLoss: 0.017477\n",
      "Training epoch: 34 [480/900 (53%)]\tLoss: 0.009411\n",
      "Training epoch: 34 [640/900 (70%)]\tLoss: 0.016689\n",
      "Training epoch: 34 [800/900 (88%)]\tLoss: 0.007235\n",
      "=========> Epoch: 34 Average loss: 0.0156\n",
      "Correlation coefficient: 0.6502\n",
      "Training epoch: 35 [0/900 (0%)]\tLoss: 0.010734\n",
      "Training epoch: 35 [160/900 (18%)]\tLoss: 0.039860\n",
      "Training epoch: 35 [320/900 (35%)]\tLoss: 0.021557\n",
      "Training epoch: 35 [480/900 (53%)]\tLoss: 0.014509\n",
      "Training epoch: 35 [640/900 (70%)]\tLoss: 0.014315\n",
      "Training epoch: 35 [800/900 (88%)]\tLoss: 0.021431\n",
      "=========> Epoch: 35 Average loss: 0.0186\n",
      "Correlation coefficient: 0.6322\n",
      "Training epoch: 36 [0/900 (0%)]\tLoss: 0.005342\n",
      "Training epoch: 36 [160/900 (18%)]\tLoss: 0.017220\n",
      "Training epoch: 36 [320/900 (35%)]\tLoss: 0.016476\n",
      "Training epoch: 36 [480/900 (53%)]\tLoss: 0.021650\n",
      "Training epoch: 36 [640/900 (70%)]\tLoss: 0.009931\n",
      "Training epoch: 36 [800/900 (88%)]\tLoss: 0.008216\n",
      "=========> Epoch: 36 Average loss: 0.0179\n",
      "Correlation coefficient: 0.6419\n",
      "Training epoch: 37 [0/900 (0%)]\tLoss: 0.010537\n",
      "Training epoch: 37 [160/900 (18%)]\tLoss: 0.008987\n",
      "Training epoch: 37 [320/900 (35%)]\tLoss: 0.015720\n",
      "Training epoch: 37 [480/900 (53%)]\tLoss: 0.013406\n",
      "Training epoch: 37 [640/900 (70%)]\tLoss: 0.031815\n",
      "Training epoch: 37 [800/900 (88%)]\tLoss: 0.023509\n",
      "=========> Epoch: 37 Average loss: 0.0150\n",
      "Correlation coefficient: 0.6455\n",
      "Training epoch: 38 [0/900 (0%)]\tLoss: 0.006998\n",
      "Training epoch: 38 [160/900 (18%)]\tLoss: 0.016966\n",
      "Training epoch: 38 [320/900 (35%)]\tLoss: 0.035621\n",
      "Training epoch: 38 [480/900 (53%)]\tLoss: 0.006970\n",
      "Training epoch: 38 [640/900 (70%)]\tLoss: 0.006097\n",
      "Training epoch: 38 [800/900 (88%)]\tLoss: 0.028515\n",
      "=========> Epoch: 38 Average loss: 0.0176\n",
      "Correlation coefficient: 0.6493\n",
      "Training epoch: 39 [0/900 (0%)]\tLoss: 0.012267\n",
      "Training epoch: 39 [160/900 (18%)]\tLoss: 0.022590\n",
      "Training epoch: 39 [320/900 (35%)]\tLoss: 0.007874\n",
      "Training epoch: 39 [480/900 (53%)]\tLoss: 0.019140\n",
      "Training epoch: 39 [640/900 (70%)]\tLoss: 0.004879\n",
      "Training epoch: 39 [800/900 (88%)]\tLoss: 0.003906\n",
      "=========> Epoch: 39 Average loss: 0.0181\n",
      "Correlation coefficient: 0.6472\n",
      "Training epoch: 40 [0/900 (0%)]\tLoss: 0.031502\n",
      "Training epoch: 40 [160/900 (18%)]\tLoss: 0.015557\n",
      "Training epoch: 40 [320/900 (35%)]\tLoss: 0.009935\n",
      "Training epoch: 40 [480/900 (53%)]\tLoss: 0.004123\n",
      "Training epoch: 40 [640/900 (70%)]\tLoss: 0.013779\n",
      "Training epoch: 40 [800/900 (88%)]\tLoss: 0.011152\n",
      "=========> Epoch: 40 Average loss: 0.0115\n",
      "Correlation coefficient: 0.6422\n",
      "Training epoch: 41 [0/900 (0%)]\tLoss: 0.004973\n",
      "Training epoch: 41 [160/900 (18%)]\tLoss: 0.008223\n",
      "Training epoch: 41 [320/900 (35%)]\tLoss: 0.003545\n",
      "Training epoch: 41 [480/900 (53%)]\tLoss: 0.004401\n",
      "Training epoch: 41 [640/900 (70%)]\tLoss: 0.007884\n",
      "Training epoch: 41 [800/900 (88%)]\tLoss: 0.004196\n",
      "=========> Epoch: 41 Average loss: 0.0080\n",
      "Correlation coefficient: 0.6544\n",
      "Training epoch: 42 [0/900 (0%)]\tLoss: 0.004308\n",
      "Training epoch: 42 [160/900 (18%)]\tLoss: 0.002401\n",
      "Training epoch: 42 [320/900 (35%)]\tLoss: 0.002629\n",
      "Training epoch: 42 [480/900 (53%)]\tLoss: 0.003422\n",
      "Training epoch: 42 [640/900 (70%)]\tLoss: 0.007701\n",
      "Training epoch: 42 [800/900 (88%)]\tLoss: 0.002326\n",
      "=========> Epoch: 42 Average loss: 0.0061\n",
      "Correlation coefficient: 0.6501\n",
      "Training epoch: 43 [0/900 (0%)]\tLoss: 0.004621\n",
      "Training epoch: 43 [160/900 (18%)]\tLoss: 0.009674\n",
      "Training epoch: 43 [320/900 (35%)]\tLoss: 0.017323\n",
      "Training epoch: 43 [480/900 (53%)]\tLoss: 0.008647\n",
      "Training epoch: 43 [640/900 (70%)]\tLoss: 0.002408\n",
      "Training epoch: 43 [800/900 (88%)]\tLoss: 0.004488\n",
      "=========> Epoch: 43 Average loss: 0.0063\n",
      "Correlation coefficient: 0.6512\n",
      "Training epoch: 44 [0/900 (0%)]\tLoss: 0.005230\n",
      "Training epoch: 44 [160/900 (18%)]\tLoss: 0.013949\n",
      "Training epoch: 44 [320/900 (35%)]\tLoss: 0.005331\n",
      "Training epoch: 44 [480/900 (53%)]\tLoss: 0.009673\n",
      "Training epoch: 44 [640/900 (70%)]\tLoss: 0.006207\n",
      "Training epoch: 44 [800/900 (88%)]\tLoss: 0.005771\n",
      "=========> Epoch: 44 Average loss: 0.0084\n",
      "Correlation coefficient: 0.6538\n",
      "Training epoch: 45 [0/900 (0%)]\tLoss: 0.018665\n",
      "Training epoch: 45 [160/900 (18%)]\tLoss: 0.010719\n",
      "Training epoch: 45 [320/900 (35%)]\tLoss: 0.008235\n",
      "Training epoch: 45 [480/900 (53%)]\tLoss: 0.005364\n",
      "Training epoch: 45 [640/900 (70%)]\tLoss: 0.006017\n",
      "Training epoch: 45 [800/900 (88%)]\tLoss: 0.007551\n",
      "=========> Epoch: 45 Average loss: 0.0128\n",
      "Correlation coefficient: 0.6611\n",
      "✅ Epoch 45: New best correlation = 0.6611\n",
      "Training epoch: 46 [0/900 (0%)]\tLoss: 0.010276\n",
      "Training epoch: 46 [160/900 (18%)]\tLoss: 0.054987\n",
      "Training epoch: 46 [320/900 (35%)]\tLoss: 0.015859\n",
      "Training epoch: 46 [480/900 (53%)]\tLoss: 0.007152\n",
      "Training epoch: 46 [640/900 (70%)]\tLoss: 0.019809\n",
      "Training epoch: 46 [800/900 (88%)]\tLoss: 0.011753\n",
      "=========> Epoch: 46 Average loss: 0.0178\n",
      "Correlation coefficient: 0.6565\n",
      "Training epoch: 47 [0/900 (0%)]\tLoss: 0.028056\n",
      "Training epoch: 47 [160/900 (18%)]\tLoss: 0.015389\n",
      "Training epoch: 47 [320/900 (35%)]\tLoss: 0.002985\n",
      "Training epoch: 47 [480/900 (53%)]\tLoss: 0.010246\n",
      "Training epoch: 47 [640/900 (70%)]\tLoss: 0.015999\n",
      "Training epoch: 47 [800/900 (88%)]\tLoss: 0.008145\n",
      "=========> Epoch: 47 Average loss: 0.0231\n",
      "Correlation coefficient: 0.6490\n",
      "Training epoch: 48 [0/900 (0%)]\tLoss: 0.011783\n",
      "Training epoch: 48 [160/900 (18%)]\tLoss: 0.004243\n",
      "Training epoch: 48 [320/900 (35%)]\tLoss: 0.034102\n",
      "Training epoch: 48 [480/900 (53%)]\tLoss: 0.042326\n",
      "Training epoch: 48 [640/900 (70%)]\tLoss: 0.016234\n",
      "Training epoch: 48 [800/900 (88%)]\tLoss: 0.008509\n",
      "=========> Epoch: 48 Average loss: 0.0173\n",
      "Correlation coefficient: 0.6500\n",
      "Training epoch: 49 [0/900 (0%)]\tLoss: 0.016563\n",
      "Training epoch: 49 [160/900 (18%)]\tLoss: 0.008183\n",
      "Training epoch: 49 [320/900 (35%)]\tLoss: 0.009429\n",
      "Training epoch: 49 [480/900 (53%)]\tLoss: 0.013256\n",
      "Training epoch: 49 [640/900 (70%)]\tLoss: 0.006209\n",
      "Training epoch: 49 [800/900 (88%)]\tLoss: 0.006213\n",
      "=========> Epoch: 49 Average loss: 0.0146\n",
      "Correlation coefficient: 0.6571\n",
      "Training epoch: 50 [0/900 (0%)]\tLoss: 0.008449\n",
      "Training epoch: 50 [160/900 (18%)]\tLoss: 0.027965\n",
      "Training epoch: 50 [320/900 (35%)]\tLoss: 0.004885\n",
      "Training epoch: 50 [480/900 (53%)]\tLoss: 0.025819\n",
      "Training epoch: 50 [640/900 (70%)]\tLoss: 0.009822\n",
      "Training epoch: 50 [800/900 (88%)]\tLoss: 0.011522\n",
      "=========> Epoch: 50 Average loss: 0.0183\n",
      "Correlation coefficient: 0.6523\n",
      "Training epoch: 51 [0/900 (0%)]\tLoss: 0.037993\n",
      "Training epoch: 51 [160/900 (18%)]\tLoss: 0.009081\n",
      "Training epoch: 51 [320/900 (35%)]\tLoss: 0.015887\n",
      "Training epoch: 51 [480/900 (53%)]\tLoss: 0.003351\n",
      "Training epoch: 51 [640/900 (70%)]\tLoss: 0.032022\n",
      "Training epoch: 51 [800/900 (88%)]\tLoss: 0.015931\n",
      "=========> Epoch: 51 Average loss: 0.0179\n",
      "Correlation coefficient: 0.6502\n",
      "Training epoch: 52 [0/900 (0%)]\tLoss: 0.012582\n",
      "Training epoch: 52 [160/900 (18%)]\tLoss: 0.010807\n",
      "Training epoch: 52 [320/900 (35%)]\tLoss: 0.004188\n",
      "Training epoch: 52 [480/900 (53%)]\tLoss: 0.009155\n",
      "Training epoch: 52 [640/900 (70%)]\tLoss: 0.007508\n",
      "Training epoch: 52 [800/900 (88%)]\tLoss: 0.017927\n",
      "=========> Epoch: 52 Average loss: 0.0124\n",
      "Correlation coefficient: 0.6611\n",
      "✅ Epoch 52: New best correlation = 0.6611\n",
      "Training epoch: 53 [0/900 (0%)]\tLoss: 0.006607\n",
      "Training epoch: 53 [160/900 (18%)]\tLoss: 0.011466\n",
      "Training epoch: 53 [320/900 (35%)]\tLoss: 0.007778\n",
      "Training epoch: 53 [480/900 (53%)]\tLoss: 0.005548\n",
      "Training epoch: 53 [640/900 (70%)]\tLoss: 0.006034\n",
      "Training epoch: 53 [800/900 (88%)]\tLoss: 0.005192\n",
      "=========> Epoch: 53 Average loss: 0.0090\n",
      "Correlation coefficient: 0.6687\n",
      "✅ Epoch 53: New best correlation = 0.6687\n",
      "Training epoch: 54 [0/900 (0%)]\tLoss: 0.017140\n",
      "Training epoch: 54 [160/900 (18%)]\tLoss: 0.002377\n",
      "Training epoch: 54 [320/900 (35%)]\tLoss: 0.004459\n",
      "Training epoch: 54 [480/900 (53%)]\tLoss: 0.006178\n",
      "Training epoch: 54 [640/900 (70%)]\tLoss: 0.010906\n",
      "Training epoch: 54 [800/900 (88%)]\tLoss: 0.006123\n",
      "=========> Epoch: 54 Average loss: 0.0101\n",
      "Correlation coefficient: 0.6585\n",
      "Training epoch: 55 [0/900 (0%)]\tLoss: 0.005432\n",
      "Training epoch: 55 [160/900 (18%)]\tLoss: 0.006917\n",
      "Training epoch: 55 [320/900 (35%)]\tLoss: 0.007781\n",
      "Training epoch: 55 [480/900 (53%)]\tLoss: 0.008137\n",
      "Training epoch: 55 [640/900 (70%)]\tLoss: 0.013990\n",
      "Training epoch: 55 [800/900 (88%)]\tLoss: 0.003707\n",
      "=========> Epoch: 55 Average loss: 0.0108\n",
      "Correlation coefficient: 0.6641\n",
      "Training epoch: 56 [0/900 (0%)]\tLoss: 0.010730\n",
      "Training epoch: 56 [160/900 (18%)]\tLoss: 0.008616\n",
      "Training epoch: 56 [320/900 (35%)]\tLoss: 0.008210\n",
      "Training epoch: 56 [480/900 (53%)]\tLoss: 0.004525\n",
      "Training epoch: 56 [640/900 (70%)]\tLoss: 0.008024\n",
      "Training epoch: 56 [800/900 (88%)]\tLoss: 0.009115\n",
      "=========> Epoch: 56 Average loss: 0.0090\n",
      "Correlation coefficient: 0.6596\n",
      "Training epoch: 57 [0/900 (0%)]\tLoss: 0.005125\n",
      "Training epoch: 57 [160/900 (18%)]\tLoss: 0.003500\n",
      "Training epoch: 57 [320/900 (35%)]\tLoss: 0.002916\n",
      "Training epoch: 57 [480/900 (53%)]\tLoss: 0.004547\n",
      "Training epoch: 57 [640/900 (70%)]\tLoss: 0.007929\n",
      "Training epoch: 57 [800/900 (88%)]\tLoss: 0.001469\n",
      "=========> Epoch: 57 Average loss: 0.0048\n",
      "Correlation coefficient: 0.6644\n",
      "Training epoch: 58 [0/900 (0%)]\tLoss: 0.001127\n",
      "Training epoch: 58 [160/900 (18%)]\tLoss: 0.003376\n",
      "Training epoch: 58 [320/900 (35%)]\tLoss: 0.002698\n",
      "Training epoch: 58 [480/900 (53%)]\tLoss: 0.005100\n",
      "Training epoch: 58 [640/900 (70%)]\tLoss: 0.001834\n",
      "Training epoch: 58 [800/900 (88%)]\tLoss: 0.001493\n",
      "=========> Epoch: 58 Average loss: 0.0031\n",
      "Correlation coefficient: 0.6651\n",
      "Training epoch: 59 [0/900 (0%)]\tLoss: 0.000564\n",
      "Training epoch: 59 [160/900 (18%)]\tLoss: 0.002821\n",
      "Training epoch: 59 [320/900 (35%)]\tLoss: 0.000752\n",
      "Training epoch: 59 [480/900 (53%)]\tLoss: 0.001648\n",
      "Training epoch: 59 [640/900 (70%)]\tLoss: 0.001411\n",
      "Training epoch: 59 [800/900 (88%)]\tLoss: 0.001579\n",
      "=========> Epoch: 59 Average loss: 0.0020\n",
      "Correlation coefficient: 0.6622\n",
      "Training epoch: 60 [0/900 (0%)]\tLoss: 0.001729\n",
      "Training epoch: 60 [160/900 (18%)]\tLoss: 0.004954\n",
      "Training epoch: 60 [320/900 (35%)]\tLoss: 0.002551\n",
      "Training epoch: 60 [480/900 (53%)]\tLoss: 0.000493\n",
      "Training epoch: 60 [640/900 (70%)]\tLoss: 0.001138\n",
      "Training epoch: 60 [800/900 (88%)]\tLoss: 0.000741\n",
      "=========> Epoch: 60 Average loss: 0.0017\n",
      "Correlation coefficient: 0.6625\n",
      "Training epoch: 61 [0/900 (0%)]\tLoss: 0.001096\n",
      "Training epoch: 61 [160/900 (18%)]\tLoss: 0.001673\n",
      "Training epoch: 61 [320/900 (35%)]\tLoss: 0.002831\n",
      "Training epoch: 61 [480/900 (53%)]\tLoss: 0.003027\n",
      "Training epoch: 61 [640/900 (70%)]\tLoss: 0.001326\n",
      "Training epoch: 61 [800/900 (88%)]\tLoss: 0.000746\n",
      "=========> Epoch: 61 Average loss: 0.0018\n",
      "Correlation coefficient: 0.6630\n",
      "Training epoch: 62 [0/900 (0%)]\tLoss: 0.001079\n",
      "Training epoch: 62 [160/900 (18%)]\tLoss: 0.000736\n",
      "Training epoch: 62 [320/900 (35%)]\tLoss: 0.001364\n",
      "Training epoch: 62 [480/900 (53%)]\tLoss: 0.000854\n",
      "Training epoch: 62 [640/900 (70%)]\tLoss: 0.001025\n",
      "Training epoch: 62 [800/900 (88%)]\tLoss: 0.000400\n",
      "=========> Epoch: 62 Average loss: 0.0015\n",
      "Correlation coefficient: 0.6612\n",
      "Training epoch: 63 [0/900 (0%)]\tLoss: 0.001317\n",
      "Training epoch: 63 [160/900 (18%)]\tLoss: 0.000773\n",
      "Training epoch: 63 [320/900 (35%)]\tLoss: 0.002147\n",
      "Training epoch: 63 [480/900 (53%)]\tLoss: 0.001379\n",
      "Training epoch: 63 [640/900 (70%)]\tLoss: 0.003286\n",
      "Training epoch: 63 [800/900 (88%)]\tLoss: 0.001269\n",
      "=========> Epoch: 63 Average loss: 0.0016\n",
      "Correlation coefficient: 0.6666\n",
      "Training epoch: 64 [0/900 (0%)]\tLoss: 0.003037\n",
      "Training epoch: 64 [160/900 (18%)]\tLoss: 0.003151\n",
      "Training epoch: 64 [320/900 (35%)]\tLoss: 0.002826\n",
      "Training epoch: 64 [480/900 (53%)]\tLoss: 0.002752\n",
      "Training epoch: 64 [640/900 (70%)]\tLoss: 0.002927\n",
      "Training epoch: 64 [800/900 (88%)]\tLoss: 0.003941\n",
      "=========> Epoch: 64 Average loss: 0.0023\n",
      "Correlation coefficient: 0.6623\n",
      "Training epoch: 65 [0/900 (0%)]\tLoss: 0.004840\n",
      "Training epoch: 65 [160/900 (18%)]\tLoss: 0.003202\n",
      "Training epoch: 65 [320/900 (35%)]\tLoss: 0.003455\n",
      "Training epoch: 65 [480/900 (53%)]\tLoss: 0.002015\n",
      "Training epoch: 65 [640/900 (70%)]\tLoss: 0.001404\n",
      "Training epoch: 65 [800/900 (88%)]\tLoss: 0.001659\n",
      "=========> Epoch: 65 Average loss: 0.0032\n",
      "Correlation coefficient: 0.6638\n",
      "Training epoch: 66 [0/900 (0%)]\tLoss: 0.001470\n",
      "Training epoch: 66 [160/900 (18%)]\tLoss: 0.004327\n",
      "Training epoch: 66 [320/900 (35%)]\tLoss: 0.005701\n",
      "Training epoch: 66 [480/900 (53%)]\tLoss: 0.002461\n",
      "Training epoch: 66 [640/900 (70%)]\tLoss: 0.003770\n",
      "Training epoch: 66 [800/900 (88%)]\tLoss: 0.003100\n",
      "=========> Epoch: 66 Average loss: 0.0065\n",
      "Correlation coefficient: 0.6599\n",
      "Training epoch: 67 [0/900 (0%)]\tLoss: 0.001978\n",
      "Training epoch: 67 [160/900 (18%)]\tLoss: 0.007958\n",
      "Training epoch: 67 [320/900 (35%)]\tLoss: 0.013916\n",
      "Training epoch: 67 [480/900 (53%)]\tLoss: 0.002769\n",
      "Training epoch: 67 [640/900 (70%)]\tLoss: 0.006122\n",
      "Training epoch: 67 [800/900 (88%)]\tLoss: 0.005030\n",
      "=========> Epoch: 67 Average loss: 0.0094\n",
      "Correlation coefficient: 0.6604\n",
      "Training epoch: 68 [0/900 (0%)]\tLoss: 0.003885\n",
      "Training epoch: 68 [160/900 (18%)]\tLoss: 0.012497\n",
      "Training epoch: 68 [320/900 (35%)]\tLoss: 0.005057\n",
      "Training epoch: 68 [480/900 (53%)]\tLoss: 0.002593\n",
      "Training epoch: 68 [640/900 (70%)]\tLoss: 0.018260\n",
      "Training epoch: 68 [800/900 (88%)]\tLoss: 0.017073\n",
      "=========> Epoch: 68 Average loss: 0.0106\n",
      "Correlation coefficient: 0.6579\n",
      "Training epoch: 69 [0/900 (0%)]\tLoss: 0.013135\n",
      "Training epoch: 69 [160/900 (18%)]\tLoss: 0.004564\n",
      "Training epoch: 69 [320/900 (35%)]\tLoss: 0.005688\n",
      "Training epoch: 69 [480/900 (53%)]\tLoss: 0.005830\n",
      "Training epoch: 69 [640/900 (70%)]\tLoss: 0.006874\n",
      "Training epoch: 69 [800/900 (88%)]\tLoss: 0.011020\n",
      "=========> Epoch: 69 Average loss: 0.0120\n",
      "Correlation coefficient: 0.6591\n",
      "Training epoch: 70 [0/900 (0%)]\tLoss: 0.012682\n",
      "Training epoch: 70 [160/900 (18%)]\tLoss: 0.023257\n",
      "Training epoch: 70 [320/900 (35%)]\tLoss: 0.031367\n",
      "Training epoch: 70 [480/900 (53%)]\tLoss: 0.014570\n",
      "Training epoch: 70 [640/900 (70%)]\tLoss: 0.050876\n",
      "Training epoch: 70 [800/900 (88%)]\tLoss: 0.010540\n",
      "=========> Epoch: 70 Average loss: 0.0147\n",
      "Correlation coefficient: 0.6557\n",
      "Training epoch: 71 [0/900 (0%)]\tLoss: 0.004260\n",
      "Training epoch: 71 [160/900 (18%)]\tLoss: 0.024858\n",
      "Training epoch: 71 [320/900 (35%)]\tLoss: 0.010193\n",
      "Training epoch: 71 [480/900 (53%)]\tLoss: 0.031411\n",
      "Training epoch: 71 [640/900 (70%)]\tLoss: 0.010398\n",
      "Training epoch: 71 [800/900 (88%)]\tLoss: 0.005377\n",
      "=========> Epoch: 71 Average loss: 0.0161\n",
      "Correlation coefficient: 0.6606\n",
      "Training epoch: 72 [0/900 (0%)]\tLoss: 0.009691\n",
      "Training epoch: 72 [160/900 (18%)]\tLoss: 0.047816\n",
      "Training epoch: 72 [320/900 (35%)]\tLoss: 0.024319\n",
      "Training epoch: 72 [480/900 (53%)]\tLoss: 0.012633\n",
      "Training epoch: 72 [640/900 (70%)]\tLoss: 0.036118\n",
      "Training epoch: 72 [800/900 (88%)]\tLoss: 0.017087\n",
      "=========> Epoch: 72 Average loss: 0.0177\n",
      "Correlation coefficient: 0.6624\n",
      "Training epoch: 73 [0/900 (0%)]\tLoss: 0.009067\n",
      "Training epoch: 73 [160/900 (18%)]\tLoss: 0.008277\n",
      "Training epoch: 73 [320/900 (35%)]\tLoss: 0.011733\n",
      "Training epoch: 73 [480/900 (53%)]\tLoss: 0.011761\n",
      "Training epoch: 73 [640/900 (70%)]\tLoss: 0.010432\n",
      "Training epoch: 73 [800/900 (88%)]\tLoss: 0.013054\n",
      "=========> Epoch: 73 Average loss: 0.0154\n",
      "Correlation coefficient: 0.6646\n",
      "Training epoch: 74 [0/900 (0%)]\tLoss: 0.017434\n",
      "Training epoch: 74 [160/900 (18%)]\tLoss: 0.008394\n",
      "Training epoch: 74 [320/900 (35%)]\tLoss: 0.014802\n",
      "Training epoch: 74 [480/900 (53%)]\tLoss: 0.011346\n",
      "Training epoch: 74 [640/900 (70%)]\tLoss: 0.005575\n",
      "Training epoch: 74 [800/900 (88%)]\tLoss: 0.006686\n",
      "=========> Epoch: 74 Average loss: 0.0125\n",
      "Correlation coefficient: 0.6474\n",
      "Training epoch: 75 [0/900 (0%)]\tLoss: 0.023769\n",
      "Training epoch: 75 [160/900 (18%)]\tLoss: 0.051238\n",
      "Training epoch: 75 [320/900 (35%)]\tLoss: 0.007453\n",
      "Training epoch: 75 [480/900 (53%)]\tLoss: 0.012686\n",
      "Training epoch: 75 [640/900 (70%)]\tLoss: 0.011245\n",
      "Training epoch: 75 [800/900 (88%)]\tLoss: 0.020533\n",
      "=========> Epoch: 75 Average loss: 0.0154\n",
      "Correlation coefficient: 0.6616\n",
      "Training epoch: 76 [0/900 (0%)]\tLoss: 0.011294\n",
      "Training epoch: 76 [160/900 (18%)]\tLoss: 0.025523\n",
      "Training epoch: 76 [320/900 (35%)]\tLoss: 0.060356\n",
      "Training epoch: 76 [480/900 (53%)]\tLoss: 0.008430\n",
      "Training epoch: 76 [640/900 (70%)]\tLoss: 0.025906\n",
      "Training epoch: 76 [800/900 (88%)]\tLoss: 0.009473\n",
      "=========> Epoch: 76 Average loss: 0.0170\n",
      "Correlation coefficient: 0.6641\n",
      "Training epoch: 77 [0/900 (0%)]\tLoss: 0.009751\n",
      "Training epoch: 77 [160/900 (18%)]\tLoss: 0.009150\n",
      "Training epoch: 77 [320/900 (35%)]\tLoss: 0.015222\n",
      "Training epoch: 77 [480/900 (53%)]\tLoss: 0.006573\n",
      "Training epoch: 77 [640/900 (70%)]\tLoss: 0.015175\n",
      "Training epoch: 77 [800/900 (88%)]\tLoss: 0.017935\n",
      "=========> Epoch: 77 Average loss: 0.0109\n",
      "Correlation coefficient: 0.6588\n",
      "Training epoch: 78 [0/900 (0%)]\tLoss: 0.019024\n",
      "Training epoch: 78 [160/900 (18%)]\tLoss: 0.016349\n",
      "Training epoch: 78 [320/900 (35%)]\tLoss: 0.007954\n",
      "Training epoch: 78 [480/900 (53%)]\tLoss: 0.005496\n",
      "Training epoch: 78 [640/900 (70%)]\tLoss: 0.008633\n",
      "Training epoch: 78 [800/900 (88%)]\tLoss: 0.001708\n",
      "=========> Epoch: 78 Average loss: 0.0113\n",
      "Correlation coefficient: 0.6535\n",
      "Training epoch: 79 [0/900 (0%)]\tLoss: 0.007304\n",
      "Training epoch: 79 [160/900 (18%)]\tLoss: 0.008909\n",
      "Training epoch: 79 [320/900 (35%)]\tLoss: 0.013622\n",
      "Training epoch: 79 [480/900 (53%)]\tLoss: 0.010257\n",
      "Training epoch: 79 [640/900 (70%)]\tLoss: 0.007885\n",
      "Training epoch: 79 [800/900 (88%)]\tLoss: 0.002471\n",
      "=========> Epoch: 79 Average loss: 0.0086\n",
      "Correlation coefficient: 0.6702\n",
      "✅ Epoch 79: New best correlation = 0.6702\n",
      "Training epoch: 80 [0/900 (0%)]\tLoss: 0.002300\n",
      "Training epoch: 80 [160/900 (18%)]\tLoss: 0.005183\n",
      "Training epoch: 80 [320/900 (35%)]\tLoss: 0.002355\n",
      "Training epoch: 80 [480/900 (53%)]\tLoss: 0.008324\n",
      "Training epoch: 80 [640/900 (70%)]\tLoss: 0.005093\n",
      "Training epoch: 80 [800/900 (88%)]\tLoss: 0.005261\n",
      "=========> Epoch: 80 Average loss: 0.0044\n",
      "Correlation coefficient: 0.6636\n",
      "Training epoch: 81 [0/900 (0%)]\tLoss: 0.006077\n",
      "Training epoch: 81 [160/900 (18%)]\tLoss: 0.001962\n",
      "Training epoch: 81 [320/900 (35%)]\tLoss: 0.002807\n",
      "Training epoch: 81 [480/900 (53%)]\tLoss: 0.003582\n",
      "Training epoch: 81 [640/900 (70%)]\tLoss: 0.002295\n",
      "Training epoch: 81 [800/900 (88%)]\tLoss: 0.003113\n",
      "=========> Epoch: 81 Average loss: 0.0037\n",
      "Correlation coefficient: 0.6561\n",
      "Training epoch: 82 [0/900 (0%)]\tLoss: 0.002416\n",
      "Training epoch: 82 [160/900 (18%)]\tLoss: 0.005052\n",
      "Training epoch: 82 [320/900 (35%)]\tLoss: 0.004264\n",
      "Training epoch: 82 [480/900 (53%)]\tLoss: 0.002618\n",
      "Training epoch: 82 [640/900 (70%)]\tLoss: 0.001326\n",
      "Training epoch: 82 [800/900 (88%)]\tLoss: 0.001282\n",
      "=========> Epoch: 82 Average loss: 0.0031\n",
      "Correlation coefficient: 0.6626\n",
      "Training epoch: 83 [0/900 (0%)]\tLoss: 0.001156\n",
      "Training epoch: 83 [160/900 (18%)]\tLoss: 0.001551\n",
      "Training epoch: 83 [320/900 (35%)]\tLoss: 0.003662\n",
      "Training epoch: 83 [480/900 (53%)]\tLoss: 0.002136\n",
      "Training epoch: 83 [640/900 (70%)]\tLoss: 0.000558\n",
      "Training epoch: 83 [800/900 (88%)]\tLoss: 0.004553\n",
      "=========> Epoch: 83 Average loss: 0.0023\n",
      "Correlation coefficient: 0.6571\n",
      "Training epoch: 84 [0/900 (0%)]\tLoss: 0.001717\n",
      "Training epoch: 84 [160/900 (18%)]\tLoss: 0.004094\n",
      "Training epoch: 84 [320/900 (35%)]\tLoss: 0.001421\n",
      "Training epoch: 84 [480/900 (53%)]\tLoss: 0.004197\n",
      "Training epoch: 84 [640/900 (70%)]\tLoss: 0.000765\n",
      "Training epoch: 84 [800/900 (88%)]\tLoss: 0.000589\n",
      "=========> Epoch: 84 Average loss: 0.0019\n",
      "Correlation coefficient: 0.6610\n",
      "Training epoch: 85 [0/900 (0%)]\tLoss: 0.002129\n",
      "Training epoch: 85 [160/900 (18%)]\tLoss: 0.001094\n",
      "Training epoch: 85 [320/900 (35%)]\tLoss: 0.001366\n",
      "Training epoch: 85 [480/900 (53%)]\tLoss: 0.001969\n",
      "Training epoch: 85 [640/900 (70%)]\tLoss: 0.002692\n",
      "Training epoch: 85 [800/900 (88%)]\tLoss: 0.001122\n",
      "=========> Epoch: 85 Average loss: 0.0019\n",
      "Correlation coefficient: 0.6621\n",
      "Training epoch: 86 [0/900 (0%)]\tLoss: 0.002343\n",
      "Training epoch: 86 [160/900 (18%)]\tLoss: 0.002259\n",
      "Training epoch: 86 [320/900 (35%)]\tLoss: 0.001546\n",
      "Training epoch: 86 [480/900 (53%)]\tLoss: 0.001485\n",
      "Training epoch: 86 [640/900 (70%)]\tLoss: 0.000731\n",
      "Training epoch: 86 [800/900 (88%)]\tLoss: 0.002715\n",
      "=========> Epoch: 86 Average loss: 0.0021\n",
      "Correlation coefficient: 0.6648\n",
      "Training epoch: 87 [0/900 (0%)]\tLoss: 0.002871\n",
      "Training epoch: 87 [160/900 (18%)]\tLoss: 0.000967\n",
      "Training epoch: 87 [320/900 (35%)]\tLoss: 0.001733\n",
      "Training epoch: 87 [480/900 (53%)]\tLoss: 0.001866\n",
      "Training epoch: 87 [640/900 (70%)]\tLoss: 0.000558\n",
      "Training epoch: 87 [800/900 (88%)]\tLoss: 0.001969\n",
      "=========> Epoch: 87 Average loss: 0.0024\n",
      "Correlation coefficient: 0.6601\n",
      "Training epoch: 88 [0/900 (0%)]\tLoss: 0.000811\n",
      "Training epoch: 88 [160/900 (18%)]\tLoss: 0.001230\n",
      "Training epoch: 88 [320/900 (35%)]\tLoss: 0.019239\n",
      "Training epoch: 88 [480/900 (53%)]\tLoss: 0.004280\n",
      "Training epoch: 88 [640/900 (70%)]\tLoss: 0.000329\n",
      "Training epoch: 88 [800/900 (88%)]\tLoss: 0.001315\n",
      "=========> Epoch: 88 Average loss: 0.0024\n",
      "Correlation coefficient: 0.6642\n",
      "Training epoch: 89 [0/900 (0%)]\tLoss: 0.002722\n",
      "Training epoch: 89 [160/900 (18%)]\tLoss: 0.001827\n",
      "Training epoch: 89 [320/900 (35%)]\tLoss: 0.001892\n",
      "Training epoch: 89 [480/900 (53%)]\tLoss: 0.003530\n",
      "Training epoch: 89 [640/900 (70%)]\tLoss: 0.001349\n",
      "Training epoch: 89 [800/900 (88%)]\tLoss: 0.001166\n",
      "=========> Epoch: 89 Average loss: 0.0022\n",
      "Correlation coefficient: 0.6643\n",
      "Training epoch: 90 [0/900 (0%)]\tLoss: 0.001582\n",
      "Training epoch: 90 [160/900 (18%)]\tLoss: 0.004151\n",
      "Training epoch: 90 [320/900 (35%)]\tLoss: 0.001010\n",
      "Training epoch: 90 [480/900 (53%)]\tLoss: 0.001048\n",
      "Training epoch: 90 [640/900 (70%)]\tLoss: 0.001254\n",
      "Training epoch: 90 [800/900 (88%)]\tLoss: 0.008835\n",
      "=========> Epoch: 90 Average loss: 0.0029\n",
      "Correlation coefficient: 0.6671\n",
      "Training epoch: 91 [0/900 (0%)]\tLoss: 0.002888\n",
      "Training epoch: 91 [160/900 (18%)]\tLoss: 0.001706\n",
      "Training epoch: 91 [320/900 (35%)]\tLoss: 0.002780\n",
      "Training epoch: 91 [480/900 (53%)]\tLoss: 0.002303\n",
      "Training epoch: 91 [640/900 (70%)]\tLoss: 0.006678\n",
      "Training epoch: 91 [800/900 (88%)]\tLoss: 0.002428\n",
      "=========> Epoch: 91 Average loss: 0.0045\n",
      "Correlation coefficient: 0.6656\n",
      "Training epoch: 92 [0/900 (0%)]\tLoss: 0.003842\n",
      "Training epoch: 92 [160/900 (18%)]\tLoss: 0.010176\n",
      "Training epoch: 92 [320/900 (35%)]\tLoss: 0.005965\n",
      "Training epoch: 92 [480/900 (53%)]\tLoss: 0.003522\n",
      "Training epoch: 92 [640/900 (70%)]\tLoss: 0.005834\n",
      "Training epoch: 92 [800/900 (88%)]\tLoss: 0.003887\n",
      "=========> Epoch: 92 Average loss: 0.0090\n",
      "Correlation coefficient: 0.6708\n",
      "✅ Epoch 92: New best correlation = 0.6708\n",
      "Training epoch: 93 [0/900 (0%)]\tLoss: 0.005235\n",
      "Training epoch: 93 [160/900 (18%)]\tLoss: 0.009811\n",
      "Training epoch: 93 [320/900 (35%)]\tLoss: 0.016376\n",
      "Training epoch: 93 [480/900 (53%)]\tLoss: 0.025275\n",
      "Training epoch: 93 [640/900 (70%)]\tLoss: 0.021990\n",
      "Training epoch: 93 [800/900 (88%)]\tLoss: 0.009112\n",
      "=========> Epoch: 93 Average loss: 0.0360\n",
      "Correlation coefficient: 0.6370\n",
      "Training epoch: 94 [0/900 (0%)]\tLoss: 0.087683\n",
      "Training epoch: 94 [160/900 (18%)]\tLoss: 0.128061\n",
      "Training epoch: 94 [320/900 (35%)]\tLoss: 0.056896\n",
      "Training epoch: 94 [480/900 (53%)]\tLoss: 0.025021\n",
      "Training epoch: 94 [640/900 (70%)]\tLoss: 0.023659\n",
      "Training epoch: 94 [800/900 (88%)]\tLoss: 0.028381\n",
      "=========> Epoch: 94 Average loss: 0.0434\n",
      "Correlation coefficient: 0.6787\n",
      "✅ Epoch 94: New best correlation = 0.6787\n",
      "Training epoch: 95 [0/900 (0%)]\tLoss: 0.074004\n",
      "Training epoch: 95 [160/900 (18%)]\tLoss: 0.039058\n",
      "Training epoch: 95 [320/900 (35%)]\tLoss: 0.033870\n",
      "Training epoch: 95 [480/900 (53%)]\tLoss: 0.025398\n",
      "Training epoch: 95 [640/900 (70%)]\tLoss: 0.008767\n",
      "Training epoch: 95 [800/900 (88%)]\tLoss: 0.034743\n",
      "=========> Epoch: 95 Average loss: 0.0330\n",
      "Correlation coefficient: 0.6681\n",
      "Training epoch: 96 [0/900 (0%)]\tLoss: 0.012303\n",
      "Training epoch: 96 [160/900 (18%)]\tLoss: 0.047122\n",
      "Training epoch: 96 [320/900 (35%)]\tLoss: 0.063319\n",
      "Training epoch: 96 [480/900 (53%)]\tLoss: 0.031072\n",
      "Training epoch: 96 [640/900 (70%)]\tLoss: 0.016367\n",
      "Training epoch: 96 [800/900 (88%)]\tLoss: 0.020892\n",
      "=========> Epoch: 96 Average loss: 0.0295\n",
      "Correlation coefficient: 0.6586\n",
      "Training epoch: 97 [0/900 (0%)]\tLoss: 0.017924\n",
      "Training epoch: 97 [160/900 (18%)]\tLoss: 0.046807\n",
      "Training epoch: 97 [320/900 (35%)]\tLoss: 0.021794\n",
      "Training epoch: 97 [480/900 (53%)]\tLoss: 0.008912\n",
      "Training epoch: 97 [640/900 (70%)]\tLoss: 0.011555\n",
      "Training epoch: 97 [800/900 (88%)]\tLoss: 0.014139\n",
      "=========> Epoch: 97 Average loss: 0.0213\n",
      "Correlation coefficient: 0.6778\n",
      "Training epoch: 98 [0/900 (0%)]\tLoss: 0.011215\n",
      "Training epoch: 98 [160/900 (18%)]\tLoss: 0.022874\n",
      "Training epoch: 98 [320/900 (35%)]\tLoss: 0.009321\n",
      "Training epoch: 98 [480/900 (53%)]\tLoss: 0.004884\n",
      "Training epoch: 98 [640/900 (70%)]\tLoss: 0.008070\n",
      "Training epoch: 98 [800/900 (88%)]\tLoss: 0.012778\n",
      "=========> Epoch: 98 Average loss: 0.0125\n",
      "Correlation coefficient: 0.6672\n",
      "Training epoch: 99 [0/900 (0%)]\tLoss: 0.006360\n",
      "Training epoch: 99 [160/900 (18%)]\tLoss: 0.007959\n",
      "Training epoch: 99 [320/900 (35%)]\tLoss: 0.008021\n",
      "Training epoch: 99 [480/900 (53%)]\tLoss: 0.019870\n",
      "Training epoch: 99 [640/900 (70%)]\tLoss: 0.034826\n",
      "Training epoch: 99 [800/900 (88%)]\tLoss: 0.002596\n",
      "=========> Epoch: 99 Average loss: 0.0079\n",
      "Correlation coefficient: 0.6713\n",
      "Training epoch: 100 [0/900 (0%)]\tLoss: 0.005349\n",
      "Training epoch: 100 [160/900 (18%)]\tLoss: 0.003184\n",
      "Training epoch: 100 [320/900 (35%)]\tLoss: 0.008436\n",
      "Training epoch: 100 [480/900 (53%)]\tLoss: 0.001318\n",
      "Training epoch: 100 [640/900 (70%)]\tLoss: 0.001999\n",
      "Training epoch: 100 [800/900 (88%)]\tLoss: 0.001902\n",
      "=========> Epoch: 100 Average loss: 0.0037\n",
      "Correlation coefficient: 0.6760\n",
      "Training epoch: 101 [0/900 (0%)]\tLoss: 0.000724\n",
      "Training epoch: 101 [160/900 (18%)]\tLoss: 0.000649\n",
      "Training epoch: 101 [320/900 (35%)]\tLoss: 0.000881\n",
      "Training epoch: 101 [480/900 (53%)]\tLoss: 0.000969\n",
      "Training epoch: 101 [640/900 (70%)]\tLoss: 0.002586\n",
      "Training epoch: 101 [800/900 (88%)]\tLoss: 0.002133\n",
      "=========> Epoch: 101 Average loss: 0.0018\n",
      "Correlation coefficient: 0.6736\n",
      "Training epoch: 102 [0/900 (0%)]\tLoss: 0.001095\n",
      "Training epoch: 102 [160/900 (18%)]\tLoss: 0.001413\n",
      "Training epoch: 102 [320/900 (35%)]\tLoss: 0.004918\n",
      "Training epoch: 102 [480/900 (53%)]\tLoss: 0.000659\n",
      "Training epoch: 102 [640/900 (70%)]\tLoss: 0.000480\n",
      "Training epoch: 102 [800/900 (88%)]\tLoss: 0.001337\n",
      "=========> Epoch: 102 Average loss: 0.0012\n",
      "Correlation coefficient: 0.6731\n",
      "Training epoch: 103 [0/900 (0%)]\tLoss: 0.000591\n",
      "Training epoch: 103 [160/900 (18%)]\tLoss: 0.000351\n",
      "Training epoch: 103 [320/900 (35%)]\tLoss: 0.000623\n",
      "Training epoch: 103 [480/900 (53%)]\tLoss: 0.000764\n",
      "Training epoch: 103 [640/900 (70%)]\tLoss: 0.000169\n",
      "Training epoch: 103 [800/900 (88%)]\tLoss: 0.001655\n",
      "=========> Epoch: 103 Average loss: 0.0009\n",
      "Correlation coefficient: 0.6748\n",
      "Training epoch: 104 [0/900 (0%)]\tLoss: 0.000378\n",
      "Training epoch: 104 [160/900 (18%)]\tLoss: 0.000977\n",
      "Training epoch: 104 [320/900 (35%)]\tLoss: 0.000402\n",
      "Training epoch: 104 [480/900 (53%)]\tLoss: 0.000501\n",
      "Training epoch: 104 [640/900 (70%)]\tLoss: 0.000287\n",
      "Training epoch: 104 [800/900 (88%)]\tLoss: 0.001007\n",
      "=========> Epoch: 104 Average loss: 0.0009\n",
      "Correlation coefficient: 0.6739\n",
      "Training epoch: 105 [0/900 (0%)]\tLoss: 0.000504\n",
      "Training epoch: 105 [160/900 (18%)]\tLoss: 0.001304\n",
      "Training epoch: 105 [320/900 (35%)]\tLoss: 0.000728\n",
      "Training epoch: 105 [480/900 (53%)]\tLoss: 0.001032\n",
      "Training epoch: 105 [640/900 (70%)]\tLoss: 0.006536\n",
      "Training epoch: 105 [800/900 (88%)]\tLoss: 0.000287\n",
      "=========> Epoch: 105 Average loss: 0.0010\n",
      "Correlation coefficient: 0.6745\n",
      "Training epoch: 106 [0/900 (0%)]\tLoss: 0.001856\n",
      "Training epoch: 106 [160/900 (18%)]\tLoss: 0.000698\n",
      "Training epoch: 106 [320/900 (35%)]\tLoss: 0.000672\n",
      "Training epoch: 106 [480/900 (53%)]\tLoss: 0.000535\n",
      "Training epoch: 106 [640/900 (70%)]\tLoss: 0.001351\n",
      "Training epoch: 106 [800/900 (88%)]\tLoss: 0.000674\n",
      "=========> Epoch: 106 Average loss: 0.0013\n",
      "Correlation coefficient: 0.6767\n",
      "Training epoch: 107 [0/900 (0%)]\tLoss: 0.001459\n",
      "Training epoch: 107 [160/900 (18%)]\tLoss: 0.000406\n",
      "Training epoch: 107 [320/900 (35%)]\tLoss: 0.002597\n",
      "Training epoch: 107 [480/900 (53%)]\tLoss: 0.000870\n",
      "Training epoch: 107 [640/900 (70%)]\tLoss: 0.001235\n",
      "Training epoch: 107 [800/900 (88%)]\tLoss: 0.000824\n",
      "=========> Epoch: 107 Average loss: 0.0011\n",
      "Correlation coefficient: 0.6760\n",
      "Training epoch: 108 [0/900 (0%)]\tLoss: 0.001656\n",
      "Training epoch: 108 [160/900 (18%)]\tLoss: 0.002197\n",
      "Training epoch: 108 [320/900 (35%)]\tLoss: 0.000455\n",
      "Training epoch: 108 [480/900 (53%)]\tLoss: 0.000424\n",
      "Training epoch: 108 [640/900 (70%)]\tLoss: 0.001294\n",
      "Training epoch: 108 [800/900 (88%)]\tLoss: 0.000815\n",
      "=========> Epoch: 108 Average loss: 0.0013\n",
      "Correlation coefficient: 0.6751\n",
      "Training epoch: 109 [0/900 (0%)]\tLoss: 0.000462\n",
      "Training epoch: 109 [160/900 (18%)]\tLoss: 0.002079\n",
      "Training epoch: 109 [320/900 (35%)]\tLoss: 0.003789\n",
      "Training epoch: 109 [480/900 (53%)]\tLoss: 0.001113\n",
      "Training epoch: 109 [640/900 (70%)]\tLoss: 0.001228\n",
      "Training epoch: 109 [800/900 (88%)]\tLoss: 0.000977\n",
      "=========> Epoch: 109 Average loss: 0.0016\n",
      "Correlation coefficient: 0.6755\n",
      "Training epoch: 110 [0/900 (0%)]\tLoss: 0.001598\n",
      "Training epoch: 110 [160/900 (18%)]\tLoss: 0.001617\n",
      "Training epoch: 110 [320/900 (35%)]\tLoss: 0.006234\n",
      "Training epoch: 110 [480/900 (53%)]\tLoss: 0.005863\n",
      "Training epoch: 110 [640/900 (70%)]\tLoss: 0.000688\n",
      "Training epoch: 110 [800/900 (88%)]\tLoss: 0.003239\n",
      "=========> Epoch: 110 Average loss: 0.0024\n",
      "Correlation coefficient: 0.6720\n",
      "Training epoch: 111 [0/900 (0%)]\tLoss: 0.002212\n",
      "Training epoch: 111 [160/900 (18%)]\tLoss: 0.003153\n",
      "Training epoch: 111 [320/900 (35%)]\tLoss: 0.004142\n",
      "Training epoch: 111 [480/900 (53%)]\tLoss: 0.007780\n",
      "Training epoch: 111 [640/900 (70%)]\tLoss: 0.000956\n",
      "Training epoch: 111 [800/900 (88%)]\tLoss: 0.003631\n",
      "=========> Epoch: 111 Average loss: 0.0040\n",
      "Correlation coefficient: 0.6644\n",
      "Training epoch: 112 [0/900 (0%)]\tLoss: 0.001988\n",
      "Training epoch: 112 [160/900 (18%)]\tLoss: 0.004910\n",
      "Training epoch: 112 [320/900 (35%)]\tLoss: 0.006647\n",
      "Training epoch: 112 [480/900 (53%)]\tLoss: 0.004083\n",
      "Training epoch: 112 [640/900 (70%)]\tLoss: 0.003660\n",
      "Training epoch: 112 [800/900 (88%)]\tLoss: 0.002348\n",
      "=========> Epoch: 112 Average loss: 0.0046\n",
      "Correlation coefficient: 0.6745\n",
      "Training epoch: 113 [0/900 (0%)]\tLoss: 0.004671\n",
      "Training epoch: 113 [160/900 (18%)]\tLoss: 0.005184\n",
      "Training epoch: 113 [320/900 (35%)]\tLoss: 0.004286\n",
      "Training epoch: 113 [480/900 (53%)]\tLoss: 0.003163\n",
      "Training epoch: 113 [640/900 (70%)]\tLoss: 0.002468\n",
      "Training epoch: 113 [800/900 (88%)]\tLoss: 0.002612\n",
      "=========> Epoch: 113 Average loss: 0.0051\n",
      "Correlation coefficient: 0.6699\n",
      "Training epoch: 114 [0/900 (0%)]\tLoss: 0.002205\n",
      "Training epoch: 114 [160/900 (18%)]\tLoss: 0.002874\n",
      "Training epoch: 114 [320/900 (35%)]\tLoss: 0.005573\n",
      "Training epoch: 114 [480/900 (53%)]\tLoss: 0.004776\n",
      "Training epoch: 114 [640/900 (70%)]\tLoss: 0.010096\n",
      "Training epoch: 114 [800/900 (88%)]\tLoss: 0.002416\n",
      "=========> Epoch: 114 Average loss: 0.0064\n",
      "Correlation coefficient: 0.6658\n",
      "Training epoch: 115 [0/900 (0%)]\tLoss: 0.009186\n",
      "Training epoch: 115 [160/900 (18%)]\tLoss: 0.009823\n",
      "Training epoch: 115 [320/900 (35%)]\tLoss: 0.020940\n",
      "Training epoch: 115 [480/900 (53%)]\tLoss: 0.004622\n",
      "Training epoch: 115 [640/900 (70%)]\tLoss: 0.008615\n",
      "Training epoch: 115 [800/900 (88%)]\tLoss: 0.008262\n",
      "=========> Epoch: 115 Average loss: 0.0076\n",
      "Correlation coefficient: 0.6756\n",
      "Training epoch: 116 [0/900 (0%)]\tLoss: 0.002857\n",
      "Training epoch: 116 [160/900 (18%)]\tLoss: 0.002748\n",
      "Training epoch: 116 [320/900 (35%)]\tLoss: 0.002945\n",
      "Training epoch: 116 [480/900 (53%)]\tLoss: 0.007657\n",
      "Training epoch: 116 [640/900 (70%)]\tLoss: 0.010110\n",
      "Training epoch: 116 [800/900 (88%)]\tLoss: 0.004535\n",
      "=========> Epoch: 116 Average loss: 0.0051\n",
      "Correlation coefficient: 0.6725\n",
      "Training epoch: 117 [0/900 (0%)]\tLoss: 0.002028\n",
      "Training epoch: 117 [160/900 (18%)]\tLoss: 0.002477\n",
      "Training epoch: 117 [320/900 (35%)]\tLoss: 0.006475\n",
      "Training epoch: 117 [480/900 (53%)]\tLoss: 0.003551\n",
      "Training epoch: 117 [640/900 (70%)]\tLoss: 0.003132\n",
      "Training epoch: 117 [800/900 (88%)]\tLoss: 0.003067\n",
      "=========> Epoch: 117 Average loss: 0.0050\n",
      "Correlation coefficient: 0.6815\n",
      "✅ Epoch 117: New best correlation = 0.6815\n",
      "Training epoch: 118 [0/900 (0%)]\tLoss: 0.004178\n",
      "Training epoch: 118 [160/900 (18%)]\tLoss: 0.008844\n",
      "Training epoch: 118 [320/900 (35%)]\tLoss: 0.005525\n",
      "Training epoch: 118 [480/900 (53%)]\tLoss: 0.005856\n",
      "Training epoch: 118 [640/900 (70%)]\tLoss: 0.001935\n",
      "Training epoch: 118 [800/900 (88%)]\tLoss: 0.002471\n",
      "=========> Epoch: 118 Average loss: 0.0046\n",
      "Correlation coefficient: 0.6628\n",
      "Training epoch: 119 [0/900 (0%)]\tLoss: 0.002582\n",
      "Training epoch: 119 [160/900 (18%)]\tLoss: 0.001748\n",
      "Training epoch: 119 [320/900 (35%)]\tLoss: 0.003078\n",
      "Training epoch: 119 [480/900 (53%)]\tLoss: 0.004101\n",
      "Training epoch: 119 [640/900 (70%)]\tLoss: 0.017952\n",
      "Training epoch: 119 [800/900 (88%)]\tLoss: 0.002043\n",
      "=========> Epoch: 119 Average loss: 0.0049\n",
      "Correlation coefficient: 0.6764\n",
      "Training epoch: 120 [0/900 (0%)]\tLoss: 0.001940\n",
      "Training epoch: 120 [160/900 (18%)]\tLoss: 0.003682\n",
      "Training epoch: 120 [320/900 (35%)]\tLoss: 0.003558\n",
      "Training epoch: 120 [480/900 (53%)]\tLoss: 0.006141\n",
      "Training epoch: 120 [640/900 (70%)]\tLoss: 0.001281\n",
      "Training epoch: 120 [800/900 (88%)]\tLoss: 0.003410\n",
      "=========> Epoch: 120 Average loss: 0.0035\n",
      "Correlation coefficient: 0.6760\n",
      "Training epoch: 121 [0/900 (0%)]\tLoss: 0.003124\n",
      "Training epoch: 121 [160/900 (18%)]\tLoss: 0.003880\n",
      "Training epoch: 121 [320/900 (35%)]\tLoss: 0.001371\n",
      "Training epoch: 121 [480/900 (53%)]\tLoss: 0.002361\n",
      "Training epoch: 121 [640/900 (70%)]\tLoss: 0.001453\n",
      "Training epoch: 121 [800/900 (88%)]\tLoss: 0.006737\n",
      "=========> Epoch: 121 Average loss: 0.0027\n",
      "Correlation coefficient: 0.6767\n",
      "Training epoch: 122 [0/900 (0%)]\tLoss: 0.001266\n",
      "Training epoch: 122 [160/900 (18%)]\tLoss: 0.003652\n",
      "Training epoch: 122 [320/900 (35%)]\tLoss: 0.003527\n",
      "Training epoch: 122 [480/900 (53%)]\tLoss: 0.001382\n",
      "Training epoch: 122 [640/900 (70%)]\tLoss: 0.002395\n",
      "Training epoch: 122 [800/900 (88%)]\tLoss: 0.008625\n",
      "=========> Epoch: 122 Average loss: 0.0028\n",
      "Correlation coefficient: 0.6712\n",
      "Training epoch: 123 [0/900 (0%)]\tLoss: 0.002874\n",
      "Training epoch: 123 [160/900 (18%)]\tLoss: 0.007713\n",
      "Training epoch: 123 [320/900 (35%)]\tLoss: 0.002650\n",
      "Training epoch: 123 [480/900 (53%)]\tLoss: 0.002317\n",
      "Training epoch: 123 [640/900 (70%)]\tLoss: 0.001946\n",
      "Training epoch: 123 [800/900 (88%)]\tLoss: 0.003280\n",
      "=========> Epoch: 123 Average loss: 0.0036\n",
      "Correlation coefficient: 0.6695\n",
      "Training epoch: 124 [0/900 (0%)]\tLoss: 0.001465\n",
      "Training epoch: 124 [160/900 (18%)]\tLoss: 0.002834\n",
      "Training epoch: 124 [320/900 (35%)]\tLoss: 0.003021\n",
      "Training epoch: 124 [480/900 (53%)]\tLoss: 0.002524\n",
      "Training epoch: 124 [640/900 (70%)]\tLoss: 0.001405\n",
      "Training epoch: 124 [800/900 (88%)]\tLoss: 0.001509\n",
      "=========> Epoch: 124 Average loss: 0.0033\n",
      "Correlation coefficient: 0.6751\n",
      "Training epoch: 125 [0/900 (0%)]\tLoss: 0.001488\n",
      "Training epoch: 125 [160/900 (18%)]\tLoss: 0.002588\n",
      "Training epoch: 125 [320/900 (35%)]\tLoss: 0.001835\n",
      "Training epoch: 125 [480/900 (53%)]\tLoss: 0.003011\n",
      "Training epoch: 125 [640/900 (70%)]\tLoss: 0.004004\n",
      "Training epoch: 125 [800/900 (88%)]\tLoss: 0.004033\n",
      "=========> Epoch: 125 Average loss: 0.0034\n",
      "Correlation coefficient: 0.6709\n",
      "Training epoch: 126 [0/900 (0%)]\tLoss: 0.003810\n",
      "Training epoch: 126 [160/900 (18%)]\tLoss: 0.006447\n",
      "Training epoch: 126 [320/900 (35%)]\tLoss: 0.005665\n",
      "Training epoch: 126 [480/900 (53%)]\tLoss: 0.004882\n",
      "Training epoch: 126 [640/900 (70%)]\tLoss: 0.003586\n",
      "Training epoch: 126 [800/900 (88%)]\tLoss: 0.001628\n",
      "=========> Epoch: 126 Average loss: 0.0039\n",
      "Correlation coefficient: 0.6689\n",
      "Training epoch: 127 [0/900 (0%)]\tLoss: 0.003599\n",
      "Training epoch: 127 [160/900 (18%)]\tLoss: 0.004805\n",
      "Training epoch: 127 [320/900 (35%)]\tLoss: 0.001864\n",
      "Training epoch: 127 [480/900 (53%)]\tLoss: 0.005633\n",
      "Training epoch: 127 [640/900 (70%)]\tLoss: 0.004266\n",
      "Training epoch: 127 [800/900 (88%)]\tLoss: 0.001759\n",
      "=========> Epoch: 127 Average loss: 0.0041\n",
      "Correlation coefficient: 0.6676\n",
      "Training epoch: 128 [0/900 (0%)]\tLoss: 0.001891\n",
      "Training epoch: 128 [160/900 (18%)]\tLoss: 0.002627\n",
      "Training epoch: 128 [320/900 (35%)]\tLoss: 0.004081\n",
      "Training epoch: 128 [480/900 (53%)]\tLoss: 0.003394\n",
      "Training epoch: 128 [640/900 (70%)]\tLoss: 0.003722\n",
      "Training epoch: 128 [800/900 (88%)]\tLoss: 0.007553\n",
      "=========> Epoch: 128 Average loss: 0.0053\n",
      "Correlation coefficient: 0.6650\n",
      "Training epoch: 129 [0/900 (0%)]\tLoss: 0.002191\n",
      "Training epoch: 129 [160/900 (18%)]\tLoss: 0.001827\n",
      "Training epoch: 129 [320/900 (35%)]\tLoss: 0.004283\n",
      "Training epoch: 129 [480/900 (53%)]\tLoss: 0.003537\n",
      "Training epoch: 129 [640/900 (70%)]\tLoss: 0.004342\n",
      "Training epoch: 129 [800/900 (88%)]\tLoss: 0.002267\n",
      "=========> Epoch: 129 Average loss: 0.0052\n",
      "Correlation coefficient: 0.6771\n",
      "Training epoch: 130 [0/900 (0%)]\tLoss: 0.005041\n",
      "Training epoch: 130 [160/900 (18%)]\tLoss: 0.004641\n",
      "Training epoch: 130 [320/900 (35%)]\tLoss: 0.005324\n",
      "Training epoch: 130 [480/900 (53%)]\tLoss: 0.006908\n",
      "Training epoch: 130 [640/900 (70%)]\tLoss: 0.006404\n",
      "Training epoch: 130 [800/900 (88%)]\tLoss: 0.004123\n",
      "=========> Epoch: 130 Average loss: 0.0052\n",
      "Correlation coefficient: 0.6774\n",
      "Training epoch: 131 [0/900 (0%)]\tLoss: 0.002767\n",
      "Training epoch: 131 [160/900 (18%)]\tLoss: 0.020248\n",
      "Training epoch: 131 [320/900 (35%)]\tLoss: 0.005887\n",
      "Training epoch: 131 [480/900 (53%)]\tLoss: 0.003028\n",
      "Training epoch: 131 [640/900 (70%)]\tLoss: 0.003812\n",
      "Training epoch: 131 [800/900 (88%)]\tLoss: 0.001970\n",
      "=========> Epoch: 131 Average loss: 0.0044\n",
      "Correlation coefficient: 0.6797\n",
      "Training epoch: 132 [0/900 (0%)]\tLoss: 0.002183\n",
      "Training epoch: 132 [160/900 (18%)]\tLoss: 0.003418\n",
      "Training epoch: 132 [320/900 (35%)]\tLoss: 0.003439\n",
      "Training epoch: 132 [480/900 (53%)]\tLoss: 0.002704\n",
      "Training epoch: 132 [640/900 (70%)]\tLoss: 0.004296\n",
      "Training epoch: 132 [800/900 (88%)]\tLoss: 0.002548\n",
      "=========> Epoch: 132 Average loss: 0.0039\n",
      "Correlation coefficient: 0.6632\n",
      "Training epoch: 133 [0/900 (0%)]\tLoss: 0.007184\n",
      "Training epoch: 133 [160/900 (18%)]\tLoss: 0.005509\n",
      "Training epoch: 133 [320/900 (35%)]\tLoss: 0.003664\n",
      "Training epoch: 133 [480/900 (53%)]\tLoss: 0.009595\n",
      "Training epoch: 133 [640/900 (70%)]\tLoss: 0.001512\n",
      "Training epoch: 133 [800/900 (88%)]\tLoss: 0.003732\n",
      "=========> Epoch: 133 Average loss: 0.0052\n",
      "Correlation coefficient: 0.6706\n",
      "Training epoch: 134 [0/900 (0%)]\tLoss: 0.003993\n",
      "Training epoch: 134 [160/900 (18%)]\tLoss: 0.003488\n",
      "Training epoch: 134 [320/900 (35%)]\tLoss: 0.006363\n",
      "Training epoch: 134 [480/900 (53%)]\tLoss: 0.007772\n",
      "Training epoch: 134 [640/900 (70%)]\tLoss: 0.007020\n",
      "Training epoch: 134 [800/900 (88%)]\tLoss: 0.002974\n",
      "=========> Epoch: 134 Average loss: 0.0049\n",
      "Correlation coefficient: 0.6717\n",
      "Training epoch: 135 [0/900 (0%)]\tLoss: 0.004652\n",
      "Training epoch: 135 [160/900 (18%)]\tLoss: 0.004702\n",
      "Training epoch: 135 [320/900 (35%)]\tLoss: 0.002557\n",
      "Training epoch: 135 [480/900 (53%)]\tLoss: 0.005960\n",
      "Training epoch: 135 [640/900 (70%)]\tLoss: 0.004678\n",
      "Training epoch: 135 [800/900 (88%)]\tLoss: 0.003923\n",
      "=========> Epoch: 135 Average loss: 0.0049\n",
      "Correlation coefficient: 0.6674\n",
      "Training epoch: 136 [0/900 (0%)]\tLoss: 0.014653\n",
      "Training epoch: 136 [160/900 (18%)]\tLoss: 0.003320\n",
      "Training epoch: 136 [320/900 (35%)]\tLoss: 0.008564\n",
      "Training epoch: 136 [480/900 (53%)]\tLoss: 0.007053\n",
      "Training epoch: 136 [640/900 (70%)]\tLoss: 0.003573\n",
      "Training epoch: 136 [800/900 (88%)]\tLoss: 0.008331\n",
      "=========> Epoch: 136 Average loss: 0.0055\n",
      "Correlation coefficient: 0.6793\n",
      "Training epoch: 137 [0/900 (0%)]\tLoss: 0.005199\n",
      "Training epoch: 137 [160/900 (18%)]\tLoss: 0.009302\n",
      "Training epoch: 137 [320/900 (35%)]\tLoss: 0.005044\n",
      "Training epoch: 137 [480/900 (53%)]\tLoss: 0.031658\n",
      "Training epoch: 137 [640/900 (70%)]\tLoss: 0.006254\n",
      "Training epoch: 137 [800/900 (88%)]\tLoss: 0.012289\n",
      "=========> Epoch: 137 Average loss: 0.0075\n",
      "Correlation coefficient: 0.6549\n",
      "Training epoch: 138 [0/900 (0%)]\tLoss: 0.006780\n",
      "Training epoch: 138 [160/900 (18%)]\tLoss: 0.003707\n",
      "Training epoch: 138 [320/900 (35%)]\tLoss: 0.008603\n",
      "Training epoch: 138 [480/900 (53%)]\tLoss: 0.010939\n",
      "Training epoch: 138 [640/900 (70%)]\tLoss: 0.003728\n",
      "Training epoch: 138 [800/900 (88%)]\tLoss: 0.014024\n",
      "=========> Epoch: 138 Average loss: 0.0082\n",
      "Correlation coefficient: 0.6798\n",
      "Training epoch: 139 [0/900 (0%)]\tLoss: 0.007751\n",
      "Training epoch: 139 [160/900 (18%)]\tLoss: 0.002121\n",
      "Training epoch: 139 [320/900 (35%)]\tLoss: 0.003694\n",
      "Training epoch: 139 [480/900 (53%)]\tLoss: 0.020585\n",
      "Training epoch: 139 [640/900 (70%)]\tLoss: 0.013006\n",
      "Training epoch: 139 [800/900 (88%)]\tLoss: 0.010816\n",
      "=========> Epoch: 139 Average loss: 0.0113\n",
      "Correlation coefficient: 0.6728\n",
      "Training epoch: 140 [0/900 (0%)]\tLoss: 0.013274\n",
      "Training epoch: 140 [160/900 (18%)]\tLoss: 0.005676\n",
      "Training epoch: 140 [320/900 (35%)]\tLoss: 0.029518\n",
      "Training epoch: 140 [480/900 (53%)]\tLoss: 0.005681\n",
      "Training epoch: 140 [640/900 (70%)]\tLoss: 0.006964\n",
      "Training epoch: 140 [800/900 (88%)]\tLoss: 0.004909\n",
      "=========> Epoch: 140 Average loss: 0.0096\n",
      "Correlation coefficient: 0.6686\n",
      "Training epoch: 141 [0/900 (0%)]\tLoss: 0.003048\n",
      "Training epoch: 141 [160/900 (18%)]\tLoss: 0.011558\n",
      "Training epoch: 141 [320/900 (35%)]\tLoss: 0.025982\n",
      "Training epoch: 141 [480/900 (53%)]\tLoss: 0.012744\n",
      "Training epoch: 141 [640/900 (70%)]\tLoss: 0.003963\n",
      "Training epoch: 141 [800/900 (88%)]\tLoss: 0.004954\n",
      "=========> Epoch: 141 Average loss: 0.0121\n",
      "Correlation coefficient: 0.6626\n",
      "Training epoch: 142 [0/900 (0%)]\tLoss: 0.018703\n",
      "Training epoch: 142 [160/900 (18%)]\tLoss: 0.012202\n",
      "Training epoch: 142 [320/900 (35%)]\tLoss: 0.012064\n",
      "Training epoch: 142 [480/900 (53%)]\tLoss: 0.007113\n",
      "Training epoch: 142 [640/900 (70%)]\tLoss: 0.007106\n",
      "Training epoch: 142 [800/900 (88%)]\tLoss: 0.008996\n",
      "=========> Epoch: 142 Average loss: 0.0131\n",
      "Correlation coefficient: 0.6771\n",
      "Training epoch: 143 [0/900 (0%)]\tLoss: 0.005714\n",
      "Training epoch: 143 [160/900 (18%)]\tLoss: 0.005142\n",
      "Training epoch: 143 [320/900 (35%)]\tLoss: 0.007742\n",
      "Training epoch: 143 [480/900 (53%)]\tLoss: 0.005606\n",
      "Training epoch: 143 [640/900 (70%)]\tLoss: 0.002946\n",
      "Training epoch: 143 [800/900 (88%)]\tLoss: 0.007714\n",
      "=========> Epoch: 143 Average loss: 0.0073\n",
      "Correlation coefficient: 0.6874\n",
      "✅ Epoch 143: New best correlation = 0.6874\n",
      "Training epoch: 144 [0/900 (0%)]\tLoss: 0.002958\n",
      "Training epoch: 144 [160/900 (18%)]\tLoss: 0.000904\n",
      "Training epoch: 144 [320/900 (35%)]\tLoss: 0.016529\n",
      "Training epoch: 144 [480/900 (53%)]\tLoss: 0.005202\n",
      "Training epoch: 144 [640/900 (70%)]\tLoss: 0.007493\n",
      "Training epoch: 144 [800/900 (88%)]\tLoss: 0.003494\n",
      "=========> Epoch: 144 Average loss: 0.0049\n",
      "Correlation coefficient: 0.6696\n",
      "Training epoch: 145 [0/900 (0%)]\tLoss: 0.009286\n",
      "Training epoch: 145 [160/900 (18%)]\tLoss: 0.003290\n",
      "Training epoch: 145 [320/900 (35%)]\tLoss: 0.004325\n",
      "Training epoch: 145 [480/900 (53%)]\tLoss: 0.004129\n",
      "Training epoch: 145 [640/900 (70%)]\tLoss: 0.001758\n",
      "Training epoch: 145 [800/900 (88%)]\tLoss: 0.004466\n",
      "=========> Epoch: 145 Average loss: 0.0039\n",
      "Correlation coefficient: 0.6753\n",
      "Training epoch: 146 [0/900 (0%)]\tLoss: 0.008217\n",
      "Training epoch: 146 [160/900 (18%)]\tLoss: 0.002681\n",
      "Training epoch: 146 [320/900 (35%)]\tLoss: 0.005870\n",
      "Training epoch: 146 [480/900 (53%)]\tLoss: 0.003227\n",
      "Training epoch: 146 [640/900 (70%)]\tLoss: 0.001520\n",
      "Training epoch: 146 [800/900 (88%)]\tLoss: 0.002053\n",
      "=========> Epoch: 146 Average loss: 0.0058\n",
      "Correlation coefficient: 0.6651\n",
      "Training epoch: 147 [0/900 (0%)]\tLoss: 0.002804\n",
      "Training epoch: 147 [160/900 (18%)]\tLoss: 0.007531\n",
      "Training epoch: 147 [320/900 (35%)]\tLoss: 0.002503\n",
      "Training epoch: 147 [480/900 (53%)]\tLoss: 0.003158\n",
      "Training epoch: 147 [640/900 (70%)]\tLoss: 0.003873\n",
      "Training epoch: 147 [800/900 (88%)]\tLoss: 0.002112\n",
      "=========> Epoch: 147 Average loss: 0.0036\n",
      "Correlation coefficient: 0.6834\n",
      "Training epoch: 148 [0/900 (0%)]\tLoss: 0.001896\n",
      "Training epoch: 148 [160/900 (18%)]\tLoss: 0.002457\n",
      "Training epoch: 148 [320/900 (35%)]\tLoss: 0.001552\n",
      "Training epoch: 148 [480/900 (53%)]\tLoss: 0.001239\n",
      "Training epoch: 148 [640/900 (70%)]\tLoss: 0.001540\n",
      "Training epoch: 148 [800/900 (88%)]\tLoss: 0.001670\n",
      "=========> Epoch: 148 Average loss: 0.0025\n",
      "Correlation coefficient: 0.6734\n",
      "Training epoch: 149 [0/900 (0%)]\tLoss: 0.001857\n",
      "Training epoch: 149 [160/900 (18%)]\tLoss: 0.001883\n",
      "Training epoch: 149 [320/900 (35%)]\tLoss: 0.002797\n",
      "Training epoch: 149 [480/900 (53%)]\tLoss: 0.002544\n",
      "Training epoch: 149 [640/900 (70%)]\tLoss: 0.007166\n",
      "Training epoch: 149 [800/900 (88%)]\tLoss: 0.008745\n",
      "=========> Epoch: 149 Average loss: 0.0024\n",
      "Correlation coefficient: 0.6764\n",
      "Training epoch: 150 [0/900 (0%)]\tLoss: 0.004331\n",
      "Training epoch: 150 [160/900 (18%)]\tLoss: 0.002060\n",
      "Training epoch: 150 [320/900 (35%)]\tLoss: 0.000646\n",
      "Training epoch: 150 [480/900 (53%)]\tLoss: 0.004955\n",
      "Training epoch: 150 [640/900 (70%)]\tLoss: 0.002386\n",
      "Training epoch: 150 [800/900 (88%)]\tLoss: 0.001108\n",
      "=========> Epoch: 150 Average loss: 0.0025\n",
      "Correlation coefficient: 0.6858\n",
      "Training epoch: 151 [0/900 (0%)]\tLoss: 0.004175\n",
      "Training epoch: 151 [160/900 (18%)]\tLoss: 0.001357\n",
      "Training epoch: 151 [320/900 (35%)]\tLoss: 0.001310\n",
      "Training epoch: 151 [480/900 (53%)]\tLoss: 0.001474\n",
      "Training epoch: 151 [640/900 (70%)]\tLoss: 0.001162\n",
      "Training epoch: 151 [800/900 (88%)]\tLoss: 0.001613\n",
      "=========> Epoch: 151 Average loss: 0.0024\n",
      "Correlation coefficient: 0.6764\n",
      "Training epoch: 152 [0/900 (0%)]\tLoss: 0.001949\n",
      "Training epoch: 152 [160/900 (18%)]\tLoss: 0.002824\n",
      "Training epoch: 152 [320/900 (35%)]\tLoss: 0.003356\n",
      "Training epoch: 152 [480/900 (53%)]\tLoss: 0.002893\n",
      "Training epoch: 152 [640/900 (70%)]\tLoss: 0.001696\n",
      "Training epoch: 152 [800/900 (88%)]\tLoss: 0.001177\n",
      "=========> Epoch: 152 Average loss: 0.0021\n",
      "Correlation coefficient: 0.6746\n",
      "Training epoch: 153 [0/900 (0%)]\tLoss: 0.002018\n",
      "Training epoch: 153 [160/900 (18%)]\tLoss: 0.008990\n",
      "Training epoch: 153 [320/900 (35%)]\tLoss: 0.001019\n",
      "Training epoch: 153 [480/900 (53%)]\tLoss: 0.002273\n",
      "Training epoch: 153 [640/900 (70%)]\tLoss: 0.003233\n",
      "Training epoch: 153 [800/900 (88%)]\tLoss: 0.002612\n",
      "=========> Epoch: 153 Average loss: 0.0029\n",
      "Correlation coefficient: 0.6715\n",
      "Training epoch: 154 [0/900 (0%)]\tLoss: 0.004040\n",
      "Training epoch: 154 [160/900 (18%)]\tLoss: 0.001930\n",
      "Training epoch: 154 [320/900 (35%)]\tLoss: 0.002481\n",
      "Training epoch: 154 [480/900 (53%)]\tLoss: 0.001703\n",
      "Training epoch: 154 [640/900 (70%)]\tLoss: 0.002316\n",
      "Training epoch: 154 [800/900 (88%)]\tLoss: 0.004219\n",
      "=========> Epoch: 154 Average loss: 0.0032\n",
      "Correlation coefficient: 0.6735\n",
      "Training epoch: 155 [0/900 (0%)]\tLoss: 0.000764\n",
      "Training epoch: 155 [160/900 (18%)]\tLoss: 0.000790\n",
      "Training epoch: 155 [320/900 (35%)]\tLoss: 0.001943\n",
      "Training epoch: 155 [480/900 (53%)]\tLoss: 0.003402\n",
      "Training epoch: 155 [640/900 (70%)]\tLoss: 0.002294\n",
      "Training epoch: 155 [800/900 (88%)]\tLoss: 0.001537\n",
      "=========> Epoch: 155 Average loss: 0.0031\n",
      "Correlation coefficient: 0.6739\n",
      "Training epoch: 156 [0/900 (0%)]\tLoss: 0.002230\n",
      "Training epoch: 156 [160/900 (18%)]\tLoss: 0.004594\n",
      "Training epoch: 156 [320/900 (35%)]\tLoss: 0.003961\n",
      "Training epoch: 156 [480/900 (53%)]\tLoss: 0.002086\n",
      "Training epoch: 156 [640/900 (70%)]\tLoss: 0.003069\n",
      "Training epoch: 156 [800/900 (88%)]\tLoss: 0.003067\n",
      "=========> Epoch: 156 Average loss: 0.0036\n",
      "Correlation coefficient: 0.6702\n",
      "Training epoch: 157 [0/900 (0%)]\tLoss: 0.006483\n",
      "Training epoch: 157 [160/900 (18%)]\tLoss: 0.001484\n",
      "Training epoch: 157 [320/900 (35%)]\tLoss: 0.000349\n",
      "Training epoch: 157 [480/900 (53%)]\tLoss: 0.002353\n",
      "Training epoch: 157 [640/900 (70%)]\tLoss: 0.005324\n",
      "Training epoch: 157 [800/900 (88%)]\tLoss: 0.001089\n",
      "=========> Epoch: 157 Average loss: 0.0027\n",
      "Correlation coefficient: 0.6785\n",
      "Training epoch: 158 [0/900 (0%)]\tLoss: 0.001758\n",
      "Training epoch: 158 [160/900 (18%)]\tLoss: 0.004983\n",
      "Training epoch: 158 [320/900 (35%)]\tLoss: 0.000809\n",
      "Training epoch: 158 [480/900 (53%)]\tLoss: 0.003577\n",
      "Training epoch: 158 [640/900 (70%)]\tLoss: 0.004176\n",
      "Training epoch: 158 [800/900 (88%)]\tLoss: 0.001362\n",
      "=========> Epoch: 158 Average loss: 0.0025\n",
      "Correlation coefficient: 0.6695\n",
      "Training epoch: 159 [0/900 (0%)]\tLoss: 0.004497\n",
      "Training epoch: 159 [160/900 (18%)]\tLoss: 0.003053\n",
      "Training epoch: 159 [320/900 (35%)]\tLoss: 0.002697\n",
      "Training epoch: 159 [480/900 (53%)]\tLoss: 0.003022\n",
      "Training epoch: 159 [640/900 (70%)]\tLoss: 0.001049\n",
      "Training epoch: 159 [800/900 (88%)]\tLoss: 0.001466\n",
      "=========> Epoch: 159 Average loss: 0.0029\n",
      "Correlation coefficient: 0.6807\n",
      "Training epoch: 160 [0/900 (0%)]\tLoss: 0.002096\n",
      "Training epoch: 160 [160/900 (18%)]\tLoss: 0.000594\n",
      "Training epoch: 160 [320/900 (35%)]\tLoss: 0.002757\n",
      "Training epoch: 160 [480/900 (53%)]\tLoss: 0.004631\n",
      "Training epoch: 160 [640/900 (70%)]\tLoss: 0.000809\n",
      "Training epoch: 160 [800/900 (88%)]\tLoss: 0.000759\n",
      "=========> Epoch: 160 Average loss: 0.0024\n",
      "Correlation coefficient: 0.6684\n",
      "Training epoch: 161 [0/900 (0%)]\tLoss: 0.001008\n",
      "Training epoch: 161 [160/900 (18%)]\tLoss: 0.004224\n",
      "Training epoch: 161 [320/900 (35%)]\tLoss: 0.001083\n",
      "Training epoch: 161 [480/900 (53%)]\tLoss: 0.001132\n",
      "Training epoch: 161 [640/900 (70%)]\tLoss: 0.002346\n",
      "Training epoch: 161 [800/900 (88%)]\tLoss: 0.002128\n",
      "=========> Epoch: 161 Average loss: 0.0021\n",
      "Correlation coefficient: 0.6790\n",
      "Training epoch: 162 [0/900 (0%)]\tLoss: 0.007352\n",
      "Training epoch: 162 [160/900 (18%)]\tLoss: 0.002408\n",
      "Training epoch: 162 [320/900 (35%)]\tLoss: 0.001165\n",
      "Training epoch: 162 [480/900 (53%)]\tLoss: 0.000722\n",
      "Training epoch: 162 [640/900 (70%)]\tLoss: 0.002230\n",
      "Training epoch: 162 [800/900 (88%)]\tLoss: 0.002512\n",
      "=========> Epoch: 162 Average loss: 0.0023\n",
      "Correlation coefficient: 0.6627\n",
      "Training epoch: 163 [0/900 (0%)]\tLoss: 0.002192\n",
      "Training epoch: 163 [160/900 (18%)]\tLoss: 0.001219\n",
      "Training epoch: 163 [320/900 (35%)]\tLoss: 0.002815\n",
      "Training epoch: 163 [480/900 (53%)]\tLoss: 0.002284\n",
      "Training epoch: 163 [640/900 (70%)]\tLoss: 0.003546\n",
      "Training epoch: 163 [800/900 (88%)]\tLoss: 0.003623\n",
      "=========> Epoch: 163 Average loss: 0.0027\n",
      "Correlation coefficient: 0.6707\n",
      "Training epoch: 164 [0/900 (0%)]\tLoss: 0.001348\n",
      "Training epoch: 164 [160/900 (18%)]\tLoss: 0.000750\n",
      "Training epoch: 164 [320/900 (35%)]\tLoss: 0.001572\n",
      "Training epoch: 164 [480/900 (53%)]\tLoss: 0.001211\n",
      "Training epoch: 164 [640/900 (70%)]\tLoss: 0.000772\n",
      "Training epoch: 164 [800/900 (88%)]\tLoss: 0.001487\n",
      "=========> Epoch: 164 Average loss: 0.0024\n",
      "Correlation coefficient: 0.6664\n",
      "Training epoch: 165 [0/900 (0%)]\tLoss: 0.005403\n",
      "Training epoch: 165 [160/900 (18%)]\tLoss: 0.001911\n",
      "Training epoch: 165 [320/900 (35%)]\tLoss: 0.001371\n",
      "Training epoch: 165 [480/900 (53%)]\tLoss: 0.004298\n",
      "Training epoch: 165 [640/900 (70%)]\tLoss: 0.002956\n",
      "Training epoch: 165 [800/900 (88%)]\tLoss: 0.001400\n",
      "=========> Epoch: 165 Average loss: 0.0028\n",
      "Correlation coefficient: 0.6761\n",
      "Training epoch: 166 [0/900 (0%)]\tLoss: 0.002080\n",
      "Training epoch: 166 [160/900 (18%)]\tLoss: 0.001513\n",
      "Training epoch: 166 [320/900 (35%)]\tLoss: 0.003004\n",
      "Training epoch: 166 [480/900 (53%)]\tLoss: 0.002089\n",
      "Training epoch: 166 [640/900 (70%)]\tLoss: 0.005028\n",
      "Training epoch: 166 [800/900 (88%)]\tLoss: 0.005461\n",
      "=========> Epoch: 166 Average loss: 0.0033\n",
      "Correlation coefficient: 0.6703\n",
      "Training epoch: 167 [0/900 (0%)]\tLoss: 0.003002\n",
      "Training epoch: 167 [160/900 (18%)]\tLoss: 0.002275\n",
      "Training epoch: 167 [320/900 (35%)]\tLoss: 0.008949\n",
      "Training epoch: 167 [480/900 (53%)]\tLoss: 0.002276\n",
      "Training epoch: 167 [640/900 (70%)]\tLoss: 0.006954\n",
      "Training epoch: 167 [800/900 (88%)]\tLoss: 0.003036\n",
      "=========> Epoch: 167 Average loss: 0.0041\n",
      "Correlation coefficient: 0.6662\n",
      "Training epoch: 168 [0/900 (0%)]\tLoss: 0.002370\n",
      "Training epoch: 168 [160/900 (18%)]\tLoss: 0.001343\n",
      "Training epoch: 168 [320/900 (35%)]\tLoss: 0.000955\n",
      "Training epoch: 168 [480/900 (53%)]\tLoss: 0.002336\n",
      "Training epoch: 168 [640/900 (70%)]\tLoss: 0.004541\n",
      "Training epoch: 168 [800/900 (88%)]\tLoss: 0.004917\n",
      "=========> Epoch: 168 Average loss: 0.0032\n",
      "Correlation coefficient: 0.6770\n",
      "Training epoch: 169 [0/900 (0%)]\tLoss: 0.004769\n",
      "Training epoch: 169 [160/900 (18%)]\tLoss: 0.004278\n",
      "Training epoch: 169 [320/900 (35%)]\tLoss: 0.005103\n",
      "Training epoch: 169 [480/900 (53%)]\tLoss: 0.001793\n",
      "Training epoch: 169 [640/900 (70%)]\tLoss: 0.001358\n",
      "Training epoch: 169 [800/900 (88%)]\tLoss: 0.002693\n",
      "=========> Epoch: 169 Average loss: 0.0025\n",
      "Correlation coefficient: 0.6774\n",
      "Training epoch: 170 [0/900 (0%)]\tLoss: 0.001156\n",
      "Training epoch: 170 [160/900 (18%)]\tLoss: 0.011154\n",
      "Training epoch: 170 [320/900 (35%)]\tLoss: 0.001870\n",
      "Training epoch: 170 [480/900 (53%)]\tLoss: 0.002419\n",
      "Training epoch: 170 [640/900 (70%)]\tLoss: 0.000760\n",
      "Training epoch: 170 [800/900 (88%)]\tLoss: 0.002112\n",
      "=========> Epoch: 170 Average loss: 0.0023\n",
      "Correlation coefficient: 0.6719\n",
      "Training epoch: 171 [0/900 (0%)]\tLoss: 0.001845\n",
      "Training epoch: 171 [160/900 (18%)]\tLoss: 0.000874\n",
      "Training epoch: 171 [320/900 (35%)]\tLoss: 0.004175\n",
      "Training epoch: 171 [480/900 (53%)]\tLoss: 0.001025\n",
      "Training epoch: 171 [640/900 (70%)]\tLoss: 0.003114\n",
      "Training epoch: 171 [800/900 (88%)]\tLoss: 0.001996\n",
      "=========> Epoch: 171 Average loss: 0.0021\n",
      "Correlation coefficient: 0.6768\n",
      "Training epoch: 172 [0/900 (0%)]\tLoss: 0.002728\n",
      "Training epoch: 172 [160/900 (18%)]\tLoss: 0.002657\n",
      "Training epoch: 172 [320/900 (35%)]\tLoss: 0.003523\n",
      "Training epoch: 172 [480/900 (53%)]\tLoss: 0.001617\n",
      "Training epoch: 172 [640/900 (70%)]\tLoss: 0.002169\n",
      "Training epoch: 172 [800/900 (88%)]\tLoss: 0.004002\n",
      "=========> Epoch: 172 Average loss: 0.0025\n",
      "Correlation coefficient: 0.6788\n",
      "Training epoch: 173 [0/900 (0%)]\tLoss: 0.001530\n",
      "Training epoch: 173 [160/900 (18%)]\tLoss: 0.004085\n",
      "Training epoch: 173 [320/900 (35%)]\tLoss: 0.002732\n",
      "Training epoch: 173 [480/900 (53%)]\tLoss: 0.000778\n",
      "Training epoch: 173 [640/900 (70%)]\tLoss: 0.004289\n",
      "Training epoch: 173 [800/900 (88%)]\tLoss: 0.006900\n",
      "=========> Epoch: 173 Average loss: 0.0034\n",
      "Correlation coefficient: 0.6816\n",
      "Training epoch: 174 [0/900 (0%)]\tLoss: 0.004017\n",
      "Training epoch: 174 [160/900 (18%)]\tLoss: 0.001568\n",
      "Training epoch: 174 [320/900 (35%)]\tLoss: 0.021398\n",
      "Training epoch: 174 [480/900 (53%)]\tLoss: 0.001824\n",
      "Training epoch: 174 [640/900 (70%)]\tLoss: 0.002888\n",
      "Training epoch: 174 [800/900 (88%)]\tLoss: 0.001693\n",
      "=========> Epoch: 174 Average loss: 0.0036\n",
      "Correlation coefficient: 0.6801\n",
      "Training epoch: 175 [0/900 (0%)]\tLoss: 0.001836\n",
      "Training epoch: 175 [160/900 (18%)]\tLoss: 0.005990\n",
      "Training epoch: 175 [320/900 (35%)]\tLoss: 0.016076\n",
      "Training epoch: 175 [480/900 (53%)]\tLoss: 0.003478\n",
      "Training epoch: 175 [640/900 (70%)]\tLoss: 0.003896\n",
      "Training epoch: 175 [800/900 (88%)]\tLoss: 0.001966\n",
      "=========> Epoch: 175 Average loss: 0.0050\n",
      "Correlation coefficient: 0.6862\n",
      "Training epoch: 176 [0/900 (0%)]\tLoss: 0.007794\n",
      "Training epoch: 176 [160/900 (18%)]\tLoss: 0.006973\n",
      "Training epoch: 176 [320/900 (35%)]\tLoss: 0.011732\n",
      "Training epoch: 176 [480/900 (53%)]\tLoss: 0.006077\n",
      "Training epoch: 176 [640/900 (70%)]\tLoss: 0.006930\n",
      "Training epoch: 176 [800/900 (88%)]\tLoss: 0.006003\n",
      "=========> Epoch: 176 Average loss: 0.0075\n",
      "Correlation coefficient: 0.6670\n",
      "Training epoch: 177 [0/900 (0%)]\tLoss: 0.010211\n",
      "Training epoch: 177 [160/900 (18%)]\tLoss: 0.011093\n",
      "Training epoch: 177 [320/900 (35%)]\tLoss: 0.015671\n",
      "Training epoch: 177 [480/900 (53%)]\tLoss: 0.012717\n",
      "Training epoch: 177 [640/900 (70%)]\tLoss: 0.002767\n",
      "Training epoch: 177 [800/900 (88%)]\tLoss: 0.015324\n",
      "=========> Epoch: 177 Average loss: 0.0091\n",
      "Correlation coefficient: 0.6842\n",
      "Training epoch: 178 [0/900 (0%)]\tLoss: 0.003087\n",
      "Training epoch: 178 [160/900 (18%)]\tLoss: 0.007858\n",
      "Training epoch: 178 [320/900 (35%)]\tLoss: 0.009666\n",
      "Training epoch: 178 [480/900 (53%)]\tLoss: 0.017508\n",
      "Training epoch: 178 [640/900 (70%)]\tLoss: 0.012232\n",
      "Training epoch: 178 [800/900 (88%)]\tLoss: 0.005758\n",
      "=========> Epoch: 178 Average loss: 0.0093\n",
      "Correlation coefficient: 0.6871\n",
      "Training epoch: 179 [0/900 (0%)]\tLoss: 0.009886\n",
      "Training epoch: 179 [160/900 (18%)]\tLoss: 0.012177\n",
      "Training epoch: 179 [320/900 (35%)]\tLoss: 0.004652\n",
      "Training epoch: 179 [480/900 (53%)]\tLoss: 0.002280\n",
      "Training epoch: 179 [640/900 (70%)]\tLoss: 0.014717\n",
      "Training epoch: 179 [800/900 (88%)]\tLoss: 0.014558\n",
      "=========> Epoch: 179 Average loss: 0.0082\n",
      "Correlation coefficient: 0.6806\n",
      "Training epoch: 180 [0/900 (0%)]\tLoss: 0.002988\n",
      "Training epoch: 180 [160/900 (18%)]\tLoss: 0.009477\n",
      "Training epoch: 180 [320/900 (35%)]\tLoss: 0.007022\n",
      "Training epoch: 180 [480/900 (53%)]\tLoss: 0.003527\n",
      "Training epoch: 180 [640/900 (70%)]\tLoss: 0.008253\n",
      "Training epoch: 180 [800/900 (88%)]\tLoss: 0.003164\n",
      "=========> Epoch: 180 Average loss: 0.0063\n",
      "Correlation coefficient: 0.6743\n",
      "Training epoch: 181 [0/900 (0%)]\tLoss: 0.006713\n",
      "Training epoch: 181 [160/900 (18%)]\tLoss: 0.004884\n",
      "Training epoch: 181 [320/900 (35%)]\tLoss: 0.004294\n",
      "Training epoch: 181 [480/900 (53%)]\tLoss: 0.004039\n",
      "Training epoch: 181 [640/900 (70%)]\tLoss: 0.002114\n",
      "Training epoch: 181 [800/900 (88%)]\tLoss: 0.006055\n",
      "=========> Epoch: 181 Average loss: 0.0044\n",
      "Correlation coefficient: 0.6808\n",
      "Training epoch: 182 [0/900 (0%)]\tLoss: 0.004805\n",
      "Training epoch: 182 [160/900 (18%)]\tLoss: 0.005346\n",
      "Training epoch: 182 [320/900 (35%)]\tLoss: 0.005279\n",
      "Training epoch: 182 [480/900 (53%)]\tLoss: 0.014983\n",
      "Training epoch: 182 [640/900 (70%)]\tLoss: 0.009236\n",
      "Training epoch: 182 [800/900 (88%)]\tLoss: 0.002777\n",
      "=========> Epoch: 182 Average loss: 0.0091\n",
      "Correlation coefficient: 0.6561\n",
      "Training epoch: 183 [0/900 (0%)]\tLoss: 0.004697\n",
      "Training epoch: 183 [160/900 (18%)]\tLoss: 0.005492\n",
      "Training epoch: 183 [320/900 (35%)]\tLoss: 0.005363\n",
      "Training epoch: 183 [480/900 (53%)]\tLoss: 0.004526\n",
      "Training epoch: 183 [640/900 (70%)]\tLoss: 0.005678\n",
      "Training epoch: 183 [800/900 (88%)]\tLoss: 0.006990\n",
      "=========> Epoch: 183 Average loss: 0.0080\n",
      "Correlation coefficient: 0.6634\n",
      "Training epoch: 184 [0/900 (0%)]\tLoss: 0.012112\n",
      "Training epoch: 184 [160/900 (18%)]\tLoss: 0.006918\n",
      "Training epoch: 184 [320/900 (35%)]\tLoss: 0.006638\n",
      "Training epoch: 184 [480/900 (53%)]\tLoss: 0.016040\n",
      "Training epoch: 184 [640/900 (70%)]\tLoss: 0.002032\n",
      "Training epoch: 184 [800/900 (88%)]\tLoss: 0.010104\n",
      "=========> Epoch: 184 Average loss: 0.0063\n",
      "Correlation coefficient: 0.6891\n",
      "✅ Epoch 184: New best correlation = 0.6891\n",
      "Training epoch: 185 [0/900 (0%)]\tLoss: 0.002063\n",
      "Training epoch: 185 [160/900 (18%)]\tLoss: 0.008362\n",
      "Training epoch: 185 [320/900 (35%)]\tLoss: 0.002639\n",
      "Training epoch: 185 [480/900 (53%)]\tLoss: 0.002786\n",
      "Training epoch: 185 [640/900 (70%)]\tLoss: 0.003226\n",
      "Training epoch: 185 [800/900 (88%)]\tLoss: 0.004289\n",
      "=========> Epoch: 185 Average loss: 0.0040\n",
      "Correlation coefficient: 0.6770\n",
      "Training epoch: 186 [0/900 (0%)]\tLoss: 0.003263\n",
      "Training epoch: 186 [160/900 (18%)]\tLoss: 0.003341\n",
      "Training epoch: 186 [320/900 (35%)]\tLoss: 0.002709\n",
      "Training epoch: 186 [480/900 (53%)]\tLoss: 0.004039\n",
      "Training epoch: 186 [640/900 (70%)]\tLoss: 0.003506\n",
      "Training epoch: 186 [800/900 (88%)]\tLoss: 0.001277\n",
      "=========> Epoch: 186 Average loss: 0.0040\n",
      "Correlation coefficient: 0.6770\n",
      "Training epoch: 187 [0/900 (0%)]\tLoss: 0.002928\n",
      "Training epoch: 187 [160/900 (18%)]\tLoss: 0.003639\n",
      "Training epoch: 187 [320/900 (35%)]\tLoss: 0.004844\n",
      "Training epoch: 187 [480/900 (53%)]\tLoss: 0.003130\n",
      "Training epoch: 187 [640/900 (70%)]\tLoss: 0.003039\n",
      "Training epoch: 187 [800/900 (88%)]\tLoss: 0.005998\n",
      "=========> Epoch: 187 Average loss: 0.0031\n",
      "Correlation coefficient: 0.6816\n",
      "Training epoch: 188 [0/900 (0%)]\tLoss: 0.000945\n",
      "Training epoch: 188 [160/900 (18%)]\tLoss: 0.004668\n",
      "Training epoch: 188 [320/900 (35%)]\tLoss: 0.004111\n",
      "Training epoch: 188 [480/900 (53%)]\tLoss: 0.003638\n",
      "Training epoch: 188 [640/900 (70%)]\tLoss: 0.001165\n",
      "Training epoch: 188 [800/900 (88%)]\tLoss: 0.001421\n",
      "=========> Epoch: 188 Average loss: 0.0032\n",
      "Correlation coefficient: 0.6792\n",
      "Training epoch: 189 [0/900 (0%)]\tLoss: 0.001723\n",
      "Training epoch: 189 [160/900 (18%)]\tLoss: 0.006693\n",
      "Training epoch: 189 [320/900 (35%)]\tLoss: 0.002059\n",
      "Training epoch: 189 [480/900 (53%)]\tLoss: 0.001712\n",
      "Training epoch: 189 [640/900 (70%)]\tLoss: 0.026752\n",
      "Training epoch: 189 [800/900 (88%)]\tLoss: 0.000994\n",
      "=========> Epoch: 189 Average loss: 0.0039\n",
      "Correlation coefficient: 0.6785\n",
      "Training epoch: 190 [0/900 (0%)]\tLoss: 0.003760\n",
      "Training epoch: 190 [160/900 (18%)]\tLoss: 0.003581\n",
      "Training epoch: 190 [320/900 (35%)]\tLoss: 0.002484\n",
      "Training epoch: 190 [480/900 (53%)]\tLoss: 0.001207\n",
      "Training epoch: 190 [640/900 (70%)]\tLoss: 0.001223\n",
      "Training epoch: 190 [800/900 (88%)]\tLoss: 0.004827\n",
      "=========> Epoch: 190 Average loss: 0.0032\n",
      "Correlation coefficient: 0.6727\n",
      "Training epoch: 191 [0/900 (0%)]\tLoss: 0.002081\n",
      "Training epoch: 191 [160/900 (18%)]\tLoss: 0.001188\n",
      "Training epoch: 191 [320/900 (35%)]\tLoss: 0.003877\n",
      "Training epoch: 191 [480/900 (53%)]\tLoss: 0.001451\n",
      "Training epoch: 191 [640/900 (70%)]\tLoss: 0.001362\n",
      "Training epoch: 191 [800/900 (88%)]\tLoss: 0.001388\n",
      "=========> Epoch: 191 Average loss: 0.0023\n",
      "Correlation coefficient: 0.6903\n",
      "✅ Epoch 191: New best correlation = 0.6903\n",
      "Training epoch: 192 [0/900 (0%)]\tLoss: 0.002770\n",
      "Training epoch: 192 [160/900 (18%)]\tLoss: 0.001410\n",
      "Training epoch: 192 [320/900 (35%)]\tLoss: 0.000651\n",
      "Training epoch: 192 [480/900 (53%)]\tLoss: 0.001224\n",
      "Training epoch: 192 [640/900 (70%)]\tLoss: 0.000833\n",
      "Training epoch: 192 [800/900 (88%)]\tLoss: 0.001561\n",
      "=========> Epoch: 192 Average loss: 0.0014\n",
      "Correlation coefficient: 0.6840\n",
      "Training epoch: 193 [0/900 (0%)]\tLoss: 0.000821\n",
      "Training epoch: 193 [160/900 (18%)]\tLoss: 0.000974\n",
      "Training epoch: 193 [320/900 (35%)]\tLoss: 0.001092\n",
      "Training epoch: 193 [480/900 (53%)]\tLoss: 0.000566\n",
      "Training epoch: 193 [640/900 (70%)]\tLoss: 0.001188\n",
      "Training epoch: 193 [800/900 (88%)]\tLoss: 0.000272\n",
      "=========> Epoch: 193 Average loss: 0.0009\n",
      "Correlation coefficient: 0.6852\n",
      "Training epoch: 194 [0/900 (0%)]\tLoss: 0.000836\n",
      "Training epoch: 194 [160/900 (18%)]\tLoss: 0.000323\n",
      "Training epoch: 194 [320/900 (35%)]\tLoss: 0.000482\n",
      "Training epoch: 194 [480/900 (53%)]\tLoss: 0.000442\n",
      "Training epoch: 194 [640/900 (70%)]\tLoss: 0.000461\n",
      "Training epoch: 194 [800/900 (88%)]\tLoss: 0.000403\n",
      "=========> Epoch: 194 Average loss: 0.0006\n",
      "Correlation coefficient: 0.6872\n",
      "Training epoch: 195 [0/900 (0%)]\tLoss: 0.000352\n",
      "Training epoch: 195 [160/900 (18%)]\tLoss: 0.000213\n",
      "Training epoch: 195 [320/900 (35%)]\tLoss: 0.002265\n",
      "Training epoch: 195 [480/900 (53%)]\tLoss: 0.000414\n",
      "Training epoch: 195 [640/900 (70%)]\tLoss: 0.000217\n",
      "Training epoch: 195 [800/900 (88%)]\tLoss: 0.000119\n",
      "=========> Epoch: 195 Average loss: 0.0004\n",
      "Correlation coefficient: 0.6884\n",
      "Training epoch: 196 [0/900 (0%)]\tLoss: 0.000140\n",
      "Training epoch: 196 [160/900 (18%)]\tLoss: 0.001660\n",
      "Training epoch: 196 [320/900 (35%)]\tLoss: 0.000443\n",
      "Training epoch: 196 [480/900 (53%)]\tLoss: 0.000435\n",
      "Training epoch: 196 [640/900 (70%)]\tLoss: 0.000158\n",
      "Training epoch: 196 [800/900 (88%)]\tLoss: 0.000144\n",
      "=========> Epoch: 196 Average loss: 0.0003\n",
      "Correlation coefficient: 0.6863\n",
      "Training epoch: 197 [0/900 (0%)]\tLoss: 0.000112\n",
      "Training epoch: 197 [160/900 (18%)]\tLoss: 0.000228\n",
      "Training epoch: 197 [320/900 (35%)]\tLoss: 0.000302\n",
      "Training epoch: 197 [480/900 (53%)]\tLoss: 0.000339\n",
      "Training epoch: 197 [640/900 (70%)]\tLoss: 0.000341\n",
      "Training epoch: 197 [800/900 (88%)]\tLoss: 0.000317\n",
      "=========> Epoch: 197 Average loss: 0.0003\n",
      "Correlation coefficient: 0.6883\n",
      "Training epoch: 198 [0/900 (0%)]\tLoss: 0.000129\n",
      "Training epoch: 198 [160/900 (18%)]\tLoss: 0.000150\n",
      "Training epoch: 198 [320/900 (35%)]\tLoss: 0.000066\n",
      "Training epoch: 198 [480/900 (53%)]\tLoss: 0.000063\n",
      "Training epoch: 198 [640/900 (70%)]\tLoss: 0.000169\n",
      "Training epoch: 198 [800/900 (88%)]\tLoss: 0.000358\n",
      "=========> Epoch: 198 Average loss: 0.0002\n",
      "Correlation coefficient: 0.6841\n",
      "Training epoch: 199 [0/900 (0%)]\tLoss: 0.000765\n",
      "Training epoch: 199 [160/900 (18%)]\tLoss: 0.000110\n",
      "Training epoch: 199 [320/900 (35%)]\tLoss: 0.000481\n",
      "Training epoch: 199 [480/900 (53%)]\tLoss: 0.000344\n",
      "Training epoch: 199 [640/900 (70%)]\tLoss: 0.000172\n",
      "Training epoch: 199 [800/900 (88%)]\tLoss: 0.000271\n",
      "=========> Epoch: 199 Average loss: 0.0003\n",
      "Correlation coefficient: 0.6850\n",
      "Training epoch: 200 [0/900 (0%)]\tLoss: 0.000335\n",
      "Training epoch: 200 [160/900 (18%)]\tLoss: 0.000338\n",
      "Training epoch: 200 [320/900 (35%)]\tLoss: 0.000336\n",
      "Training epoch: 200 [480/900 (53%)]\tLoss: 0.000105\n",
      "Training epoch: 200 [640/900 (70%)]\tLoss: 0.000184\n",
      "Training epoch: 200 [800/900 (88%)]\tLoss: 0.000117\n",
      "=========> Epoch: 200 Average loss: 0.0002\n",
      "Correlation coefficient: 0.6865\n",
      "Training epoch: 201 [0/900 (0%)]\tLoss: 0.000515\n",
      "Training epoch: 201 [160/900 (18%)]\tLoss: 0.000495\n",
      "Training epoch: 201 [320/900 (35%)]\tLoss: 0.000206\n",
      "Training epoch: 201 [480/900 (53%)]\tLoss: 0.000229\n",
      "Training epoch: 201 [640/900 (70%)]\tLoss: 0.000089\n",
      "Training epoch: 201 [800/900 (88%)]\tLoss: 0.000120\n",
      "=========> Epoch: 201 Average loss: 0.0002\n",
      "Correlation coefficient: 0.6849\n",
      "Training epoch: 202 [0/900 (0%)]\tLoss: 0.000171\n",
      "Training epoch: 202 [160/900 (18%)]\tLoss: 0.000187\n",
      "Training epoch: 202 [320/900 (35%)]\tLoss: 0.000068\n",
      "Training epoch: 202 [480/900 (53%)]\tLoss: 0.000204\n",
      "Training epoch: 202 [640/900 (70%)]\tLoss: 0.000849\n",
      "Training epoch: 202 [800/900 (88%)]\tLoss: 0.000202\n",
      "=========> Epoch: 202 Average loss: 0.0004\n",
      "Correlation coefficient: 0.6869\n",
      "Training epoch: 203 [0/900 (0%)]\tLoss: 0.000154\n",
      "Training epoch: 203 [160/900 (18%)]\tLoss: 0.000298\n",
      "Training epoch: 203 [320/900 (35%)]\tLoss: 0.000387\n",
      "Training epoch: 203 [480/900 (53%)]\tLoss: 0.000389\n",
      "Training epoch: 203 [640/900 (70%)]\tLoss: 0.000798\n",
      "Training epoch: 203 [800/900 (88%)]\tLoss: 0.000184\n",
      "=========> Epoch: 203 Average loss: 0.0004\n",
      "Correlation coefficient: 0.6808\n",
      "Training epoch: 204 [0/900 (0%)]\tLoss: 0.000237\n",
      "Training epoch: 204 [160/900 (18%)]\tLoss: 0.000253\n",
      "Training epoch: 204 [320/900 (35%)]\tLoss: 0.000535\n",
      "Training epoch: 204 [480/900 (53%)]\tLoss: 0.000292\n",
      "Training epoch: 204 [640/900 (70%)]\tLoss: 0.000712\n",
      "Training epoch: 204 [800/900 (88%)]\tLoss: 0.000381\n",
      "=========> Epoch: 204 Average loss: 0.0006\n",
      "Correlation coefficient: 0.6854\n",
      "Training epoch: 205 [0/900 (0%)]\tLoss: 0.000328\n",
      "Training epoch: 205 [160/900 (18%)]\tLoss: 0.001152\n",
      "Training epoch: 205 [320/900 (35%)]\tLoss: 0.000564\n",
      "Training epoch: 205 [480/900 (53%)]\tLoss: 0.000687\n",
      "Training epoch: 205 [640/900 (70%)]\tLoss: 0.000586\n",
      "Training epoch: 205 [800/900 (88%)]\tLoss: 0.000667\n",
      "=========> Epoch: 205 Average loss: 0.0010\n",
      "Correlation coefficient: 0.6851\n",
      "Training epoch: 206 [0/900 (0%)]\tLoss: 0.003278\n",
      "Training epoch: 206 [160/900 (18%)]\tLoss: 0.002564\n",
      "Training epoch: 206 [320/900 (35%)]\tLoss: 0.002844\n",
      "Training epoch: 206 [480/900 (53%)]\tLoss: 0.001383\n",
      "Training epoch: 206 [640/900 (70%)]\tLoss: 0.000772\n",
      "Training epoch: 206 [800/900 (88%)]\tLoss: 0.001378\n",
      "=========> Epoch: 206 Average loss: 0.0012\n",
      "Correlation coefficient: 0.6855\n",
      "Training epoch: 207 [0/900 (0%)]\tLoss: 0.001119\n",
      "Training epoch: 207 [160/900 (18%)]\tLoss: 0.000963\n",
      "Training epoch: 207 [320/900 (35%)]\tLoss: 0.000645\n",
      "Training epoch: 207 [480/900 (53%)]\tLoss: 0.013959\n",
      "Training epoch: 207 [640/900 (70%)]\tLoss: 0.001887\n",
      "Training epoch: 207 [800/900 (88%)]\tLoss: 0.001096\n",
      "=========> Epoch: 207 Average loss: 0.0015\n",
      "Correlation coefficient: 0.6834\n",
      "Training epoch: 208 [0/900 (0%)]\tLoss: 0.001771\n",
      "Training epoch: 208 [160/900 (18%)]\tLoss: 0.001777\n",
      "Training epoch: 208 [320/900 (35%)]\tLoss: 0.003054\n",
      "Training epoch: 208 [480/900 (53%)]\tLoss: 0.000861\n",
      "Training epoch: 208 [640/900 (70%)]\tLoss: 0.001563\n",
      "Training epoch: 208 [800/900 (88%)]\tLoss: 0.002548\n",
      "=========> Epoch: 208 Average loss: 0.0018\n",
      "Correlation coefficient: 0.6852\n",
      "Training epoch: 209 [0/900 (0%)]\tLoss: 0.000892\n",
      "Training epoch: 209 [160/900 (18%)]\tLoss: 0.000522\n",
      "Training epoch: 209 [320/900 (35%)]\tLoss: 0.000488\n",
      "Training epoch: 209 [480/900 (53%)]\tLoss: 0.001985\n",
      "Training epoch: 209 [640/900 (70%)]\tLoss: 0.001934\n",
      "Training epoch: 209 [800/900 (88%)]\tLoss: 0.002879\n",
      "=========> Epoch: 209 Average loss: 0.0024\n",
      "Correlation coefficient: 0.6780\n",
      "Training epoch: 210 [0/900 (0%)]\tLoss: 0.001724\n",
      "Training epoch: 210 [160/900 (18%)]\tLoss: 0.004084\n",
      "Training epoch: 210 [320/900 (35%)]\tLoss: 0.001552\n",
      "Training epoch: 210 [480/900 (53%)]\tLoss: 0.014457\n",
      "Training epoch: 210 [640/900 (70%)]\tLoss: 0.011422\n",
      "Training epoch: 210 [800/900 (88%)]\tLoss: 0.004403\n",
      "=========> Epoch: 210 Average loss: 0.0051\n",
      "Correlation coefficient: 0.6862\n",
      "Training epoch: 211 [0/900 (0%)]\tLoss: 0.003212\n",
      "Training epoch: 211 [160/900 (18%)]\tLoss: 0.011831\n",
      "Training epoch: 211 [320/900 (35%)]\tLoss: 0.013103\n",
      "Training epoch: 211 [480/900 (53%)]\tLoss: 0.011028\n",
      "Training epoch: 211 [640/900 (70%)]\tLoss: 0.014893\n",
      "Training epoch: 211 [800/900 (88%)]\tLoss: 0.004658\n",
      "=========> Epoch: 211 Average loss: 0.0100\n",
      "Correlation coefficient: 0.6653\n",
      "Training epoch: 212 [0/900 (0%)]\tLoss: 0.004757\n",
      "Training epoch: 212 [160/900 (18%)]\tLoss: 0.010269\n",
      "Training epoch: 212 [320/900 (35%)]\tLoss: 0.022260\n",
      "Training epoch: 212 [480/900 (53%)]\tLoss: 0.004593\n",
      "Training epoch: 212 [640/900 (70%)]\tLoss: 0.006734\n",
      "Training epoch: 212 [800/900 (88%)]\tLoss: 0.015791\n",
      "=========> Epoch: 212 Average loss: 0.0104\n",
      "Correlation coefficient: 0.6737\n",
      "Training epoch: 213 [0/900 (0%)]\tLoss: 0.006829\n",
      "Training epoch: 213 [160/900 (18%)]\tLoss: 0.001201\n",
      "Training epoch: 213 [320/900 (35%)]\tLoss: 0.008475\n",
      "Training epoch: 213 [480/900 (53%)]\tLoss: 0.008173\n",
      "Training epoch: 213 [640/900 (70%)]\tLoss: 0.021340\n",
      "Training epoch: 213 [800/900 (88%)]\tLoss: 0.006201\n",
      "=========> Epoch: 213 Average loss: 0.0089\n",
      "Correlation coefficient: 0.6656\n",
      "Training epoch: 214 [0/900 (0%)]\tLoss: 0.006152\n",
      "Training epoch: 214 [160/900 (18%)]\tLoss: 0.006167\n",
      "Training epoch: 214 [320/900 (35%)]\tLoss: 0.010882\n",
      "Training epoch: 214 [480/900 (53%)]\tLoss: 0.007027\n",
      "Training epoch: 214 [640/900 (70%)]\tLoss: 0.005056\n",
      "Training epoch: 214 [800/900 (88%)]\tLoss: 0.003732\n",
      "=========> Epoch: 214 Average loss: 0.0073\n",
      "Correlation coefficient: 0.6796\n",
      "Training epoch: 215 [0/900 (0%)]\tLoss: 0.003065\n",
      "Training epoch: 215 [160/900 (18%)]\tLoss: 0.003162\n",
      "Training epoch: 215 [320/900 (35%)]\tLoss: 0.001729\n",
      "Training epoch: 215 [480/900 (53%)]\tLoss: 0.007471\n",
      "Training epoch: 215 [640/900 (70%)]\tLoss: 0.006519\n",
      "Training epoch: 215 [800/900 (88%)]\tLoss: 0.010174\n",
      "=========> Epoch: 215 Average loss: 0.0070\n",
      "Correlation coefficient: 0.6736\n",
      "Training epoch: 216 [0/900 (0%)]\tLoss: 0.005295\n",
      "Training epoch: 216 [160/900 (18%)]\tLoss: 0.005151\n",
      "Training epoch: 216 [320/900 (35%)]\tLoss: 0.004563\n",
      "Training epoch: 216 [480/900 (53%)]\tLoss: 0.012229\n",
      "Training epoch: 216 [640/900 (70%)]\tLoss: 0.005880\n",
      "Training epoch: 216 [800/900 (88%)]\tLoss: 0.003511\n",
      "=========> Epoch: 216 Average loss: 0.0051\n",
      "Correlation coefficient: 0.6731\n",
      "Training epoch: 217 [0/900 (0%)]\tLoss: 0.002115\n",
      "Training epoch: 217 [160/900 (18%)]\tLoss: 0.002368\n",
      "Training epoch: 217 [320/900 (35%)]\tLoss: 0.004544\n",
      "Training epoch: 217 [480/900 (53%)]\tLoss: 0.002808\n",
      "Training epoch: 217 [640/900 (70%)]\tLoss: 0.013230\n",
      "Training epoch: 217 [800/900 (88%)]\tLoss: 0.001435\n",
      "=========> Epoch: 217 Average loss: 0.0042\n",
      "Correlation coefficient: 0.6777\n",
      "Training epoch: 218 [0/900 (0%)]\tLoss: 0.001482\n",
      "Training epoch: 218 [160/900 (18%)]\tLoss: 0.003746\n",
      "Training epoch: 218 [320/900 (35%)]\tLoss: 0.005038\n",
      "Training epoch: 218 [480/900 (53%)]\tLoss: 0.004025\n",
      "Training epoch: 218 [640/900 (70%)]\tLoss: 0.001486\n",
      "Training epoch: 218 [800/900 (88%)]\tLoss: 0.002162\n",
      "=========> Epoch: 218 Average loss: 0.0031\n",
      "Correlation coefficient: 0.6790\n",
      "Training epoch: 219 [0/900 (0%)]\tLoss: 0.001178\n",
      "Training epoch: 219 [160/900 (18%)]\tLoss: 0.000661\n",
      "Training epoch: 219 [320/900 (35%)]\tLoss: 0.002496\n",
      "Training epoch: 219 [480/900 (53%)]\tLoss: 0.001654\n",
      "Training epoch: 219 [640/900 (70%)]\tLoss: 0.002253\n",
      "Training epoch: 219 [800/900 (88%)]\tLoss: 0.000709\n",
      "=========> Epoch: 219 Average loss: 0.0019\n",
      "Correlation coefficient: 0.6747\n",
      "Training epoch: 220 [0/900 (0%)]\tLoss: 0.000485\n",
      "Training epoch: 220 [160/900 (18%)]\tLoss: 0.001330\n",
      "Training epoch: 220 [320/900 (35%)]\tLoss: 0.002611\n",
      "Training epoch: 220 [480/900 (53%)]\tLoss: 0.000817\n",
      "Training epoch: 220 [640/900 (70%)]\tLoss: 0.001048\n",
      "Training epoch: 220 [800/900 (88%)]\tLoss: 0.000786\n",
      "=========> Epoch: 220 Average loss: 0.0012\n",
      "Correlation coefficient: 0.6755\n",
      "Training epoch: 221 [0/900 (0%)]\tLoss: 0.001539\n",
      "Training epoch: 221 [160/900 (18%)]\tLoss: 0.000828\n",
      "Training epoch: 221 [320/900 (35%)]\tLoss: 0.000861\n",
      "Training epoch: 221 [480/900 (53%)]\tLoss: 0.000469\n",
      "Training epoch: 221 [640/900 (70%)]\tLoss: 0.001101\n",
      "Training epoch: 221 [800/900 (88%)]\tLoss: 0.000971\n",
      "=========> Epoch: 221 Average loss: 0.0008\n",
      "Correlation coefficient: 0.6803\n",
      "Training epoch: 222 [0/900 (0%)]\tLoss: 0.000374\n",
      "Training epoch: 222 [160/900 (18%)]\tLoss: 0.000478\n",
      "Training epoch: 222 [320/900 (35%)]\tLoss: 0.000977\n",
      "Training epoch: 222 [480/900 (53%)]\tLoss: 0.001406\n",
      "Training epoch: 222 [640/900 (70%)]\tLoss: 0.000398\n",
      "Training epoch: 222 [800/900 (88%)]\tLoss: 0.000204\n",
      "=========> Epoch: 222 Average loss: 0.0005\n",
      "Correlation coefficient: 0.6780\n",
      "Training epoch: 223 [0/900 (0%)]\tLoss: 0.000357\n",
      "Training epoch: 223 [160/900 (18%)]\tLoss: 0.000537\n",
      "Training epoch: 223 [320/900 (35%)]\tLoss: 0.000484\n",
      "Training epoch: 223 [480/900 (53%)]\tLoss: 0.000223\n",
      "Training epoch: 223 [640/900 (70%)]\tLoss: 0.000324\n",
      "Training epoch: 223 [800/900 (88%)]\tLoss: 0.000451\n",
      "=========> Epoch: 223 Average loss: 0.0005\n",
      "Correlation coefficient: 0.6763\n",
      "Training epoch: 224 [0/900 (0%)]\tLoss: 0.000194\n",
      "Training epoch: 224 [160/900 (18%)]\tLoss: 0.000644\n",
      "Training epoch: 224 [320/900 (35%)]\tLoss: 0.000348\n",
      "Training epoch: 224 [480/900 (53%)]\tLoss: 0.000471\n",
      "Training epoch: 224 [640/900 (70%)]\tLoss: 0.000293\n",
      "Training epoch: 224 [800/900 (88%)]\tLoss: 0.000368\n",
      "=========> Epoch: 224 Average loss: 0.0004\n",
      "Correlation coefficient: 0.6801\n",
      "Training epoch: 225 [0/900 (0%)]\tLoss: 0.001094\n",
      "Training epoch: 225 [160/900 (18%)]\tLoss: 0.000387\n",
      "Training epoch: 225 [320/900 (35%)]\tLoss: 0.002122\n",
      "Training epoch: 225 [480/900 (53%)]\tLoss: 0.000200\n",
      "Training epoch: 225 [640/900 (70%)]\tLoss: 0.000849\n",
      "Training epoch: 225 [800/900 (88%)]\tLoss: 0.001253\n",
      "=========> Epoch: 225 Average loss: 0.0006\n",
      "Correlation coefficient: 0.6791\n",
      "Training epoch: 226 [0/900 (0%)]\tLoss: 0.000611\n",
      "Training epoch: 226 [160/900 (18%)]\tLoss: 0.000915\n",
      "Training epoch: 226 [320/900 (35%)]\tLoss: 0.000392\n",
      "Training epoch: 226 [480/900 (53%)]\tLoss: 0.000567\n",
      "Training epoch: 226 [640/900 (70%)]\tLoss: 0.000803\n",
      "Training epoch: 226 [800/900 (88%)]\tLoss: 0.001074\n",
      "=========> Epoch: 226 Average loss: 0.0008\n",
      "Correlation coefficient: 0.6771\n",
      "Training epoch: 227 [0/900 (0%)]\tLoss: 0.000234\n",
      "Training epoch: 227 [160/900 (18%)]\tLoss: 0.000506\n",
      "Training epoch: 227 [320/900 (35%)]\tLoss: 0.000922\n",
      "Training epoch: 227 [480/900 (53%)]\tLoss: 0.000377\n",
      "Training epoch: 227 [640/900 (70%)]\tLoss: 0.000283\n",
      "Training epoch: 227 [800/900 (88%)]\tLoss: 0.000868\n",
      "=========> Epoch: 227 Average loss: 0.0005\n",
      "Correlation coefficient: 0.6784\n",
      "Training epoch: 228 [0/900 (0%)]\tLoss: 0.000099\n",
      "Training epoch: 228 [160/900 (18%)]\tLoss: 0.000663\n",
      "Training epoch: 228 [320/900 (35%)]\tLoss: 0.000541\n",
      "Training epoch: 228 [480/900 (53%)]\tLoss: 0.000582\n",
      "Training epoch: 228 [640/900 (70%)]\tLoss: 0.000398\n",
      "Training epoch: 228 [800/900 (88%)]\tLoss: 0.000672\n",
      "=========> Epoch: 228 Average loss: 0.0004\n",
      "Correlation coefficient: 0.6782\n",
      "Training epoch: 229 [0/900 (0%)]\tLoss: 0.000286\n",
      "Training epoch: 229 [160/900 (18%)]\tLoss: 0.001244\n",
      "Training epoch: 229 [320/900 (35%)]\tLoss: 0.000643\n",
      "Training epoch: 229 [480/900 (53%)]\tLoss: 0.000261\n",
      "Training epoch: 229 [640/900 (70%)]\tLoss: 0.000478\n",
      "Training epoch: 229 [800/900 (88%)]\tLoss: 0.000446\n",
      "=========> Epoch: 229 Average loss: 0.0005\n",
      "Correlation coefficient: 0.6782\n",
      "Training epoch: 230 [0/900 (0%)]\tLoss: 0.000371\n",
      "Training epoch: 230 [160/900 (18%)]\tLoss: 0.000243\n",
      "Training epoch: 230 [320/900 (35%)]\tLoss: 0.000212\n",
      "Training epoch: 230 [480/900 (53%)]\tLoss: 0.000213\n",
      "Training epoch: 230 [640/900 (70%)]\tLoss: 0.000189\n",
      "Training epoch: 230 [800/900 (88%)]\tLoss: 0.000768\n",
      "=========> Epoch: 230 Average loss: 0.0004\n",
      "Correlation coefficient: 0.6775\n",
      "Training epoch: 231 [0/900 (0%)]\tLoss: 0.000662\n",
      "Training epoch: 231 [160/900 (18%)]\tLoss: 0.000730\n",
      "Training epoch: 231 [320/900 (35%)]\tLoss: 0.000515\n",
      "Training epoch: 231 [480/900 (53%)]\tLoss: 0.000329\n",
      "Training epoch: 231 [640/900 (70%)]\tLoss: 0.000106\n",
      "Training epoch: 231 [800/900 (88%)]\tLoss: 0.000676\n",
      "=========> Epoch: 231 Average loss: 0.0004\n",
      "Correlation coefficient: 0.6766\n",
      "Training epoch: 232 [0/900 (0%)]\tLoss: 0.000510\n",
      "Training epoch: 232 [160/900 (18%)]\tLoss: 0.000130\n",
      "Training epoch: 232 [320/900 (35%)]\tLoss: 0.000368\n",
      "Training epoch: 232 [480/900 (53%)]\tLoss: 0.000165\n",
      "Training epoch: 232 [640/900 (70%)]\tLoss: 0.000377\n",
      "Training epoch: 232 [800/900 (88%)]\tLoss: 0.000367\n",
      "=========> Epoch: 232 Average loss: 0.0003\n",
      "Correlation coefficient: 0.6783\n",
      "Training epoch: 233 [0/900 (0%)]\tLoss: 0.000377\n",
      "Training epoch: 233 [160/900 (18%)]\tLoss: 0.000218\n",
      "Training epoch: 233 [320/900 (35%)]\tLoss: 0.000061\n",
      "Training epoch: 233 [480/900 (53%)]\tLoss: 0.000329\n",
      "Training epoch: 233 [640/900 (70%)]\tLoss: 0.000269\n",
      "Training epoch: 233 [800/900 (88%)]\tLoss: 0.000341\n",
      "=========> Epoch: 233 Average loss: 0.0003\n",
      "Correlation coefficient: 0.6766\n",
      "Training epoch: 234 [0/900 (0%)]\tLoss: 0.000387\n",
      "Training epoch: 234 [160/900 (18%)]\tLoss: 0.000558\n",
      "Training epoch: 234 [320/900 (35%)]\tLoss: 0.000175\n",
      "Training epoch: 234 [480/900 (53%)]\tLoss: 0.000502\n",
      "Training epoch: 234 [640/900 (70%)]\tLoss: 0.000378\n",
      "Training epoch: 234 [800/900 (88%)]\tLoss: 0.000254\n",
      "=========> Epoch: 234 Average loss: 0.0004\n",
      "Correlation coefficient: 0.6772\n",
      "Training epoch: 235 [0/900 (0%)]\tLoss: 0.000183\n",
      "Training epoch: 235 [160/900 (18%)]\tLoss: 0.000197\n",
      "Training epoch: 235 [320/900 (35%)]\tLoss: 0.000404\n",
      "Training epoch: 235 [480/900 (53%)]\tLoss: 0.004659\n",
      "Training epoch: 235 [640/900 (70%)]\tLoss: 0.000841\n",
      "Training epoch: 235 [800/900 (88%)]\tLoss: 0.004043\n",
      "=========> Epoch: 235 Average loss: 0.0012\n",
      "Correlation coefficient: 0.6748\n",
      "Training epoch: 236 [0/900 (0%)]\tLoss: 0.001005\n",
      "Training epoch: 236 [160/900 (18%)]\tLoss: 0.001716\n",
      "Training epoch: 236 [320/900 (35%)]\tLoss: 0.000186\n",
      "Training epoch: 236 [480/900 (53%)]\tLoss: 0.002350\n",
      "Training epoch: 236 [640/900 (70%)]\tLoss: 0.001555\n",
      "Training epoch: 236 [800/900 (88%)]\tLoss: 0.004596\n",
      "=========> Epoch: 236 Average loss: 0.0024\n",
      "Correlation coefficient: 0.6775\n",
      "Training epoch: 237 [0/900 (0%)]\tLoss: 0.002785\n",
      "Training epoch: 237 [160/900 (18%)]\tLoss: 0.004759\n",
      "Training epoch: 237 [320/900 (35%)]\tLoss: 0.002727\n",
      "Training epoch: 237 [480/900 (53%)]\tLoss: 0.001471\n",
      "Training epoch: 237 [640/900 (70%)]\tLoss: 0.005023\n",
      "Training epoch: 237 [800/900 (88%)]\tLoss: 0.002238\n",
      "=========> Epoch: 237 Average loss: 0.0030\n",
      "Correlation coefficient: 0.6736\n",
      "Training epoch: 238 [0/900 (0%)]\tLoss: 0.003037\n",
      "Training epoch: 238 [160/900 (18%)]\tLoss: 0.002496\n",
      "Training epoch: 238 [320/900 (35%)]\tLoss: 0.007092\n",
      "Training epoch: 238 [480/900 (53%)]\tLoss: 0.005577\n",
      "Training epoch: 238 [640/900 (70%)]\tLoss: 0.003641\n",
      "Training epoch: 238 [800/900 (88%)]\tLoss: 0.005426\n",
      "=========> Epoch: 238 Average loss: 0.0043\n",
      "Correlation coefficient: 0.6687\n",
      "Training epoch: 239 [0/900 (0%)]\tLoss: 0.003632\n",
      "Training epoch: 239 [160/900 (18%)]\tLoss: 0.003482\n",
      "Training epoch: 239 [320/900 (35%)]\tLoss: 0.005914\n",
      "Training epoch: 239 [480/900 (53%)]\tLoss: 0.010088\n",
      "Training epoch: 239 [640/900 (70%)]\tLoss: 0.002200\n",
      "Training epoch: 239 [800/900 (88%)]\tLoss: 0.003457\n",
      "=========> Epoch: 239 Average loss: 0.0048\n",
      "Correlation coefficient: 0.6649\n",
      "Training epoch: 240 [0/900 (0%)]\tLoss: 0.001404\n",
      "Training epoch: 240 [160/900 (18%)]\tLoss: 0.008487\n",
      "Training epoch: 240 [320/900 (35%)]\tLoss: 0.007242\n",
      "Training epoch: 240 [480/900 (53%)]\tLoss: 0.005301\n",
      "Training epoch: 240 [640/900 (70%)]\tLoss: 0.002014\n",
      "Training epoch: 240 [800/900 (88%)]\tLoss: 0.005702\n",
      "=========> Epoch: 240 Average loss: 0.0059\n",
      "Correlation coefficient: 0.6501\n",
      "Training epoch: 241 [0/900 (0%)]\tLoss: 0.009447\n",
      "Training epoch: 241 [160/900 (18%)]\tLoss: 0.004525\n",
      "Training epoch: 241 [320/900 (35%)]\tLoss: 0.007876\n",
      "Training epoch: 241 [480/900 (53%)]\tLoss: 0.006246\n",
      "Training epoch: 241 [640/900 (70%)]\tLoss: 0.003939\n",
      "Training epoch: 241 [800/900 (88%)]\tLoss: 0.011515\n",
      "=========> Epoch: 241 Average loss: 0.0091\n",
      "Correlation coefficient: 0.6484\n",
      "⏹️  Epoch 241 early stopping (no improvement for 50 epochs)\n",
      "🏁 Fold 10 best correlation: 0.6903\n",
      "\n",
      "===== Cross-validation completed =====\n",
      "Best correlations for each fold: [0.7216 0.7844 0.6893 0.6213 0.642  0.7069 0.6485 0.6564 0.6728 0.6903]\n",
      "Mean correlation: 0.6833 ± 0.0446\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# =============================================================================\n",
    "# 10-fold cross-validation\n",
    "# =============================================================================\n",
    "def set_random_seed(seed=42):\n",
    "    \"\"\"Set random seed to ensure reproducible results\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True \n",
    "    torch.backends.cudnn.benchmark = False     \n",
    "\n",
    "def load_pretrained_weights(model, pretrained_path, device):\n",
    "    \"\"\"\n",
    "    Load pre-trained weights\n",
    "    \n",
    "    Args:\n",
    "        model: Target model\n",
    "        pretrained_path: Pre-trained model path\n",
    "        device: Device\n",
    "    \n",
    "    Returns:\n",
    "        bool: Whether loading was successful\n",
    "    \"\"\"\n",
    "    if not os.path.exists(pretrained_path):\n",
    "        return False\n",
    "    \n",
    "    try:\n",
    "        pretrained_state = torch.load(pretrained_path, map_location=device)\n",
    "        model_state = model.state_dict()\n",
    "        \n",
    "        # Filter out shape-mismatched parameters\n",
    "        filtered_params = {k: v for k, v in pretrained_state.items() \n",
    "                          if k in model_state and v.size() == model_state[k].size()}\n",
    "        \n",
    "        # Update model parameters\n",
    "        model_state.update(filtered_params)\n",
    "        model.load_state_dict(model_state, strict=False)\n",
    "        \n",
    "        # Print unmatched parameters\n",
    "        unmatched_params = [name for name in model_state.keys() if name not in filtered_params]\n",
    "        if unmatched_params:\n",
    "            print(f\"⚠️  The following parameters did not match pre-trained weights: {unmatched_params}\")\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Pre-trained model loading failed: {e}\")\n",
    "        return False\n",
    "\n",
    "def cross_validation_phenotype_prediction(genotype_data, phenotype_data, scaler, device):\n",
    "    \"\"\"\n",
    "    Execute 10-fold cross-validation for phenotype prediction\n",
    "    \n",
    "    Args:\n",
    "        genotype_data: Genotype data\n",
    "        phenotype_data: Phenotype data\n",
    "        scaler: Scaler\n",
    "        device: Device\n",
    "    \n",
    "    Returns:\n",
    "        list: Best correlation coefficients for each fold\n",
    "    \"\"\"\n",
    "    set_random_seed(config.RANDOM_SEED)\n",
    "    \n",
    "    kf = KFold(n_splits=config.N_SPLITS, shuffle=True, random_state=config.RANDOM_SEED)\n",
    "    all_best_correlations = []\n",
    "    fold_losses = [[] for _ in range(config.N_SPLITS)]\n",
    "    \n",
    "    for fold, (train_index, test_index) in enumerate(kf.split(genotype_data), start=1):\n",
    "        print(f\"\\n========== Cross-validation Fold {fold}/{config.N_SPLITS} ==========\")\n",
    "        \n",
    "        # Set random seed for each fold\n",
    "        set_random_seed(config.RANDOM_SEED + fold)\n",
    "        \n",
    "        # Split data\n",
    "        X_train, X_test = genotype_data[train_index], genotype_data[test_index]\n",
    "        y_train, y_test = phenotype_data[train_index], phenotype_data[test_index]\n",
    "        \n",
    "        # Initialize model\n",
    "        model = Mamba2PhenotypePredictor(\n",
    "            in_channels=X_train.shape[2],\n",
    "            out_channels=config.OUT_CHANNELS,\n",
    "            kernel_size=config.KERNEL_SIZE,\n",
    "            stride=config.STRIDE,\n",
    "            d_state=config.D_STATE\n",
    "        ).to(device)\n",
    "        \n",
    "        # Load pre-trained weights\n",
    "        if config.USE_PRETRAINED and os.path.exists(\"model/model_state.pth\"):\n",
    "            print(f\"🔄 Fold {fold}: Loading pre-trained model...\")\n",
    "            if load_pretrained_weights(model, \"model/model_state.pth\", device):\n",
    "                print(f\"✅ Fold {fold}: Pre-trained model loaded successfully\")\n",
    "            else:\n",
    "                print(f\"⚠️  Fold {fold}: Pre-trained model loading failed, using random initialization\")\n",
    "        else:\n",
    "            if config.USE_PRETRAINED:\n",
    "                print(f\"⚠️  Fold {fold}: Pre-trained model file does not exist, using random initialization\")\n",
    "            else:\n",
    "                print(f\"🔄 Fold {fold}: Using random initialization\")\n",
    "        \n",
    "        # Set optimizer and loss function\n",
    "        criterion = nn.MSELoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.00040854050495879857)\n",
    "        \n",
    "        # Create data loaders\n",
    "        train_dataset = PhenotypeDataset(X_train, y_train)\n",
    "        test_dataset = PhenotypeDataset(X_test, y_test)\n",
    "        train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=len(test_dataset), shuffle=False)\n",
    "        \n",
    "        # Train model\n",
    "        best_corr = -1.0\n",
    "        early_stopping_counter = 0\n",
    "        \n",
    "        for epoch in range(1, 101):  \n",
    "            corr, predictions, epoch_loss_values = train_phenotype_predictor(\n",
    "                epoch, model, device, optimizer, criterion, train_loader, test_loader, scaler\n",
    "            )\n",
    "            \n",
    "            fold_losses[fold - 1].extend(epoch_loss_values)\n",
    "            \n",
    "            # Early stopping mechanism\n",
    "            if corr > best_corr:\n",
    "                best_corr = corr\n",
    "                print(f\"✅ Epoch {epoch}: New best correlation = {best_corr:.4f}\")\n",
    "                torch.save(model.state_dict(), f\"model/fold_{fold}_best_model.pth\")\n",
    "                early_stopping_counter = 0\n",
    "            else:\n",
    "                early_stopping_counter += 1\n",
    "            \n",
    "            if early_stopping_counter >= config.EARLY_STOPPING_PATIENCE:\n",
    "                print(f\"⏹️  Epoch {epoch} early stopping (no improvement for {config.EARLY_STOPPING_PATIENCE} epochs)\")\n",
    "                break\n",
    "        \n",
    "        print(f\"🏁 Fold {fold} best correlation: {best_corr:.4f}\")\n",
    "        all_best_correlations.append(best_corr)\n",
    "    \n",
    "    return all_best_correlations\n",
    "\n",
    "# Execute 10-fold cross-validation\n",
    "all_best_correlations = cross_validation_phenotype_prediction(\n",
    "    genotype_encoded, phenotype_normalized, phenotype_scaler, device\n",
    ")\n",
    "\n",
    "print(\"\\n===== Cross-validation completed =====\")\n",
    "print(\"Best correlations for each fold:\", np.round(all_best_correlations, 4))\n",
    "print(f\"Mean correlation: {np.mean(all_best_correlations):.4f} ± {np.std(all_best_correlations):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total runtime: 1396.18 seconds\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Results statistics and summary\n",
    "# =============================================================================\n",
    "def calculate_runtime_summary(start_time):\n",
    "    \"\"\"Calculate runtime statistics\"\"\"\n",
    "    end_time = time.time()\n",
    "    elapsed_time = end_time - start_time\n",
    "    print(f\"Total runtime: {elapsed_time:.2f} seconds\")\n",
    "    return elapsed_time\n",
    "\n",
    "# Calculate runtime\n",
    "total_runtime = calculate_runtime_summary(start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing ratio: 0.0\n",
      "Mean correlation: 0.6833\n",
      "Standard deviation: 0.0446\n",
      "Best correlation: 0.7844\n",
      "Worst correlation: 0.6213\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Final results output\n",
    "# =============================================================================\n",
    "def print_final_results(missing_ratio, correlations):\n",
    "    \"\"\"Print final results\"\"\"\n",
    "    print(f\"Missing ratio: {missing_ratio}\")\n",
    "    mean_correlation = np.mean(correlations)\n",
    "    std_correlation = np.std(correlations)\n",
    "    \n",
    "    print(f\"Mean correlation: {mean_correlation:.4f}\")\n",
    "    print(f\"Standard deviation: {std_correlation:.4f}\")\n",
    "    print(f\"Best correlation: {np.max(correlations):.4f}\")\n",
    "    print(f\"Worst correlation: {np.min(correlations):.4f}\")\n",
    "    \n",
    "    return mean_correlation\n",
    "\n",
    "# Output final results\n",
    "final_mean_correlation = print_final_results(config.MISSING_RATIO, all_best_correlations)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
